Starting /home/h/hc443/.bash_profile ... standard AFS bash profile

========================
Home directory : /home/h/hc443 is not in AFS -- skipping quota check
========================

On host node436 :
	 19:53:39 up 72 days, 23:52,  0 users,  load average: 1.08, 1.07, 1.01

=== === === Your Kerberos ticket and AFS token status === === ===
klist: No credentials cache found (filename: /tmp/krb5cc_515598)
Kerberos : 
AFS      : 

Loading default modules ...
Create file : "/home/h/hc443/.modules" to customize.

No modules loaded
2023-10-04 19:53:46,342 - INFO - Log directory: ./libgptb/log
2023-10-04 19:53:46,343 - INFO - Begin pipeline, task=GCL, model_name=GBT, dataset_name=Cora, exp_id=89190
2023-10-04 19:53:46,343 - INFO - {'task': 'GCL', 'model': 'GBT', 'dataset': 'Cora', 'saved_model': True, 'train': True, 'seed': 0, 'nhid': 512, 'dataset_class': 'PyGDataset', 'executor': 'GBTExecutor', 'evaluator': 'GBTEvaluator', 'pe1': 0.5, 'pe2': 0.5, 'pf1': 0.1, 'pf2': 0.1, 'gpu': True, 'gpu_id': 0, 'max_epoch': 4000, 'train_loss': 'none', 'epoch': 0, 'learner': 'adam', 'learning_rate': 0.0005, 'lr_decay': False, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': False, 'hyper_tune': False, 'device': device(type='cuda', index=0), 'exp_id': 89190}
{'task': 'GCL', 'model': 'GBT', 'dataset': 'Cora', 'saved_model': True, 'train': True, 'seed': 0, 'nhid': 512, 'dataset_class': 'PyGDataset', 'executor': 'GBTExecutor', 'evaluator': 'GBTEvaluator', 'pe1': 0.5, 'pe2': 0.5, 'pf1': 0.1, 'pf2': 0.1, 'gpu': True, 'gpu_id': 0, 'max_epoch': 4000, 'train_loss': 'none', 'epoch': 0, 'learner': 'adam', 'learning_rate': 0.0005, 'lr_decay': False, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': False, 'hyper_tune': False, 'device': device(type='cuda', index=0), 'exp_id': 89190}
GBT
2023-10-04 19:53:50,545 - INFO - GBT(
  (gconv): GConv(
    (act): PReLU(num_parameters=1)
    (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
    (conv1): GCNConv(1433, 1024)
    (conv2): GCNConv(1024, 512)
  )
  (encoder_model): Encoder(
    (encoder): GConv(
      (act): PReLU(num_parameters=1)
      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
      (conv1): GCNConv(1433, 1024)
      (conv2): GCNConv(1024, 512)
    )
  )
  (contrast_model): WithinEmbedContrast()
)
2023-10-04 19:53:50,546 - INFO - gconv.act.weight	torch.Size([1])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.bn.weight	torch.Size([1024])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.bn.bias	torch.Size([1024])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.conv1.bias	torch.Size([1024])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.conv1.lin.weight	torch.Size([1024, 1433])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.conv2.bias	torch.Size([512])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - gconv.conv2.lin.weight	torch.Size([512, 1024])	cuda:0	True
2023-10-04 19:53:50,546 - INFO - Total parameter numbers: 1995265
2023-10-04 19:53:50,546 - INFO - You select `adam` optimizer.
2023-10-04 19:53:50,546 - INFO - Start training ...
2023-10-04 19:53:50,546 - INFO - num_batches:6
/home/h/hc443/miniconda3/envs/pygcl/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead
  warnings.warn(out)
2023-10-04 19:53:50,998 - INFO - epoch complete!
2023-10-04 19:53:50,998 - INFO - evaluating now!
2023-10-04 19:53:50,998 - INFO - Epoch [0/4000] train_loss: 95.3963, lr: 0.000500, 0.45s
2023-10-04 19:53:51,509 - INFO - Saved model at 0
2023-10-04 19:53:51,510 - INFO - Val loss decrease from inf to 95.3963, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch0.tar
2023-10-04 19:53:51,552 - INFO - epoch complete!
2023-10-04 19:53:51,553 - INFO - evaluating now!
2023-10-04 19:53:51,553 - INFO - Epoch [1/4000] train_loss: 94.5286, lr: 0.000500, 0.04s
2023-10-04 19:53:51,784 - INFO - Saved model at 1
2023-10-04 19:53:51,784 - INFO - Val loss decrease from 95.3963 to 94.5286, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1.tar
2023-10-04 19:53:51,827 - INFO - epoch complete!
2023-10-04 19:53:51,828 - INFO - evaluating now!
2023-10-04 19:53:51,828 - INFO - Epoch [2/4000] train_loss: 75.3657, lr: 0.000500, 0.04s
2023-10-04 19:53:52,059 - INFO - Saved model at 2
2023-10-04 19:53:52,059 - INFO - Val loss decrease from 94.5286 to 75.3657, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch2.tar
2023-10-04 19:53:52,102 - INFO - epoch complete!
2023-10-04 19:53:52,102 - INFO - evaluating now!
2023-10-04 19:53:52,102 - INFO - Epoch [3/4000] train_loss: 82.3437, lr: 0.000500, 0.04s
2023-10-04 19:53:52,144 - INFO - epoch complete!
2023-10-04 19:53:52,145 - INFO - evaluating now!
2023-10-04 19:53:52,145 - INFO - Epoch [4/4000] train_loss: 77.7033, lr: 0.000500, 0.04s
2023-10-04 19:53:52,186 - INFO - epoch complete!
2023-10-04 19:53:52,186 - INFO - evaluating now!
2023-10-04 19:53:52,187 - INFO - Epoch [5/4000] train_loss: 62.5389, lr: 0.000500, 0.04s
2023-10-04 19:53:52,426 - INFO - Saved model at 5
2023-10-04 19:53:52,426 - INFO - Val loss decrease from 75.3657 to 62.5389, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch5.tar
2023-10-04 19:53:52,468 - INFO - epoch complete!
2023-10-04 19:53:52,468 - INFO - evaluating now!
2023-10-04 19:53:52,469 - INFO - Epoch [6/4000] train_loss: 62.5071, lr: 0.000500, 0.04s
2023-10-04 19:53:52,705 - INFO - Saved model at 6
2023-10-04 19:53:52,706 - INFO - Val loss decrease from 62.5389 to 62.5071, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch6.tar
2023-10-04 19:53:52,748 - INFO - epoch complete!
2023-10-04 19:53:52,749 - INFO - evaluating now!
2023-10-04 19:53:52,749 - INFO - Epoch [7/4000] train_loss: 54.0657, lr: 0.000500, 0.04s
2023-10-04 19:53:52,980 - INFO - Saved model at 7
2023-10-04 19:53:52,980 - INFO - Val loss decrease from 62.5071 to 54.0657, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch7.tar
2023-10-04 19:53:53,022 - INFO - epoch complete!
2023-10-04 19:53:53,023 - INFO - evaluating now!
2023-10-04 19:53:53,023 - INFO - Epoch [8/4000] train_loss: 54.9247, lr: 0.000500, 0.04s
2023-10-04 19:53:53,065 - INFO - epoch complete!
2023-10-04 19:53:53,065 - INFO - evaluating now!
2023-10-04 19:53:53,066 - INFO - Epoch [9/4000] train_loss: 48.9078, lr: 0.000500, 0.04s
2023-10-04 19:53:53,296 - INFO - Saved model at 9
2023-10-04 19:53:53,297 - INFO - Val loss decrease from 54.0657 to 48.9078, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch9.tar
2023-10-04 19:53:53,340 - INFO - epoch complete!
2023-10-04 19:53:53,340 - INFO - evaluating now!
2023-10-04 19:53:53,340 - INFO - Epoch [10/4000] train_loss: 50.5464, lr: 0.000500, 0.04s
2023-10-04 19:53:53,383 - INFO - epoch complete!
2023-10-04 19:53:53,383 - INFO - evaluating now!
2023-10-04 19:53:53,383 - INFO - Epoch [11/4000] train_loss: 46.0386, lr: 0.000500, 0.04s
2023-10-04 19:53:53,614 - INFO - Saved model at 11
2023-10-04 19:53:53,614 - INFO - Val loss decrease from 48.9078 to 46.0386, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch11.tar
2023-10-04 19:53:53,657 - INFO - epoch complete!
2023-10-04 19:53:53,658 - INFO - evaluating now!
2023-10-04 19:53:53,658 - INFO - Epoch [12/4000] train_loss: 46.3628, lr: 0.000500, 0.04s
2023-10-04 19:53:53,700 - INFO - epoch complete!
2023-10-04 19:53:53,700 - INFO - evaluating now!
2023-10-04 19:53:53,700 - INFO - Epoch [13/4000] train_loss: 42.9495, lr: 0.000500, 0.04s
2023-10-04 19:53:53,931 - INFO - Saved model at 13
2023-10-04 19:53:53,931 - INFO - Val loss decrease from 46.0386 to 42.9495, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch13.tar
2023-10-04 19:53:53,973 - INFO - epoch complete!
2023-10-04 19:53:53,974 - INFO - evaluating now!
2023-10-04 19:53:53,974 - INFO - Epoch [14/4000] train_loss: 41.5714, lr: 0.000500, 0.04s
2023-10-04 19:53:54,209 - INFO - Saved model at 14
2023-10-04 19:53:54,209 - INFO - Val loss decrease from 42.9495 to 41.5714, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch14.tar
2023-10-04 19:53:54,251 - INFO - epoch complete!
2023-10-04 19:53:54,252 - INFO - evaluating now!
2023-10-04 19:53:54,252 - INFO - Epoch [15/4000] train_loss: 40.5747, lr: 0.000500, 0.04s
2023-10-04 19:53:54,496 - INFO - Saved model at 15
2023-10-04 19:53:54,496 - INFO - Val loss decrease from 41.5714 to 40.5747, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch15.tar
2023-10-04 19:53:54,538 - INFO - epoch complete!
2023-10-04 19:53:54,539 - INFO - evaluating now!
2023-10-04 19:53:54,539 - INFO - Epoch [16/4000] train_loss: 41.8502, lr: 0.000500, 0.04s
2023-10-04 19:53:54,581 - INFO - epoch complete!
2023-10-04 19:53:54,581 - INFO - evaluating now!
2023-10-04 19:53:54,581 - INFO - Epoch [17/4000] train_loss: 42.2846, lr: 0.000500, 0.04s
2023-10-04 19:53:54,623 - INFO - epoch complete!
2023-10-04 19:53:54,623 - INFO - evaluating now!
2023-10-04 19:53:54,624 - INFO - Epoch [18/4000] train_loss: 39.6877, lr: 0.000500, 0.04s
2023-10-04 19:53:54,863 - INFO - Saved model at 18
2023-10-04 19:53:54,864 - INFO - Val loss decrease from 40.5747 to 39.6877, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch18.tar
2023-10-04 19:53:54,905 - INFO - epoch complete!
2023-10-04 19:53:54,906 - INFO - evaluating now!
2023-10-04 19:53:54,906 - INFO - Epoch [19/4000] train_loss: 38.2585, lr: 0.000500, 0.04s
2023-10-04 19:53:55,147 - INFO - Saved model at 19
2023-10-04 19:53:55,147 - INFO - Val loss decrease from 39.6877 to 38.2585, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch19.tar
2023-10-04 19:53:55,187 - INFO - epoch complete!
2023-10-04 19:53:55,188 - INFO - evaluating now!
2023-10-04 19:53:55,188 - INFO - Epoch [20/4000] train_loss: 35.2660, lr: 0.000500, 0.04s
2023-10-04 19:53:55,430 - INFO - Saved model at 20
2023-10-04 19:53:55,431 - INFO - Val loss decrease from 38.2585 to 35.2660, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch20.tar
2023-10-04 19:53:55,472 - INFO - epoch complete!
2023-10-04 19:53:55,472 - INFO - evaluating now!
2023-10-04 19:53:55,472 - INFO - Epoch [21/4000] train_loss: 32.9478, lr: 0.000500, 0.04s
2023-10-04 19:53:55,703 - INFO - Saved model at 21
2023-10-04 19:53:55,703 - INFO - Val loss decrease from 35.2660 to 32.9478, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch21.tar
2023-10-04 19:53:55,744 - INFO - epoch complete!
2023-10-04 19:53:55,745 - INFO - evaluating now!
2023-10-04 19:53:55,745 - INFO - Epoch [22/4000] train_loss: 34.3946, lr: 0.000500, 0.04s
2023-10-04 19:53:55,786 - INFO - epoch complete!
2023-10-04 19:53:55,786 - INFO - evaluating now!
2023-10-04 19:53:55,786 - INFO - Epoch [23/4000] train_loss: 34.7258, lr: 0.000500, 0.04s
2023-10-04 19:53:55,828 - INFO - epoch complete!
2023-10-04 19:53:55,828 - INFO - evaluating now!
2023-10-04 19:53:55,828 - INFO - Epoch [24/4000] train_loss: 32.9849, lr: 0.000500, 0.04s
2023-10-04 19:53:55,869 - INFO - epoch complete!
2023-10-04 19:53:55,869 - INFO - evaluating now!
2023-10-04 19:53:55,869 - INFO - Epoch [25/4000] train_loss: 32.6763, lr: 0.000500, 0.04s
2023-10-04 19:53:56,100 - INFO - Saved model at 25
2023-10-04 19:53:56,100 - INFO - Val loss decrease from 32.9478 to 32.6763, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch25.tar
2023-10-04 19:53:56,141 - INFO - epoch complete!
2023-10-04 19:53:56,142 - INFO - evaluating now!
2023-10-04 19:53:56,142 - INFO - Epoch [26/4000] train_loss: 33.0662, lr: 0.000500, 0.04s
2023-10-04 19:53:56,183 - INFO - epoch complete!
2023-10-04 19:53:56,183 - INFO - evaluating now!
2023-10-04 19:53:56,183 - INFO - Epoch [27/4000] train_loss: 32.7319, lr: 0.000500, 0.04s
2023-10-04 19:53:56,223 - INFO - epoch complete!
2023-10-04 19:53:56,224 - INFO - evaluating now!
2023-10-04 19:53:56,224 - INFO - Epoch [28/4000] train_loss: 30.0178, lr: 0.000500, 0.04s
2023-10-04 19:53:56,455 - INFO - Saved model at 28
2023-10-04 19:53:56,456 - INFO - Val loss decrease from 32.6763 to 30.0178, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch28.tar
2023-10-04 19:53:56,496 - INFO - epoch complete!
2023-10-04 19:53:56,497 - INFO - evaluating now!
2023-10-04 19:53:56,497 - INFO - Epoch [29/4000] train_loss: 29.2681, lr: 0.000500, 0.04s
2023-10-04 19:53:56,736 - INFO - Saved model at 29
2023-10-04 19:53:56,737 - INFO - Val loss decrease from 30.0178 to 29.2681, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch29.tar
2023-10-04 19:53:56,779 - INFO - epoch complete!
2023-10-04 19:53:56,780 - INFO - evaluating now!
2023-10-04 19:53:56,780 - INFO - Epoch [30/4000] train_loss: 30.0596, lr: 0.000500, 0.04s
2023-10-04 19:53:56,820 - INFO - epoch complete!
2023-10-04 19:53:56,821 - INFO - evaluating now!
2023-10-04 19:53:56,821 - INFO - Epoch [31/4000] train_loss: 28.8847, lr: 0.000500, 0.04s
2023-10-04 19:53:57,052 - INFO - Saved model at 31
2023-10-04 19:53:57,052 - INFO - Val loss decrease from 29.2681 to 28.8847, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch31.tar
2023-10-04 19:53:57,093 - INFO - epoch complete!
2023-10-04 19:53:57,093 - INFO - evaluating now!
2023-10-04 19:53:57,093 - INFO - Epoch [32/4000] train_loss: 29.6985, lr: 0.000500, 0.04s
2023-10-04 19:53:57,134 - INFO - epoch complete!
2023-10-04 19:53:57,134 - INFO - evaluating now!
2023-10-04 19:53:57,134 - INFO - Epoch [33/4000] train_loss: 29.0617, lr: 0.000500, 0.04s
2023-10-04 19:53:57,175 - INFO - epoch complete!
2023-10-04 19:53:57,176 - INFO - evaluating now!
2023-10-04 19:53:57,176 - INFO - Epoch [34/4000] train_loss: 28.2557, lr: 0.000500, 0.04s
2023-10-04 19:53:57,405 - INFO - Saved model at 34
2023-10-04 19:53:57,405 - INFO - Val loss decrease from 28.8847 to 28.2557, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch34.tar
2023-10-04 19:53:57,446 - INFO - epoch complete!
2023-10-04 19:53:57,446 - INFO - evaluating now!
2023-10-04 19:53:57,447 - INFO - Epoch [35/4000] train_loss: 28.0522, lr: 0.000500, 0.04s
2023-10-04 19:53:57,678 - INFO - Saved model at 35
2023-10-04 19:53:57,678 - INFO - Val loss decrease from 28.2557 to 28.0522, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch35.tar
2023-10-04 19:53:57,719 - INFO - epoch complete!
2023-10-04 19:53:57,719 - INFO - evaluating now!
2023-10-04 19:53:57,719 - INFO - Epoch [36/4000] train_loss: 27.3051, lr: 0.000500, 0.04s
2023-10-04 19:53:57,950 - INFO - Saved model at 36
2023-10-04 19:53:57,951 - INFO - Val loss decrease from 28.0522 to 27.3051, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch36.tar
2023-10-04 19:53:57,992 - INFO - epoch complete!
2023-10-04 19:53:57,992 - INFO - evaluating now!
2023-10-04 19:53:57,992 - INFO - Epoch [37/4000] train_loss: 25.5908, lr: 0.000500, 0.04s
2023-10-04 19:53:58,223 - INFO - Saved model at 37
2023-10-04 19:53:58,223 - INFO - Val loss decrease from 27.3051 to 25.5908, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch37.tar
2023-10-04 19:53:58,265 - INFO - epoch complete!
2023-10-04 19:53:58,265 - INFO - evaluating now!
2023-10-04 19:53:58,265 - INFO - Epoch [38/4000] train_loss: 27.5817, lr: 0.000500, 0.04s
2023-10-04 19:53:58,306 - INFO - epoch complete!
2023-10-04 19:53:58,306 - INFO - evaluating now!
2023-10-04 19:53:58,306 - INFO - Epoch [39/4000] train_loss: 26.8052, lr: 0.000500, 0.04s
2023-10-04 19:53:58,347 - INFO - epoch complete!
2023-10-04 19:53:58,347 - INFO - evaluating now!
2023-10-04 19:53:58,347 - INFO - Epoch [40/4000] train_loss: 25.3689, lr: 0.000500, 0.04s
2023-10-04 19:53:58,578 - INFO - Saved model at 40
2023-10-04 19:53:58,578 - INFO - Val loss decrease from 25.5908 to 25.3689, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch40.tar
2023-10-04 19:53:58,621 - INFO - epoch complete!
2023-10-04 19:53:58,621 - INFO - evaluating now!
2023-10-04 19:53:58,621 - INFO - Epoch [41/4000] train_loss: 25.2410, lr: 0.000500, 0.04s
2023-10-04 19:53:58,852 - INFO - Saved model at 41
2023-10-04 19:53:58,852 - INFO - Val loss decrease from 25.3689 to 25.2410, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch41.tar
2023-10-04 19:53:58,894 - INFO - epoch complete!
2023-10-04 19:53:58,894 - INFO - evaluating now!
2023-10-04 19:53:58,894 - INFO - Epoch [42/4000] train_loss: 25.0244, lr: 0.000500, 0.04s
2023-10-04 19:53:59,125 - INFO - Saved model at 42
2023-10-04 19:53:59,125 - INFO - Val loss decrease from 25.2410 to 25.0244, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch42.tar
2023-10-04 19:53:59,166 - INFO - epoch complete!
2023-10-04 19:53:59,167 - INFO - evaluating now!
2023-10-04 19:53:59,167 - INFO - Epoch [43/4000] train_loss: 24.2691, lr: 0.000500, 0.04s
2023-10-04 19:53:59,397 - INFO - Saved model at 43
2023-10-04 19:53:59,398 - INFO - Val loss decrease from 25.0244 to 24.2691, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch43.tar
2023-10-04 19:53:59,439 - INFO - epoch complete!
2023-10-04 19:53:59,439 - INFO - evaluating now!
2023-10-04 19:53:59,440 - INFO - Epoch [44/4000] train_loss: 26.2216, lr: 0.000500, 0.04s
2023-10-04 19:53:59,480 - INFO - epoch complete!
2023-10-04 19:53:59,481 - INFO - evaluating now!
2023-10-04 19:53:59,481 - INFO - Epoch [45/4000] train_loss: 22.5665, lr: 0.000500, 0.04s
2023-10-04 19:54:00,044 - INFO - Saved model at 45
2023-10-04 19:54:00,044 - INFO - Val loss decrease from 24.2691 to 22.5665, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch45.tar
2023-10-04 19:54:00,085 - INFO - epoch complete!
2023-10-04 19:54:00,085 - INFO - evaluating now!
2023-10-04 19:54:00,085 - INFO - Epoch [46/4000] train_loss: 22.6689, lr: 0.000500, 0.04s
2023-10-04 19:54:00,126 - INFO - epoch complete!
2023-10-04 19:54:00,126 - INFO - evaluating now!
2023-10-04 19:54:00,126 - INFO - Epoch [47/4000] train_loss: 22.5615, lr: 0.000500, 0.04s
2023-10-04 19:54:00,357 - INFO - Saved model at 47
2023-10-04 19:54:00,357 - INFO - Val loss decrease from 22.5665 to 22.5615, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch47.tar
2023-10-04 19:54:00,399 - INFO - epoch complete!
2023-10-04 19:54:00,399 - INFO - evaluating now!
2023-10-04 19:54:00,399 - INFO - Epoch [48/4000] train_loss: 24.9287, lr: 0.000500, 0.04s
2023-10-04 19:54:00,440 - INFO - epoch complete!
2023-10-04 19:54:00,440 - INFO - evaluating now!
2023-10-04 19:54:00,441 - INFO - Epoch [49/4000] train_loss: 22.3212, lr: 0.000500, 0.04s
2023-10-04 19:54:00,671 - INFO - Saved model at 49
2023-10-04 19:54:00,672 - INFO - Val loss decrease from 22.5615 to 22.3212, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch49.tar
2023-10-04 19:54:00,713 - INFO - epoch complete!
2023-10-04 19:54:00,713 - INFO - evaluating now!
2023-10-04 19:54:00,713 - INFO - Epoch [50/4000] train_loss: 22.4670, lr: 0.000500, 0.04s
2023-10-04 19:54:00,754 - INFO - epoch complete!
2023-10-04 19:54:00,754 - INFO - evaluating now!
2023-10-04 19:54:00,754 - INFO - Epoch [51/4000] train_loss: 23.0477, lr: 0.000500, 0.04s
2023-10-04 19:54:00,794 - INFO - epoch complete!
2023-10-04 19:54:00,795 - INFO - evaluating now!
2023-10-04 19:54:00,795 - INFO - Epoch [52/4000] train_loss: 22.9339, lr: 0.000500, 0.04s
2023-10-04 19:54:00,835 - INFO - epoch complete!
2023-10-04 19:54:00,835 - INFO - evaluating now!
2023-10-04 19:54:00,835 - INFO - Epoch [53/4000] train_loss: 23.6516, lr: 0.000500, 0.04s
2023-10-04 19:54:00,875 - INFO - epoch complete!
2023-10-04 19:54:00,876 - INFO - evaluating now!
2023-10-04 19:54:00,876 - INFO - Epoch [54/4000] train_loss: 22.0078, lr: 0.000500, 0.04s
2023-10-04 19:54:01,104 - INFO - Saved model at 54
2023-10-04 19:54:01,104 - INFO - Val loss decrease from 22.3212 to 22.0078, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch54.tar
2023-10-04 19:54:01,144 - INFO - epoch complete!
2023-10-04 19:54:01,145 - INFO - evaluating now!
2023-10-04 19:54:01,145 - INFO - Epoch [55/4000] train_loss: 21.5424, lr: 0.000500, 0.04s
2023-10-04 19:54:01,376 - INFO - Saved model at 55
2023-10-04 19:54:01,376 - INFO - Val loss decrease from 22.0078 to 21.5424, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch55.tar
2023-10-04 19:54:01,416 - INFO - epoch complete!
2023-10-04 19:54:01,416 - INFO - evaluating now!
2023-10-04 19:54:01,416 - INFO - Epoch [56/4000] train_loss: 22.0101, lr: 0.000500, 0.04s
2023-10-04 19:54:01,458 - INFO - epoch complete!
2023-10-04 19:54:01,458 - INFO - evaluating now!
2023-10-04 19:54:01,458 - INFO - Epoch [57/4000] train_loss: 21.4308, lr: 0.000500, 0.04s
2023-10-04 19:54:01,690 - INFO - Saved model at 57
2023-10-04 19:54:01,690 - INFO - Val loss decrease from 21.5424 to 21.4308, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch57.tar
2023-10-04 19:54:01,730 - INFO - epoch complete!
2023-10-04 19:54:01,730 - INFO - evaluating now!
2023-10-04 19:54:01,730 - INFO - Epoch [58/4000] train_loss: 21.7860, lr: 0.000500, 0.04s
2023-10-04 19:54:01,770 - INFO - epoch complete!
2023-10-04 19:54:01,771 - INFO - evaluating now!
2023-10-04 19:54:01,771 - INFO - Epoch [59/4000] train_loss: 20.3407, lr: 0.000500, 0.04s
2023-10-04 19:54:02,001 - INFO - Saved model at 59
2023-10-04 19:54:02,002 - INFO - Val loss decrease from 21.4308 to 20.3407, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch59.tar
2023-10-04 19:54:02,042 - INFO - epoch complete!
2023-10-04 19:54:02,042 - INFO - evaluating now!
2023-10-04 19:54:02,042 - INFO - Epoch [60/4000] train_loss: 21.7483, lr: 0.000500, 0.04s
2023-10-04 19:54:02,082 - INFO - epoch complete!
2023-10-04 19:54:02,082 - INFO - evaluating now!
2023-10-04 19:54:02,082 - INFO - Epoch [61/4000] train_loss: 20.8508, lr: 0.000500, 0.04s
2023-10-04 19:54:02,122 - INFO - epoch complete!
2023-10-04 19:54:02,122 - INFO - evaluating now!
2023-10-04 19:54:02,122 - INFO - Epoch [62/4000] train_loss: 20.2750, lr: 0.000500, 0.04s
2023-10-04 19:54:02,353 - INFO - Saved model at 62
2023-10-04 19:54:02,353 - INFO - Val loss decrease from 20.3407 to 20.2750, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch62.tar
2023-10-04 19:54:02,394 - INFO - epoch complete!
2023-10-04 19:54:02,394 - INFO - evaluating now!
2023-10-04 19:54:02,394 - INFO - Epoch [63/4000] train_loss: 22.5001, lr: 0.000500, 0.04s
2023-10-04 19:54:02,434 - INFO - epoch complete!
2023-10-04 19:54:02,434 - INFO - evaluating now!
2023-10-04 19:54:02,434 - INFO - Epoch [64/4000] train_loss: 19.8755, lr: 0.000500, 0.04s
2023-10-04 19:54:02,667 - INFO - Saved model at 64
2023-10-04 19:54:02,667 - INFO - Val loss decrease from 20.2750 to 19.8755, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch64.tar
2023-10-04 19:54:02,708 - INFO - epoch complete!
2023-10-04 19:54:02,708 - INFO - evaluating now!
2023-10-04 19:54:02,708 - INFO - Epoch [65/4000] train_loss: 20.5901, lr: 0.000500, 0.04s
2023-10-04 19:54:02,750 - INFO - epoch complete!
2023-10-04 19:54:02,750 - INFO - evaluating now!
2023-10-04 19:54:02,750 - INFO - Epoch [66/4000] train_loss: 20.3759, lr: 0.000500, 0.04s
2023-10-04 19:54:02,789 - INFO - epoch complete!
2023-10-04 19:54:02,790 - INFO - evaluating now!
2023-10-04 19:54:02,790 - INFO - Epoch [67/4000] train_loss: 19.7499, lr: 0.000500, 0.04s
2023-10-04 19:54:03,020 - INFO - Saved model at 67
2023-10-04 19:54:03,021 - INFO - Val loss decrease from 19.8755 to 19.7499, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch67.tar
2023-10-04 19:54:03,060 - INFO - epoch complete!
2023-10-04 19:54:03,061 - INFO - evaluating now!
2023-10-04 19:54:03,061 - INFO - Epoch [68/4000] train_loss: 20.0025, lr: 0.000500, 0.04s
2023-10-04 19:54:03,100 - INFO - epoch complete!
2023-10-04 19:54:03,101 - INFO - evaluating now!
2023-10-04 19:54:03,101 - INFO - Epoch [69/4000] train_loss: 19.9730, lr: 0.000500, 0.04s
2023-10-04 19:54:03,140 - INFO - epoch complete!
2023-10-04 19:54:03,140 - INFO - evaluating now!
2023-10-04 19:54:03,140 - INFO - Epoch [70/4000] train_loss: 18.9370, lr: 0.000500, 0.04s
2023-10-04 19:54:03,371 - INFO - Saved model at 70
2023-10-04 19:54:03,372 - INFO - Val loss decrease from 19.7499 to 18.9370, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch70.tar
2023-10-04 19:54:03,411 - INFO - epoch complete!
2023-10-04 19:54:03,412 - INFO - evaluating now!
2023-10-04 19:54:03,412 - INFO - Epoch [71/4000] train_loss: 19.0118, lr: 0.000500, 0.04s
2023-10-04 19:54:03,452 - INFO - epoch complete!
2023-10-04 19:54:03,452 - INFO - evaluating now!
2023-10-04 19:54:03,452 - INFO - Epoch [72/4000] train_loss: 19.2307, lr: 0.000500, 0.04s
2023-10-04 19:54:03,491 - INFO - epoch complete!
2023-10-04 19:54:03,491 - INFO - evaluating now!
2023-10-04 19:54:03,491 - INFO - Epoch [73/4000] train_loss: 19.5247, lr: 0.000500, 0.04s
2023-10-04 19:54:03,530 - INFO - epoch complete!
2023-10-04 19:54:03,530 - INFO - evaluating now!
2023-10-04 19:54:03,530 - INFO - Epoch [74/4000] train_loss: 21.1180, lr: 0.000500, 0.04s
2023-10-04 19:54:03,571 - INFO - epoch complete!
2023-10-04 19:54:03,572 - INFO - evaluating now!
2023-10-04 19:54:03,572 - INFO - Epoch [75/4000] train_loss: 18.3087, lr: 0.000500, 0.04s
2023-10-04 19:54:03,805 - INFO - Saved model at 75
2023-10-04 19:54:03,805 - INFO - Val loss decrease from 18.9370 to 18.3087, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch75.tar
2023-10-04 19:54:03,846 - INFO - epoch complete!
2023-10-04 19:54:03,846 - INFO - evaluating now!
2023-10-04 19:54:03,846 - INFO - Epoch [76/4000] train_loss: 18.8993, lr: 0.000500, 0.04s
2023-10-04 19:54:03,888 - INFO - epoch complete!
2023-10-04 19:54:03,889 - INFO - evaluating now!
2023-10-04 19:54:03,889 - INFO - Epoch [77/4000] train_loss: 18.4479, lr: 0.000500, 0.04s
2023-10-04 19:54:03,928 - INFO - epoch complete!
2023-10-04 19:54:03,928 - INFO - evaluating now!
2023-10-04 19:54:03,928 - INFO - Epoch [78/4000] train_loss: 17.8980, lr: 0.000500, 0.04s
2023-10-04 19:54:04,159 - INFO - Saved model at 78
2023-10-04 19:54:04,159 - INFO - Val loss decrease from 18.3087 to 17.8980, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch78.tar
2023-10-04 19:54:04,199 - INFO - epoch complete!
2023-10-04 19:54:04,200 - INFO - evaluating now!
2023-10-04 19:54:04,200 - INFO - Epoch [79/4000] train_loss: 19.7308, lr: 0.000500, 0.04s
2023-10-04 19:54:04,239 - INFO - epoch complete!
2023-10-04 19:54:04,239 - INFO - evaluating now!
2023-10-04 19:54:04,239 - INFO - Epoch [80/4000] train_loss: 19.1041, lr: 0.000500, 0.04s
2023-10-04 19:54:04,278 - INFO - epoch complete!
2023-10-04 19:54:04,279 - INFO - evaluating now!
2023-10-04 19:54:04,279 - INFO - Epoch [81/4000] train_loss: 18.4716, lr: 0.000500, 0.04s
2023-10-04 19:54:04,318 - INFO - epoch complete!
2023-10-04 19:54:04,318 - INFO - evaluating now!
2023-10-04 19:54:04,318 - INFO - Epoch [82/4000] train_loss: 19.2196, lr: 0.000500, 0.04s
2023-10-04 19:54:04,358 - INFO - epoch complete!
2023-10-04 19:54:04,358 - INFO - evaluating now!
2023-10-04 19:54:04,358 - INFO - Epoch [83/4000] train_loss: 18.1970, lr: 0.000500, 0.04s
2023-10-04 19:54:04,397 - INFO - epoch complete!
2023-10-04 19:54:04,397 - INFO - evaluating now!
2023-10-04 19:54:04,397 - INFO - Epoch [84/4000] train_loss: 19.1364, lr: 0.000500, 0.04s
2023-10-04 19:54:04,437 - INFO - epoch complete!
2023-10-04 19:54:04,437 - INFO - evaluating now!
2023-10-04 19:54:04,437 - INFO - Epoch [85/4000] train_loss: 16.5932, lr: 0.000500, 0.04s
2023-10-04 19:54:04,668 - INFO - Saved model at 85
2023-10-04 19:54:04,668 - INFO - Val loss decrease from 17.8980 to 16.5932, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch85.tar
2023-10-04 19:54:04,708 - INFO - epoch complete!
2023-10-04 19:54:04,708 - INFO - evaluating now!
2023-10-04 19:54:04,708 - INFO - Epoch [86/4000] train_loss: 17.6970, lr: 0.000500, 0.04s
2023-10-04 19:54:04,747 - INFO - epoch complete!
2023-10-04 19:54:04,748 - INFO - evaluating now!
2023-10-04 19:54:04,748 - INFO - Epoch [87/4000] train_loss: 17.6094, lr: 0.000500, 0.04s
2023-10-04 19:54:04,787 - INFO - epoch complete!
2023-10-04 19:54:04,787 - INFO - evaluating now!
2023-10-04 19:54:04,787 - INFO - Epoch [88/4000] train_loss: 17.0260, lr: 0.000500, 0.04s
2023-10-04 19:54:04,827 - INFO - epoch complete!
2023-10-04 19:54:04,827 - INFO - evaluating now!
2023-10-04 19:54:04,827 - INFO - Epoch [89/4000] train_loss: 16.7508, lr: 0.000500, 0.04s
2023-10-04 19:54:04,866 - INFO - epoch complete!
2023-10-04 19:54:04,866 - INFO - evaluating now!
2023-10-04 19:54:04,867 - INFO - Epoch [90/4000] train_loss: 17.6448, lr: 0.000500, 0.04s
2023-10-04 19:54:04,905 - INFO - epoch complete!
2023-10-04 19:54:04,906 - INFO - evaluating now!
2023-10-04 19:54:04,906 - INFO - Epoch [91/4000] train_loss: 15.8616, lr: 0.000500, 0.04s
2023-10-04 19:54:05,134 - INFO - Saved model at 91
2023-10-04 19:54:05,135 - INFO - Val loss decrease from 16.5932 to 15.8616, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch91.tar
2023-10-04 19:54:05,174 - INFO - epoch complete!
2023-10-04 19:54:05,175 - INFO - evaluating now!
2023-10-04 19:54:05,175 - INFO - Epoch [92/4000] train_loss: 16.9484, lr: 0.000500, 0.04s
2023-10-04 19:54:05,215 - INFO - epoch complete!
2023-10-04 19:54:05,215 - INFO - evaluating now!
2023-10-04 19:54:05,215 - INFO - Epoch [93/4000] train_loss: 16.4893, lr: 0.000500, 0.04s
2023-10-04 19:54:05,255 - INFO - epoch complete!
2023-10-04 19:54:05,255 - INFO - evaluating now!
2023-10-04 19:54:05,255 - INFO - Epoch [94/4000] train_loss: 16.9986, lr: 0.000500, 0.04s
2023-10-04 19:54:05,295 - INFO - epoch complete!
2023-10-04 19:54:05,295 - INFO - evaluating now!
2023-10-04 19:54:05,295 - INFO - Epoch [95/4000] train_loss: 17.1852, lr: 0.000500, 0.04s
2023-10-04 19:54:05,335 - INFO - epoch complete!
2023-10-04 19:54:05,335 - INFO - evaluating now!
2023-10-04 19:54:05,335 - INFO - Epoch [96/4000] train_loss: 17.4321, lr: 0.000500, 0.04s
2023-10-04 19:54:05,375 - INFO - epoch complete!
2023-10-04 19:54:05,375 - INFO - evaluating now!
2023-10-04 19:54:05,375 - INFO - Epoch [97/4000] train_loss: 17.0893, lr: 0.000500, 0.04s
2023-10-04 19:54:05,414 - INFO - epoch complete!
2023-10-04 19:54:05,414 - INFO - evaluating now!
2023-10-04 19:54:05,415 - INFO - Epoch [98/4000] train_loss: 16.2964, lr: 0.000500, 0.04s
2023-10-04 19:54:05,454 - INFO - epoch complete!
2023-10-04 19:54:05,454 - INFO - evaluating now!
2023-10-04 19:54:05,454 - INFO - Epoch [99/4000] train_loss: 16.5275, lr: 0.000500, 0.04s
2023-10-04 19:54:05,493 - INFO - epoch complete!
2023-10-04 19:54:05,493 - INFO - evaluating now!
2023-10-04 19:54:05,494 - INFO - Epoch [100/4000] train_loss: 16.1589, lr: 0.000500, 0.04s
2023-10-04 19:54:05,532 - INFO - epoch complete!
2023-10-04 19:54:05,532 - INFO - evaluating now!
2023-10-04 19:54:05,532 - INFO - Epoch [101/4000] train_loss: 15.2384, lr: 0.000500, 0.04s
2023-10-04 19:54:05,763 - INFO - Saved model at 101
2023-10-04 19:54:05,763 - INFO - Val loss decrease from 15.8616 to 15.2384, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch101.tar
2023-10-04 19:54:05,805 - INFO - epoch complete!
2023-10-04 19:54:05,805 - INFO - evaluating now!
2023-10-04 19:54:05,805 - INFO - Epoch [102/4000] train_loss: 15.8249, lr: 0.000500, 0.04s
2023-10-04 19:54:05,845 - INFO - epoch complete!
2023-10-04 19:54:05,845 - INFO - evaluating now!
2023-10-04 19:54:05,845 - INFO - Epoch [103/4000] train_loss: 16.3811, lr: 0.000500, 0.04s
2023-10-04 19:54:05,885 - INFO - epoch complete!
2023-10-04 19:54:05,885 - INFO - evaluating now!
2023-10-04 19:54:05,885 - INFO - Epoch [104/4000] train_loss: 14.8059, lr: 0.000500, 0.04s
2023-10-04 19:54:06,116 - INFO - Saved model at 104
2023-10-04 19:54:06,116 - INFO - Val loss decrease from 15.2384 to 14.8059, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch104.tar
2023-10-04 19:54:06,157 - INFO - epoch complete!
2023-10-04 19:54:06,158 - INFO - evaluating now!
2023-10-04 19:54:06,158 - INFO - Epoch [105/4000] train_loss: 16.1459, lr: 0.000500, 0.04s
2023-10-04 19:54:06,198 - INFO - epoch complete!
2023-10-04 19:54:06,198 - INFO - evaluating now!
2023-10-04 19:54:06,198 - INFO - Epoch [106/4000] train_loss: 15.8053, lr: 0.000500, 0.04s
2023-10-04 19:54:06,238 - INFO - epoch complete!
2023-10-04 19:54:06,238 - INFO - evaluating now!
2023-10-04 19:54:06,238 - INFO - Epoch [107/4000] train_loss: 15.6418, lr: 0.000500, 0.04s
2023-10-04 19:54:06,278 - INFO - epoch complete!
2023-10-04 19:54:06,278 - INFO - evaluating now!
2023-10-04 19:54:06,278 - INFO - Epoch [108/4000] train_loss: 15.4035, lr: 0.000500, 0.04s
2023-10-04 19:54:06,318 - INFO - epoch complete!
2023-10-04 19:54:06,318 - INFO - evaluating now!
2023-10-04 19:54:06,318 - INFO - Epoch [109/4000] train_loss: 15.2023, lr: 0.000500, 0.04s
2023-10-04 19:54:06,358 - INFO - epoch complete!
2023-10-04 19:54:06,358 - INFO - evaluating now!
2023-10-04 19:54:06,358 - INFO - Epoch [110/4000] train_loss: 15.3703, lr: 0.000500, 0.04s
2023-10-04 19:54:06,398 - INFO - epoch complete!
2023-10-04 19:54:06,398 - INFO - evaluating now!
2023-10-04 19:54:06,398 - INFO - Epoch [111/4000] train_loss: 14.8611, lr: 0.000500, 0.04s
2023-10-04 19:54:06,438 - INFO - epoch complete!
2023-10-04 19:54:06,438 - INFO - evaluating now!
2023-10-04 19:54:06,438 - INFO - Epoch [112/4000] train_loss: 16.9248, lr: 0.000500, 0.04s
2023-10-04 19:54:06,478 - INFO - epoch complete!
2023-10-04 19:54:06,479 - INFO - evaluating now!
2023-10-04 19:54:06,479 - INFO - Epoch [113/4000] train_loss: 15.7039, lr: 0.000500, 0.04s
2023-10-04 19:54:06,518 - INFO - epoch complete!
2023-10-04 19:54:06,519 - INFO - evaluating now!
2023-10-04 19:54:06,519 - INFO - Epoch [114/4000] train_loss: 16.3759, lr: 0.000500, 0.04s
2023-10-04 19:54:06,558 - INFO - epoch complete!
2023-10-04 19:54:06,559 - INFO - evaluating now!
2023-10-04 19:54:06,559 - INFO - Epoch [115/4000] train_loss: 15.7706, lr: 0.000500, 0.04s
2023-10-04 19:54:06,600 - INFO - epoch complete!
2023-10-04 19:54:06,601 - INFO - evaluating now!
2023-10-04 19:54:06,601 - INFO - Epoch [116/4000] train_loss: 15.2899, lr: 0.000500, 0.04s
2023-10-04 19:54:06,640 - INFO - epoch complete!
2023-10-04 19:54:06,640 - INFO - evaluating now!
2023-10-04 19:54:06,640 - INFO - Epoch [117/4000] train_loss: 15.2952, lr: 0.000500, 0.04s
2023-10-04 19:54:06,679 - INFO - epoch complete!
2023-10-04 19:54:06,679 - INFO - evaluating now!
2023-10-04 19:54:06,679 - INFO - Epoch [118/4000] train_loss: 15.0687, lr: 0.000500, 0.04s
2023-10-04 19:54:06,718 - INFO - epoch complete!
2023-10-04 19:54:06,718 - INFO - evaluating now!
2023-10-04 19:54:06,718 - INFO - Epoch [119/4000] train_loss: 14.2459, lr: 0.000500, 0.04s
2023-10-04 19:54:06,949 - INFO - Saved model at 119
2023-10-04 19:54:06,949 - INFO - Val loss decrease from 14.8059 to 14.2459, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch119.tar
2023-10-04 19:54:06,989 - INFO - epoch complete!
2023-10-04 19:54:06,990 - INFO - evaluating now!
2023-10-04 19:54:06,990 - INFO - Epoch [120/4000] train_loss: 15.1749, lr: 0.000500, 0.04s
2023-10-04 19:54:07,029 - INFO - epoch complete!
2023-10-04 19:54:07,030 - INFO - evaluating now!
2023-10-04 19:54:07,030 - INFO - Epoch [121/4000] train_loss: 15.8408, lr: 0.000500, 0.04s
2023-10-04 19:54:07,069 - INFO - epoch complete!
2023-10-04 19:54:07,069 - INFO - evaluating now!
2023-10-04 19:54:07,069 - INFO - Epoch [122/4000] train_loss: 15.2426, lr: 0.000500, 0.04s
2023-10-04 19:54:07,109 - INFO - epoch complete!
2023-10-04 19:54:07,109 - INFO - evaluating now!
2023-10-04 19:54:07,109 - INFO - Epoch [123/4000] train_loss: 14.9324, lr: 0.000500, 0.04s
2023-10-04 19:54:07,148 - INFO - epoch complete!
2023-10-04 19:54:07,148 - INFO - evaluating now!
2023-10-04 19:54:07,149 - INFO - Epoch [124/4000] train_loss: 14.5885, lr: 0.000500, 0.04s
2023-10-04 19:54:07,187 - INFO - epoch complete!
2023-10-04 19:54:07,187 - INFO - evaluating now!
2023-10-04 19:54:07,187 - INFO - Epoch [125/4000] train_loss: 14.6489, lr: 0.000500, 0.04s
2023-10-04 19:54:07,225 - INFO - epoch complete!
2023-10-04 19:54:07,226 - INFO - evaluating now!
2023-10-04 19:54:07,226 - INFO - Epoch [126/4000] train_loss: 13.7536, lr: 0.000500, 0.04s
2023-10-04 19:54:07,456 - INFO - Saved model at 126
2023-10-04 19:54:07,456 - INFO - Val loss decrease from 14.2459 to 13.7536, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch126.tar
2023-10-04 19:54:07,497 - INFO - epoch complete!
2023-10-04 19:54:07,498 - INFO - evaluating now!
2023-10-04 19:54:07,498 - INFO - Epoch [127/4000] train_loss: 14.5830, lr: 0.000500, 0.04s
2023-10-04 19:54:07,537 - INFO - epoch complete!
2023-10-04 19:54:07,537 - INFO - evaluating now!
2023-10-04 19:54:07,537 - INFO - Epoch [128/4000] train_loss: 15.4103, lr: 0.000500, 0.04s
2023-10-04 19:54:07,576 - INFO - epoch complete!
2023-10-04 19:54:07,577 - INFO - evaluating now!
2023-10-04 19:54:07,577 - INFO - Epoch [129/4000] train_loss: 14.4546, lr: 0.000500, 0.04s
2023-10-04 19:54:07,615 - INFO - epoch complete!
2023-10-04 19:54:07,616 - INFO - evaluating now!
2023-10-04 19:54:07,616 - INFO - Epoch [130/4000] train_loss: 13.8367, lr: 0.000500, 0.04s
2023-10-04 19:54:07,655 - INFO - epoch complete!
2023-10-04 19:54:07,655 - INFO - evaluating now!
2023-10-04 19:54:07,656 - INFO - Epoch [131/4000] train_loss: 14.8534, lr: 0.000500, 0.04s
2023-10-04 19:54:07,694 - INFO - epoch complete!
2023-10-04 19:54:07,694 - INFO - evaluating now!
2023-10-04 19:54:07,694 - INFO - Epoch [132/4000] train_loss: 15.0723, lr: 0.000500, 0.04s
2023-10-04 19:54:07,733 - INFO - epoch complete!
2023-10-04 19:54:07,733 - INFO - evaluating now!
2023-10-04 19:54:07,733 - INFO - Epoch [133/4000] train_loss: 14.0948, lr: 0.000500, 0.04s
2023-10-04 19:54:07,772 - INFO - epoch complete!
2023-10-04 19:54:07,772 - INFO - evaluating now!
2023-10-04 19:54:07,772 - INFO - Epoch [134/4000] train_loss: 13.6413, lr: 0.000500, 0.04s
2023-10-04 19:54:08,002 - INFO - Saved model at 134
2023-10-04 19:54:08,003 - INFO - Val loss decrease from 13.7536 to 13.6413, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch134.tar
2023-10-04 19:54:08,044 - INFO - epoch complete!
2023-10-04 19:54:08,044 - INFO - evaluating now!
2023-10-04 19:54:08,044 - INFO - Epoch [135/4000] train_loss: 13.3648, lr: 0.000500, 0.04s
2023-10-04 19:54:08,275 - INFO - Saved model at 135
2023-10-04 19:54:08,275 - INFO - Val loss decrease from 13.6413 to 13.3648, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch135.tar
2023-10-04 19:54:08,314 - INFO - epoch complete!
2023-10-04 19:54:08,314 - INFO - evaluating now!
2023-10-04 19:54:08,314 - INFO - Epoch [136/4000] train_loss: 14.3003, lr: 0.000500, 0.04s
2023-10-04 19:54:08,353 - INFO - epoch complete!
2023-10-04 19:54:08,354 - INFO - evaluating now!
2023-10-04 19:54:08,354 - INFO - Epoch [137/4000] train_loss: 14.1589, lr: 0.000500, 0.04s
2023-10-04 19:54:08,392 - INFO - epoch complete!
2023-10-04 19:54:08,393 - INFO - evaluating now!
2023-10-04 19:54:08,393 - INFO - Epoch [138/4000] train_loss: 13.2385, lr: 0.000500, 0.04s
2023-10-04 19:54:08,623 - INFO - Saved model at 138
2023-10-04 19:54:08,623 - INFO - Val loss decrease from 13.3648 to 13.2385, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch138.tar
2023-10-04 19:54:08,663 - INFO - epoch complete!
2023-10-04 19:54:08,663 - INFO - evaluating now!
2023-10-04 19:54:08,663 - INFO - Epoch [139/4000] train_loss: 14.7971, lr: 0.000500, 0.04s
2023-10-04 19:54:08,703 - INFO - epoch complete!
2023-10-04 19:54:08,703 - INFO - evaluating now!
2023-10-04 19:54:08,703 - INFO - Epoch [140/4000] train_loss: 12.9289, lr: 0.000500, 0.04s
2023-10-04 19:54:08,933 - INFO - Saved model at 140
2023-10-04 19:54:08,934 - INFO - Val loss decrease from 13.2385 to 12.9289, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch140.tar
2023-10-04 19:54:08,974 - INFO - epoch complete!
2023-10-04 19:54:08,974 - INFO - evaluating now!
2023-10-04 19:54:08,974 - INFO - Epoch [141/4000] train_loss: 13.6395, lr: 0.000500, 0.04s
2023-10-04 19:54:09,014 - INFO - epoch complete!
2023-10-04 19:54:09,014 - INFO - evaluating now!
2023-10-04 19:54:09,014 - INFO - Epoch [142/4000] train_loss: 14.5125, lr: 0.000500, 0.04s
2023-10-04 19:54:09,054 - INFO - epoch complete!
2023-10-04 19:54:09,054 - INFO - evaluating now!
2023-10-04 19:54:09,054 - INFO - Epoch [143/4000] train_loss: 13.9887, lr: 0.000500, 0.04s
2023-10-04 19:54:09,093 - INFO - epoch complete!
2023-10-04 19:54:09,093 - INFO - evaluating now!
2023-10-04 19:54:09,094 - INFO - Epoch [144/4000] train_loss: 13.7701, lr: 0.000500, 0.04s
2023-10-04 19:54:09,133 - INFO - epoch complete!
2023-10-04 19:54:09,133 - INFO - evaluating now!
2023-10-04 19:54:09,133 - INFO - Epoch [145/4000] train_loss: 14.2510, lr: 0.000500, 0.04s
2023-10-04 19:54:09,174 - INFO - epoch complete!
2023-10-04 19:54:09,174 - INFO - evaluating now!
2023-10-04 19:54:09,174 - INFO - Epoch [146/4000] train_loss: 12.3628, lr: 0.000500, 0.04s
2023-10-04 19:54:09,405 - INFO - Saved model at 146
2023-10-04 19:54:09,405 - INFO - Val loss decrease from 12.9289 to 12.3628, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch146.tar
2023-10-04 19:54:09,445 - INFO - epoch complete!
2023-10-04 19:54:09,445 - INFO - evaluating now!
2023-10-04 19:54:09,445 - INFO - Epoch [147/4000] train_loss: 13.2940, lr: 0.000500, 0.04s
2023-10-04 19:54:09,485 - INFO - epoch complete!
2023-10-04 19:54:09,486 - INFO - evaluating now!
2023-10-04 19:54:09,486 - INFO - Epoch [148/4000] train_loss: 13.9096, lr: 0.000500, 0.04s
2023-10-04 19:54:09,525 - INFO - epoch complete!
2023-10-04 19:54:09,525 - INFO - evaluating now!
2023-10-04 19:54:09,525 - INFO - Epoch [149/4000] train_loss: 13.9289, lr: 0.000500, 0.04s
2023-10-04 19:54:09,564 - INFO - epoch complete!
2023-10-04 19:54:09,564 - INFO - evaluating now!
2023-10-04 19:54:09,564 - INFO - Epoch [150/4000] train_loss: 12.7888, lr: 0.000500, 0.04s
2023-10-04 19:54:09,603 - INFO - epoch complete!
2023-10-04 19:54:09,604 - INFO - evaluating now!
2023-10-04 19:54:09,604 - INFO - Epoch [151/4000] train_loss: 14.7178, lr: 0.000500, 0.04s
2023-10-04 19:54:09,643 - INFO - epoch complete!
2023-10-04 19:54:09,643 - INFO - evaluating now!
2023-10-04 19:54:09,643 - INFO - Epoch [152/4000] train_loss: 13.0588, lr: 0.000500, 0.04s
2023-10-04 19:54:09,682 - INFO - epoch complete!
2023-10-04 19:54:09,682 - INFO - evaluating now!
2023-10-04 19:54:09,682 - INFO - Epoch [153/4000] train_loss: 12.6609, lr: 0.000500, 0.04s
2023-10-04 19:54:09,721 - INFO - epoch complete!
2023-10-04 19:54:09,721 - INFO - evaluating now!
2023-10-04 19:54:09,721 - INFO - Epoch [154/4000] train_loss: 12.6164, lr: 0.000500, 0.04s
2023-10-04 19:54:09,760 - INFO - epoch complete!
2023-10-04 19:54:09,760 - INFO - evaluating now!
2023-10-04 19:54:09,761 - INFO - Epoch [155/4000] train_loss: 13.2898, lr: 0.000500, 0.04s
2023-10-04 19:54:09,799 - INFO - epoch complete!
2023-10-04 19:54:09,800 - INFO - evaluating now!
2023-10-04 19:54:09,800 - INFO - Epoch [156/4000] train_loss: 13.2103, lr: 0.000500, 0.04s
2023-10-04 19:54:09,838 - INFO - epoch complete!
2023-10-04 19:54:09,839 - INFO - evaluating now!
2023-10-04 19:54:09,839 - INFO - Epoch [157/4000] train_loss: 12.9117, lr: 0.000500, 0.04s
2023-10-04 19:54:09,877 - INFO - epoch complete!
2023-10-04 19:54:09,878 - INFO - evaluating now!
2023-10-04 19:54:09,878 - INFO - Epoch [158/4000] train_loss: 11.7270, lr: 0.000500, 0.04s
2023-10-04 19:54:10,108 - INFO - Saved model at 158
2023-10-04 19:54:10,108 - INFO - Val loss decrease from 12.3628 to 11.7270, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch158.tar
2023-10-04 19:54:10,147 - INFO - epoch complete!
2023-10-04 19:54:10,148 - INFO - evaluating now!
2023-10-04 19:54:10,148 - INFO - Epoch [159/4000] train_loss: 13.8064, lr: 0.000500, 0.04s
2023-10-04 19:54:10,186 - INFO - epoch complete!
2023-10-04 19:54:10,186 - INFO - evaluating now!
2023-10-04 19:54:10,187 - INFO - Epoch [160/4000] train_loss: 13.0732, lr: 0.000500, 0.04s
2023-10-04 19:54:10,227 - INFO - epoch complete!
2023-10-04 19:54:10,228 - INFO - evaluating now!
2023-10-04 19:54:10,228 - INFO - Epoch [161/4000] train_loss: 12.4430, lr: 0.000500, 0.04s
2023-10-04 19:54:10,268 - INFO - epoch complete!
2023-10-04 19:54:10,268 - INFO - evaluating now!
2023-10-04 19:54:10,268 - INFO - Epoch [162/4000] train_loss: 12.8837, lr: 0.000500, 0.04s
2023-10-04 19:54:10,307 - INFO - epoch complete!
2023-10-04 19:54:10,307 - INFO - evaluating now!
2023-10-04 19:54:10,307 - INFO - Epoch [163/4000] train_loss: 13.3998, lr: 0.000500, 0.04s
2023-10-04 19:54:10,346 - INFO - epoch complete!
2023-10-04 19:54:10,346 - INFO - evaluating now!
2023-10-04 19:54:10,346 - INFO - Epoch [164/4000] train_loss: 13.2267, lr: 0.000500, 0.04s
2023-10-04 19:54:10,385 - INFO - epoch complete!
2023-10-04 19:54:10,385 - INFO - evaluating now!
2023-10-04 19:54:10,385 - INFO - Epoch [165/4000] train_loss: 12.1290, lr: 0.000500, 0.04s
2023-10-04 19:54:10,425 - INFO - epoch complete!
2023-10-04 19:54:10,426 - INFO - evaluating now!
2023-10-04 19:54:10,426 - INFO - Epoch [166/4000] train_loss: 12.5718, lr: 0.000500, 0.04s
2023-10-04 19:54:10,465 - INFO - epoch complete!
2023-10-04 19:54:10,465 - INFO - evaluating now!
2023-10-04 19:54:10,466 - INFO - Epoch [167/4000] train_loss: 12.0151, lr: 0.000500, 0.04s
2023-10-04 19:54:10,504 - INFO - epoch complete!
2023-10-04 19:54:10,504 - INFO - evaluating now!
2023-10-04 19:54:10,505 - INFO - Epoch [168/4000] train_loss: 11.9582, lr: 0.000500, 0.04s
2023-10-04 19:54:10,543 - INFO - epoch complete!
2023-10-04 19:54:10,543 - INFO - evaluating now!
2023-10-04 19:54:10,544 - INFO - Epoch [169/4000] train_loss: 13.1588, lr: 0.000500, 0.04s
2023-10-04 19:54:10,582 - INFO - epoch complete!
2023-10-04 19:54:10,582 - INFO - evaluating now!
2023-10-04 19:54:10,583 - INFO - Epoch [170/4000] train_loss: 12.8353, lr: 0.000500, 0.04s
2023-10-04 19:54:10,621 - INFO - epoch complete!
2023-10-04 19:54:10,621 - INFO - evaluating now!
2023-10-04 19:54:10,621 - INFO - Epoch [171/4000] train_loss: 13.9220, lr: 0.000500, 0.04s
2023-10-04 19:54:10,660 - INFO - epoch complete!
2023-10-04 19:54:10,660 - INFO - evaluating now!
2023-10-04 19:54:10,660 - INFO - Epoch [172/4000] train_loss: 12.8898, lr: 0.000500, 0.04s
2023-10-04 19:54:10,699 - INFO - epoch complete!
2023-10-04 19:54:10,699 - INFO - evaluating now!
2023-10-04 19:54:10,699 - INFO - Epoch [173/4000] train_loss: 12.6011, lr: 0.000500, 0.04s
2023-10-04 19:54:10,738 - INFO - epoch complete!
2023-10-04 19:54:10,738 - INFO - evaluating now!
2023-10-04 19:54:10,738 - INFO - Epoch [174/4000] train_loss: 12.3973, lr: 0.000500, 0.04s
2023-10-04 19:54:10,777 - INFO - epoch complete!
2023-10-04 19:54:10,777 - INFO - evaluating now!
2023-10-04 19:54:10,777 - INFO - Epoch [175/4000] train_loss: 12.3849, lr: 0.000500, 0.04s
2023-10-04 19:54:10,817 - INFO - epoch complete!
2023-10-04 19:54:10,818 - INFO - evaluating now!
2023-10-04 19:54:10,818 - INFO - Epoch [176/4000] train_loss: 12.4013, lr: 0.000500, 0.04s
2023-10-04 19:54:10,856 - INFO - epoch complete!
2023-10-04 19:54:10,857 - INFO - evaluating now!
2023-10-04 19:54:10,857 - INFO - Epoch [177/4000] train_loss: 12.3291, lr: 0.000500, 0.04s
2023-10-04 19:54:10,895 - INFO - epoch complete!
2023-10-04 19:54:10,896 - INFO - evaluating now!
2023-10-04 19:54:10,896 - INFO - Epoch [178/4000] train_loss: 12.6867, lr: 0.000500, 0.04s
2023-10-04 19:54:10,934 - INFO - epoch complete!
2023-10-04 19:54:10,935 - INFO - evaluating now!
2023-10-04 19:54:10,935 - INFO - Epoch [179/4000] train_loss: 12.5403, lr: 0.000500, 0.04s
2023-10-04 19:54:10,974 - INFO - epoch complete!
2023-10-04 19:54:10,974 - INFO - evaluating now!
2023-10-04 19:54:10,974 - INFO - Epoch [180/4000] train_loss: 12.0477, lr: 0.000500, 0.04s
2023-10-04 19:54:11,013 - INFO - epoch complete!
2023-10-04 19:54:11,013 - INFO - evaluating now!
2023-10-04 19:54:11,013 - INFO - Epoch [181/4000] train_loss: 12.0165, lr: 0.000500, 0.04s
2023-10-04 19:54:11,052 - INFO - epoch complete!
2023-10-04 19:54:11,052 - INFO - evaluating now!
2023-10-04 19:54:11,052 - INFO - Epoch [182/4000] train_loss: 11.2607, lr: 0.000500, 0.04s
2023-10-04 19:54:11,283 - INFO - Saved model at 182
2023-10-04 19:54:11,283 - INFO - Val loss decrease from 11.7270 to 11.2607, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch182.tar
2023-10-04 19:54:11,323 - INFO - epoch complete!
2023-10-04 19:54:11,323 - INFO - evaluating now!
2023-10-04 19:54:11,324 - INFO - Epoch [183/4000] train_loss: 11.8857, lr: 0.000500, 0.04s
2023-10-04 19:54:11,363 - INFO - epoch complete!
2023-10-04 19:54:11,364 - INFO - evaluating now!
2023-10-04 19:54:11,364 - INFO - Epoch [184/4000] train_loss: 12.0758, lr: 0.000500, 0.04s
2023-10-04 19:54:11,403 - INFO - epoch complete!
2023-10-04 19:54:11,404 - INFO - evaluating now!
2023-10-04 19:54:11,404 - INFO - Epoch [185/4000] train_loss: 11.8515, lr: 0.000500, 0.04s
2023-10-04 19:54:11,443 - INFO - epoch complete!
2023-10-04 19:54:11,444 - INFO - evaluating now!
2023-10-04 19:54:11,444 - INFO - Epoch [186/4000] train_loss: 12.1622, lr: 0.000500, 0.04s
2023-10-04 19:54:11,483 - INFO - epoch complete!
2023-10-04 19:54:11,484 - INFO - evaluating now!
2023-10-04 19:54:11,484 - INFO - Epoch [187/4000] train_loss: 12.1144, lr: 0.000500, 0.04s
2023-10-04 19:54:11,523 - INFO - epoch complete!
2023-10-04 19:54:11,524 - INFO - evaluating now!
2023-10-04 19:54:11,524 - INFO - Epoch [188/4000] train_loss: 11.4391, lr: 0.000500, 0.04s
2023-10-04 19:54:11,563 - INFO - epoch complete!
2023-10-04 19:54:11,564 - INFO - evaluating now!
2023-10-04 19:54:11,564 - INFO - Epoch [189/4000] train_loss: 12.3475, lr: 0.000500, 0.04s
2023-10-04 19:54:11,603 - INFO - epoch complete!
2023-10-04 19:54:11,604 - INFO - evaluating now!
2023-10-04 19:54:11,604 - INFO - Epoch [190/4000] train_loss: 11.8252, lr: 0.000500, 0.04s
2023-10-04 19:54:11,643 - INFO - epoch complete!
2023-10-04 19:54:11,643 - INFO - evaluating now!
2023-10-04 19:54:11,644 - INFO - Epoch [191/4000] train_loss: 12.1357, lr: 0.000500, 0.04s
2023-10-04 19:54:11,683 - INFO - epoch complete!
2023-10-04 19:54:11,683 - INFO - evaluating now!
2023-10-04 19:54:11,684 - INFO - Epoch [192/4000] train_loss: 11.3265, lr: 0.000500, 0.04s
2023-10-04 19:54:11,723 - INFO - epoch complete!
2023-10-04 19:54:11,723 - INFO - evaluating now!
2023-10-04 19:54:11,723 - INFO - Epoch [193/4000] train_loss: 11.6793, lr: 0.000500, 0.04s
2023-10-04 19:54:11,763 - INFO - epoch complete!
2023-10-04 19:54:11,763 - INFO - evaluating now!
2023-10-04 19:54:11,763 - INFO - Epoch [194/4000] train_loss: 12.4749, lr: 0.000500, 0.04s
2023-10-04 19:54:11,803 - INFO - epoch complete!
2023-10-04 19:54:11,803 - INFO - evaluating now!
2023-10-04 19:54:11,803 - INFO - Epoch [195/4000] train_loss: 11.6702, lr: 0.000500, 0.04s
2023-10-04 19:54:11,843 - INFO - epoch complete!
2023-10-04 19:54:11,843 - INFO - evaluating now!
2023-10-04 19:54:11,843 - INFO - Epoch [196/4000] train_loss: 12.4795, lr: 0.000500, 0.04s
2023-10-04 19:54:11,885 - INFO - epoch complete!
2023-10-04 19:54:11,885 - INFO - evaluating now!
2023-10-04 19:54:11,885 - INFO - Epoch [197/4000] train_loss: 12.4547, lr: 0.000500, 0.04s
2023-10-04 19:54:11,925 - INFO - epoch complete!
2023-10-04 19:54:11,925 - INFO - evaluating now!
2023-10-04 19:54:11,925 - INFO - Epoch [198/4000] train_loss: 12.1958, lr: 0.000500, 0.04s
2023-10-04 19:54:11,965 - INFO - epoch complete!
2023-10-04 19:54:11,965 - INFO - evaluating now!
2023-10-04 19:54:11,965 - INFO - Epoch [199/4000] train_loss: 11.3999, lr: 0.000500, 0.04s
2023-10-04 19:54:12,005 - INFO - epoch complete!
2023-10-04 19:54:12,005 - INFO - evaluating now!
2023-10-04 19:54:12,005 - INFO - Epoch [200/4000] train_loss: 11.8948, lr: 0.000500, 0.04s
2023-10-04 19:54:12,045 - INFO - epoch complete!
2023-10-04 19:54:12,045 - INFO - evaluating now!
2023-10-04 19:54:12,045 - INFO - Epoch [201/4000] train_loss: 11.8416, lr: 0.000500, 0.04s
2023-10-04 19:54:12,085 - INFO - epoch complete!
2023-10-04 19:54:12,085 - INFO - evaluating now!
2023-10-04 19:54:12,085 - INFO - Epoch [202/4000] train_loss: 11.2962, lr: 0.000500, 0.04s
2023-10-04 19:54:12,125 - INFO - epoch complete!
2023-10-04 19:54:12,125 - INFO - evaluating now!
2023-10-04 19:54:12,125 - INFO - Epoch [203/4000] train_loss: 10.8790, lr: 0.000500, 0.04s
2023-10-04 19:54:12,378 - INFO - Saved model at 203
2023-10-04 19:54:12,378 - INFO - Val loss decrease from 11.2607 to 10.8790, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch203.tar
2023-10-04 19:54:12,419 - INFO - epoch complete!
2023-10-04 19:54:12,420 - INFO - evaluating now!
2023-10-04 19:54:12,420 - INFO - Epoch [204/4000] train_loss: 10.8101, lr: 0.000500, 0.04s
2023-10-04 19:54:12,686 - INFO - Saved model at 204
2023-10-04 19:54:12,686 - INFO - Val loss decrease from 10.8790 to 10.8101, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch204.tar
2023-10-04 19:54:12,726 - INFO - epoch complete!
2023-10-04 19:54:12,727 - INFO - evaluating now!
2023-10-04 19:54:12,727 - INFO - Epoch [205/4000] train_loss: 11.2101, lr: 0.000500, 0.04s
2023-10-04 19:54:12,766 - INFO - epoch complete!
2023-10-04 19:54:12,766 - INFO - evaluating now!
2023-10-04 19:54:12,767 - INFO - Epoch [206/4000] train_loss: 11.1844, lr: 0.000500, 0.04s
2023-10-04 19:54:12,806 - INFO - epoch complete!
2023-10-04 19:54:12,806 - INFO - evaluating now!
2023-10-04 19:54:12,806 - INFO - Epoch [207/4000] train_loss: 11.0911, lr: 0.000500, 0.04s
2023-10-04 19:54:12,845 - INFO - epoch complete!
2023-10-04 19:54:12,845 - INFO - evaluating now!
2023-10-04 19:54:12,846 - INFO - Epoch [208/4000] train_loss: 10.7123, lr: 0.000500, 0.04s
2023-10-04 19:54:13,142 - INFO - Saved model at 208
2023-10-04 19:54:13,142 - INFO - Val loss decrease from 10.8101 to 10.7123, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch208.tar
2023-10-04 19:54:13,182 - INFO - epoch complete!
2023-10-04 19:54:13,182 - INFO - evaluating now!
2023-10-04 19:54:13,183 - INFO - Epoch [209/4000] train_loss: 10.5307, lr: 0.000500, 0.04s
2023-10-04 19:54:13,439 - INFO - Saved model at 209
2023-10-04 19:54:13,439 - INFO - Val loss decrease from 10.7123 to 10.5307, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch209.tar
2023-10-04 19:54:13,480 - INFO - epoch complete!
2023-10-04 19:54:13,480 - INFO - evaluating now!
2023-10-04 19:54:13,480 - INFO - Epoch [210/4000] train_loss: 10.6388, lr: 0.000500, 0.04s
2023-10-04 19:54:13,520 - INFO - epoch complete!
2023-10-04 19:54:13,520 - INFO - evaluating now!
2023-10-04 19:54:13,520 - INFO - Epoch [211/4000] train_loss: 10.6468, lr: 0.000500, 0.04s
2023-10-04 19:54:13,559 - INFO - epoch complete!
2023-10-04 19:54:13,559 - INFO - evaluating now!
2023-10-04 19:54:13,560 - INFO - Epoch [212/4000] train_loss: 10.9673, lr: 0.000500, 0.04s
2023-10-04 19:54:13,599 - INFO - epoch complete!
2023-10-04 19:54:13,599 - INFO - evaluating now!
2023-10-04 19:54:13,599 - INFO - Epoch [213/4000] train_loss: 12.0261, lr: 0.000500, 0.04s
2023-10-04 19:54:13,638 - INFO - epoch complete!
2023-10-04 19:54:13,639 - INFO - evaluating now!
2023-10-04 19:54:13,639 - INFO - Epoch [214/4000] train_loss: 11.7510, lr: 0.000500, 0.04s
2023-10-04 19:54:13,678 - INFO - epoch complete!
2023-10-04 19:54:13,678 - INFO - evaluating now!
2023-10-04 19:54:13,678 - INFO - Epoch [215/4000] train_loss: 11.4155, lr: 0.000500, 0.04s
2023-10-04 19:54:13,717 - INFO - epoch complete!
2023-10-04 19:54:13,717 - INFO - evaluating now!
2023-10-04 19:54:13,718 - INFO - Epoch [216/4000] train_loss: 11.5817, lr: 0.000500, 0.04s
2023-10-04 19:54:13,756 - INFO - epoch complete!
2023-10-04 19:54:13,756 - INFO - evaluating now!
2023-10-04 19:54:13,756 - INFO - Epoch [217/4000] train_loss: 11.0478, lr: 0.000500, 0.04s
2023-10-04 19:54:13,794 - INFO - epoch complete!
2023-10-04 19:54:13,795 - INFO - evaluating now!
2023-10-04 19:54:13,795 - INFO - Epoch [218/4000] train_loss: 11.0715, lr: 0.000500, 0.04s
2023-10-04 19:54:13,833 - INFO - epoch complete!
2023-10-04 19:54:13,833 - INFO - evaluating now!
2023-10-04 19:54:13,834 - INFO - Epoch [219/4000] train_loss: 11.6258, lr: 0.000500, 0.04s
2023-10-04 19:54:13,873 - INFO - epoch complete!
2023-10-04 19:54:13,873 - INFO - evaluating now!
2023-10-04 19:54:13,873 - INFO - Epoch [220/4000] train_loss: 10.2122, lr: 0.000500, 0.04s
2023-10-04 19:54:14,215 - INFO - Saved model at 220
2023-10-04 19:54:14,215 - INFO - Val loss decrease from 10.5307 to 10.2122, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch220.tar
2023-10-04 19:54:14,255 - INFO - epoch complete!
2023-10-04 19:54:14,255 - INFO - evaluating now!
2023-10-04 19:54:14,255 - INFO - Epoch [221/4000] train_loss: 9.9855, lr: 0.000500, 0.04s
2023-10-04 19:54:14,510 - INFO - Saved model at 221
2023-10-04 19:54:14,510 - INFO - Val loss decrease from 10.2122 to 9.9855, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch221.tar
2023-10-04 19:54:14,550 - INFO - epoch complete!
2023-10-04 19:54:14,550 - INFO - evaluating now!
2023-10-04 19:54:14,550 - INFO - Epoch [222/4000] train_loss: 10.5996, lr: 0.000500, 0.04s
2023-10-04 19:54:14,590 - INFO - epoch complete!
2023-10-04 19:54:14,590 - INFO - evaluating now!
2023-10-04 19:54:14,590 - INFO - Epoch [223/4000] train_loss: 10.3159, lr: 0.000500, 0.04s
2023-10-04 19:54:14,629 - INFO - epoch complete!
2023-10-04 19:54:14,629 - INFO - evaluating now!
2023-10-04 19:54:14,630 - INFO - Epoch [224/4000] train_loss: 11.4698, lr: 0.000500, 0.04s
2023-10-04 19:54:14,668 - INFO - epoch complete!
2023-10-04 19:54:14,669 - INFO - evaluating now!
2023-10-04 19:54:14,669 - INFO - Epoch [225/4000] train_loss: 10.4200, lr: 0.000500, 0.04s
2023-10-04 19:54:14,708 - INFO - epoch complete!
2023-10-04 19:54:14,708 - INFO - evaluating now!
2023-10-04 19:54:14,708 - INFO - Epoch [226/4000] train_loss: 9.9865, lr: 0.000500, 0.04s
2023-10-04 19:54:14,747 - INFO - epoch complete!
2023-10-04 19:54:14,747 - INFO - evaluating now!
2023-10-04 19:54:14,747 - INFO - Epoch [227/4000] train_loss: 10.6458, lr: 0.000500, 0.04s
2023-10-04 19:54:14,786 - INFO - epoch complete!
2023-10-04 19:54:14,786 - INFO - evaluating now!
2023-10-04 19:54:14,786 - INFO - Epoch [228/4000] train_loss: 11.5480, lr: 0.000500, 0.04s
2023-10-04 19:54:14,825 - INFO - epoch complete!
2023-10-04 19:54:14,826 - INFO - evaluating now!
2023-10-04 19:54:14,826 - INFO - Epoch [229/4000] train_loss: 10.3365, lr: 0.000500, 0.04s
2023-10-04 19:54:14,864 - INFO - epoch complete!
2023-10-04 19:54:14,865 - INFO - evaluating now!
2023-10-04 19:54:14,865 - INFO - Epoch [230/4000] train_loss: 10.4368, lr: 0.000500, 0.04s
2023-10-04 19:54:14,903 - INFO - epoch complete!
2023-10-04 19:54:14,903 - INFO - evaluating now!
2023-10-04 19:54:14,903 - INFO - Epoch [231/4000] train_loss: 10.8632, lr: 0.000500, 0.04s
2023-10-04 19:54:14,942 - INFO - epoch complete!
2023-10-04 19:54:14,943 - INFO - evaluating now!
2023-10-04 19:54:14,943 - INFO - Epoch [232/4000] train_loss: 10.6009, lr: 0.000500, 0.04s
2023-10-04 19:54:14,981 - INFO - epoch complete!
2023-10-04 19:54:14,982 - INFO - evaluating now!
2023-10-04 19:54:14,982 - INFO - Epoch [233/4000] train_loss: 10.2988, lr: 0.000500, 0.04s
2023-10-04 19:54:15,021 - INFO - epoch complete!
2023-10-04 19:54:15,021 - INFO - evaluating now!
2023-10-04 19:54:15,021 - INFO - Epoch [234/4000] train_loss: 10.4847, lr: 0.000500, 0.04s
2023-10-04 19:54:15,060 - INFO - epoch complete!
2023-10-04 19:54:15,060 - INFO - evaluating now!
2023-10-04 19:54:15,060 - INFO - Epoch [235/4000] train_loss: 10.1014, lr: 0.000500, 0.04s
2023-10-04 19:54:15,099 - INFO - epoch complete!
2023-10-04 19:54:15,100 - INFO - evaluating now!
2023-10-04 19:54:15,100 - INFO - Epoch [236/4000] train_loss: 10.6290, lr: 0.000500, 0.04s
2023-10-04 19:54:15,139 - INFO - epoch complete!
2023-10-04 19:54:15,139 - INFO - evaluating now!
2023-10-04 19:54:15,139 - INFO - Epoch [237/4000] train_loss: 10.5878, lr: 0.000500, 0.04s
2023-10-04 19:54:15,178 - INFO - epoch complete!
2023-10-04 19:54:15,178 - INFO - evaluating now!
2023-10-04 19:54:15,178 - INFO - Epoch [238/4000] train_loss: 10.6041, lr: 0.000500, 0.04s
2023-10-04 19:54:15,217 - INFO - epoch complete!
2023-10-04 19:54:15,217 - INFO - evaluating now!
2023-10-04 19:54:15,218 - INFO - Epoch [239/4000] train_loss: 10.4590, lr: 0.000500, 0.04s
2023-10-04 19:54:15,256 - INFO - epoch complete!
2023-10-04 19:54:15,256 - INFO - evaluating now!
2023-10-04 19:54:15,256 - INFO - Epoch [240/4000] train_loss: 10.2846, lr: 0.000500, 0.04s
2023-10-04 19:54:15,295 - INFO - epoch complete!
2023-10-04 19:54:15,296 - INFO - evaluating now!
2023-10-04 19:54:15,296 - INFO - Epoch [241/4000] train_loss: 10.6487, lr: 0.000500, 0.04s
2023-10-04 19:54:15,334 - INFO - epoch complete!
2023-10-04 19:54:15,335 - INFO - evaluating now!
2023-10-04 19:54:15,335 - INFO - Epoch [242/4000] train_loss: 11.1330, lr: 0.000500, 0.04s
2023-10-04 19:54:15,374 - INFO - epoch complete!
2023-10-04 19:54:15,374 - INFO - evaluating now!
2023-10-04 19:54:15,374 - INFO - Epoch [243/4000] train_loss: 10.5004, lr: 0.000500, 0.04s
2023-10-04 19:54:15,413 - INFO - epoch complete!
2023-10-04 19:54:15,413 - INFO - evaluating now!
2023-10-04 19:54:15,413 - INFO - Epoch [244/4000] train_loss: 11.0643, lr: 0.000500, 0.04s
2023-10-04 19:54:15,454 - INFO - epoch complete!
2023-10-04 19:54:15,454 - INFO - evaluating now!
2023-10-04 19:54:15,454 - INFO - Epoch [245/4000] train_loss: 10.2619, lr: 0.000500, 0.04s
2023-10-04 19:54:15,493 - INFO - epoch complete!
2023-10-04 19:54:15,494 - INFO - evaluating now!
2023-10-04 19:54:15,494 - INFO - Epoch [246/4000] train_loss: 10.1467, lr: 0.000500, 0.04s
2023-10-04 19:54:15,533 - INFO - epoch complete!
2023-10-04 19:54:15,533 - INFO - evaluating now!
2023-10-04 19:54:15,533 - INFO - Epoch [247/4000] train_loss: 10.2572, lr: 0.000500, 0.04s
2023-10-04 19:54:15,572 - INFO - epoch complete!
2023-10-04 19:54:15,572 - INFO - evaluating now!
2023-10-04 19:54:15,572 - INFO - Epoch [248/4000] train_loss: 10.2521, lr: 0.000500, 0.04s
2023-10-04 19:54:15,611 - INFO - epoch complete!
2023-10-04 19:54:15,611 - INFO - evaluating now!
2023-10-04 19:54:15,611 - INFO - Epoch [249/4000] train_loss: 10.3367, lr: 0.000500, 0.04s
2023-10-04 19:54:15,650 - INFO - epoch complete!
2023-10-04 19:54:15,651 - INFO - evaluating now!
2023-10-04 19:54:15,651 - INFO - Epoch [250/4000] train_loss: 10.3097, lr: 0.000500, 0.04s
2023-10-04 19:54:15,689 - INFO - epoch complete!
2023-10-04 19:54:15,690 - INFO - evaluating now!
2023-10-04 19:54:15,690 - INFO - Epoch [251/4000] train_loss: 10.2911, lr: 0.000500, 0.04s
2023-10-04 19:54:15,728 - INFO - epoch complete!
2023-10-04 19:54:15,729 - INFO - evaluating now!
2023-10-04 19:54:15,729 - INFO - Epoch [252/4000] train_loss: 10.7916, lr: 0.000500, 0.04s
2023-10-04 19:54:15,767 - INFO - epoch complete!
2023-10-04 19:54:15,767 - INFO - evaluating now!
2023-10-04 19:54:15,768 - INFO - Epoch [253/4000] train_loss: 10.8469, lr: 0.000500, 0.04s
2023-10-04 19:54:15,806 - INFO - epoch complete!
2023-10-04 19:54:15,807 - INFO - evaluating now!
2023-10-04 19:54:15,807 - INFO - Epoch [254/4000] train_loss: 9.9088, lr: 0.000500, 0.04s
2023-10-04 19:54:16,040 - INFO - Saved model at 254
2023-10-04 19:54:16,041 - INFO - Val loss decrease from 9.9855 to 9.9088, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch254.tar
2023-10-04 19:54:16,080 - INFO - epoch complete!
2023-10-04 19:54:16,081 - INFO - evaluating now!
2023-10-04 19:54:16,081 - INFO - Epoch [255/4000] train_loss: 10.1222, lr: 0.000500, 0.04s
2023-10-04 19:54:16,120 - INFO - epoch complete!
2023-10-04 19:54:16,120 - INFO - evaluating now!
2023-10-04 19:54:16,120 - INFO - Epoch [256/4000] train_loss: 9.5225, lr: 0.000500, 0.04s
2023-10-04 19:54:16,351 - INFO - Saved model at 256
2023-10-04 19:54:16,351 - INFO - Val loss decrease from 9.9088 to 9.5225, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch256.tar
2023-10-04 19:54:16,391 - INFO - epoch complete!
2023-10-04 19:54:16,391 - INFO - evaluating now!
2023-10-04 19:54:16,391 - INFO - Epoch [257/4000] train_loss: 9.9773, lr: 0.000500, 0.04s
2023-10-04 19:54:16,431 - INFO - epoch complete!
2023-10-04 19:54:16,431 - INFO - evaluating now!
2023-10-04 19:54:16,431 - INFO - Epoch [258/4000] train_loss: 10.4373, lr: 0.000500, 0.04s
2023-10-04 19:54:16,471 - INFO - epoch complete!
2023-10-04 19:54:16,471 - INFO - evaluating now!
2023-10-04 19:54:16,472 - INFO - Epoch [259/4000] train_loss: 10.7432, lr: 0.000500, 0.04s
2023-10-04 19:54:16,511 - INFO - epoch complete!
2023-10-04 19:54:16,511 - INFO - evaluating now!
2023-10-04 19:54:16,512 - INFO - Epoch [260/4000] train_loss: 10.4815, lr: 0.000500, 0.04s
2023-10-04 19:54:16,551 - INFO - epoch complete!
2023-10-04 19:54:16,552 - INFO - evaluating now!
2023-10-04 19:54:16,552 - INFO - Epoch [261/4000] train_loss: 9.9390, lr: 0.000500, 0.04s
2023-10-04 19:54:16,591 - INFO - epoch complete!
2023-10-04 19:54:16,592 - INFO - evaluating now!
2023-10-04 19:54:16,592 - INFO - Epoch [262/4000] train_loss: 9.9947, lr: 0.000500, 0.04s
2023-10-04 19:54:16,631 - INFO - epoch complete!
2023-10-04 19:54:16,632 - INFO - evaluating now!
2023-10-04 19:54:16,632 - INFO - Epoch [263/4000] train_loss: 10.0065, lr: 0.000500, 0.04s
2023-10-04 19:54:16,671 - INFO - epoch complete!
2023-10-04 19:54:16,671 - INFO - evaluating now!
2023-10-04 19:54:16,671 - INFO - Epoch [264/4000] train_loss: 11.4752, lr: 0.000500, 0.04s
2023-10-04 19:54:16,710 - INFO - epoch complete!
2023-10-04 19:54:16,711 - INFO - evaluating now!
2023-10-04 19:54:16,711 - INFO - Epoch [265/4000] train_loss: 10.3994, lr: 0.000500, 0.04s
2023-10-04 19:54:16,750 - INFO - epoch complete!
2023-10-04 19:54:16,750 - INFO - evaluating now!
2023-10-04 19:54:16,750 - INFO - Epoch [266/4000] train_loss: 10.4519, lr: 0.000500, 0.04s
2023-10-04 19:54:16,789 - INFO - epoch complete!
2023-10-04 19:54:16,790 - INFO - evaluating now!
2023-10-04 19:54:16,790 - INFO - Epoch [267/4000] train_loss: 9.7758, lr: 0.000500, 0.04s
2023-10-04 19:54:16,829 - INFO - epoch complete!
2023-10-04 19:54:16,829 - INFO - evaluating now!
2023-10-04 19:54:16,829 - INFO - Epoch [268/4000] train_loss: 10.2733, lr: 0.000500, 0.04s
2023-10-04 19:54:16,867 - INFO - epoch complete!
2023-10-04 19:54:16,868 - INFO - evaluating now!
2023-10-04 19:54:16,868 - INFO - Epoch [269/4000] train_loss: 9.7372, lr: 0.000500, 0.04s
2023-10-04 19:54:16,906 - INFO - epoch complete!
2023-10-04 19:54:16,906 - INFO - evaluating now!
2023-10-04 19:54:16,906 - INFO - Epoch [270/4000] train_loss: 9.6639, lr: 0.000500, 0.04s
2023-10-04 19:54:16,945 - INFO - epoch complete!
2023-10-04 19:54:16,945 - INFO - evaluating now!
2023-10-04 19:54:16,945 - INFO - Epoch [271/4000] train_loss: 9.5616, lr: 0.000500, 0.04s
2023-10-04 19:54:16,983 - INFO - epoch complete!
2023-10-04 19:54:16,984 - INFO - evaluating now!
2023-10-04 19:54:16,984 - INFO - Epoch [272/4000] train_loss: 9.7886, lr: 0.000500, 0.04s
2023-10-04 19:54:17,022 - INFO - epoch complete!
2023-10-04 19:54:17,022 - INFO - evaluating now!
2023-10-04 19:54:17,022 - INFO - Epoch [273/4000] train_loss: 9.7486, lr: 0.000500, 0.04s
2023-10-04 19:54:17,061 - INFO - epoch complete!
2023-10-04 19:54:17,061 - INFO - evaluating now!
2023-10-04 19:54:17,061 - INFO - Epoch [274/4000] train_loss: 9.7658, lr: 0.000500, 0.04s
2023-10-04 19:54:17,099 - INFO - epoch complete!
2023-10-04 19:54:17,100 - INFO - evaluating now!
2023-10-04 19:54:17,100 - INFO - Epoch [275/4000] train_loss: 9.2254, lr: 0.000500, 0.04s
2023-10-04 19:54:17,332 - INFO - Saved model at 275
2023-10-04 19:54:17,332 - INFO - Val loss decrease from 9.5225 to 9.2254, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch275.tar
2023-10-04 19:54:17,372 - INFO - epoch complete!
2023-10-04 19:54:17,372 - INFO - evaluating now!
2023-10-04 19:54:17,373 - INFO - Epoch [276/4000] train_loss: 9.5040, lr: 0.000500, 0.04s
2023-10-04 19:54:17,412 - INFO - epoch complete!
2023-10-04 19:54:17,412 - INFO - evaluating now!
2023-10-04 19:54:17,413 - INFO - Epoch [277/4000] train_loss: 9.7461, lr: 0.000500, 0.04s
2023-10-04 19:54:17,452 - INFO - epoch complete!
2023-10-04 19:54:17,452 - INFO - evaluating now!
2023-10-04 19:54:17,452 - INFO - Epoch [278/4000] train_loss: 10.1048, lr: 0.000500, 0.04s
2023-10-04 19:54:17,491 - INFO - epoch complete!
2023-10-04 19:54:17,491 - INFO - evaluating now!
2023-10-04 19:54:17,491 - INFO - Epoch [279/4000] train_loss: 9.7195, lr: 0.000500, 0.04s
2023-10-04 19:54:17,530 - INFO - epoch complete!
2023-10-04 19:54:17,530 - INFO - evaluating now!
2023-10-04 19:54:17,530 - INFO - Epoch [280/4000] train_loss: 9.0309, lr: 0.000500, 0.04s
2023-10-04 19:54:17,761 - INFO - Saved model at 280
2023-10-04 19:54:17,761 - INFO - Val loss decrease from 9.2254 to 9.0309, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch280.tar
2023-10-04 19:54:17,801 - INFO - epoch complete!
2023-10-04 19:54:17,801 - INFO - evaluating now!
2023-10-04 19:54:17,801 - INFO - Epoch [281/4000] train_loss: 9.2605, lr: 0.000500, 0.04s
2023-10-04 19:54:17,841 - INFO - epoch complete!
2023-10-04 19:54:17,841 - INFO - evaluating now!
2023-10-04 19:54:17,841 - INFO - Epoch [282/4000] train_loss: 9.8930, lr: 0.000500, 0.04s
2023-10-04 19:54:17,881 - INFO - epoch complete!
2023-10-04 19:54:17,881 - INFO - evaluating now!
2023-10-04 19:54:17,881 - INFO - Epoch [283/4000] train_loss: 8.5858, lr: 0.000500, 0.04s
2023-10-04 19:54:18,112 - INFO - Saved model at 283
2023-10-04 19:54:18,112 - INFO - Val loss decrease from 9.0309 to 8.5858, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch283.tar
2023-10-04 19:54:18,152 - INFO - epoch complete!
2023-10-04 19:54:18,153 - INFO - evaluating now!
2023-10-04 19:54:18,153 - INFO - Epoch [284/4000] train_loss: 9.5953, lr: 0.000500, 0.04s
2023-10-04 19:54:18,192 - INFO - epoch complete!
2023-10-04 19:54:18,193 - INFO - evaluating now!
2023-10-04 19:54:18,193 - INFO - Epoch [285/4000] train_loss: 9.7749, lr: 0.000500, 0.04s
2023-10-04 19:54:18,232 - INFO - epoch complete!
2023-10-04 19:54:18,233 - INFO - evaluating now!
2023-10-04 19:54:18,233 - INFO - Epoch [286/4000] train_loss: 9.8825, lr: 0.000500, 0.04s
2023-10-04 19:54:18,272 - INFO - epoch complete!
2023-10-04 19:54:18,273 - INFO - evaluating now!
2023-10-04 19:54:18,273 - INFO - Epoch [287/4000] train_loss: 9.8781, lr: 0.000500, 0.04s
2023-10-04 19:54:18,312 - INFO - epoch complete!
2023-10-04 19:54:18,313 - INFO - evaluating now!
2023-10-04 19:54:18,313 - INFO - Epoch [288/4000] train_loss: 9.8195, lr: 0.000500, 0.04s
2023-10-04 19:54:18,352 - INFO - epoch complete!
2023-10-04 19:54:18,353 - INFO - evaluating now!
2023-10-04 19:54:18,353 - INFO - Epoch [289/4000] train_loss: 9.7131, lr: 0.000500, 0.04s
2023-10-04 19:54:18,392 - INFO - epoch complete!
2023-10-04 19:54:18,392 - INFO - evaluating now!
2023-10-04 19:54:18,392 - INFO - Epoch [290/4000] train_loss: 9.1846, lr: 0.000500, 0.04s
2023-10-04 19:54:18,431 - INFO - epoch complete!
2023-10-04 19:54:18,431 - INFO - evaluating now!
2023-10-04 19:54:18,431 - INFO - Epoch [291/4000] train_loss: 9.4877, lr: 0.000500, 0.04s
2023-10-04 19:54:18,470 - INFO - epoch complete!
2023-10-04 19:54:18,470 - INFO - evaluating now!
2023-10-04 19:54:18,470 - INFO - Epoch [292/4000] train_loss: 9.2664, lr: 0.000500, 0.04s
2023-10-04 19:54:18,509 - INFO - epoch complete!
2023-10-04 19:54:18,509 - INFO - evaluating now!
2023-10-04 19:54:18,509 - INFO - Epoch [293/4000] train_loss: 9.3014, lr: 0.000500, 0.04s
2023-10-04 19:54:18,548 - INFO - epoch complete!
2023-10-04 19:54:18,548 - INFO - evaluating now!
2023-10-04 19:54:18,548 - INFO - Epoch [294/4000] train_loss: 8.8812, lr: 0.000500, 0.04s
2023-10-04 19:54:18,586 - INFO - epoch complete!
2023-10-04 19:54:18,587 - INFO - evaluating now!
2023-10-04 19:54:18,587 - INFO - Epoch [295/4000] train_loss: 9.7826, lr: 0.000500, 0.04s
2023-10-04 19:54:18,625 - INFO - epoch complete!
2023-10-04 19:54:18,626 - INFO - evaluating now!
2023-10-04 19:54:18,626 - INFO - Epoch [296/4000] train_loss: 9.6724, lr: 0.000500, 0.04s
2023-10-04 19:54:18,665 - INFO - epoch complete!
2023-10-04 19:54:18,665 - INFO - evaluating now!
2023-10-04 19:54:18,665 - INFO - Epoch [297/4000] train_loss: 9.5392, lr: 0.000500, 0.04s
2023-10-04 19:54:18,704 - INFO - epoch complete!
2023-10-04 19:54:18,705 - INFO - evaluating now!
2023-10-04 19:54:18,705 - INFO - Epoch [298/4000] train_loss: 8.5816, lr: 0.000500, 0.04s
2023-10-04 19:54:18,935 - INFO - Saved model at 298
2023-10-04 19:54:18,935 - INFO - Val loss decrease from 8.5858 to 8.5816, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch298.tar
2023-10-04 19:54:18,975 - INFO - epoch complete!
2023-10-04 19:54:18,975 - INFO - evaluating now!
2023-10-04 19:54:18,975 - INFO - Epoch [299/4000] train_loss: 9.5460, lr: 0.000500, 0.04s
2023-10-04 19:54:19,014 - INFO - epoch complete!
2023-10-04 19:54:19,014 - INFO - evaluating now!
2023-10-04 19:54:19,015 - INFO - Epoch [300/4000] train_loss: 9.5854, lr: 0.000500, 0.04s
2023-10-04 19:54:19,053 - INFO - epoch complete!
2023-10-04 19:54:19,054 - INFO - evaluating now!
2023-10-04 19:54:19,054 - INFO - Epoch [301/4000] train_loss: 8.8163, lr: 0.000500, 0.04s
2023-10-04 19:54:19,093 - INFO - epoch complete!
2023-10-04 19:54:19,093 - INFO - evaluating now!
2023-10-04 19:54:19,093 - INFO - Epoch [302/4000] train_loss: 9.1879, lr: 0.000500, 0.04s
2023-10-04 19:54:19,132 - INFO - epoch complete!
2023-10-04 19:54:19,132 - INFO - evaluating now!
2023-10-04 19:54:19,132 - INFO - Epoch [303/4000] train_loss: 9.7285, lr: 0.000500, 0.04s
2023-10-04 19:54:19,171 - INFO - epoch complete!
2023-10-04 19:54:19,171 - INFO - evaluating now!
2023-10-04 19:54:19,172 - INFO - Epoch [304/4000] train_loss: 8.9683, lr: 0.000500, 0.04s
2023-10-04 19:54:19,211 - INFO - epoch complete!
2023-10-04 19:54:19,211 - INFO - evaluating now!
2023-10-04 19:54:19,211 - INFO - Epoch [305/4000] train_loss: 9.2654, lr: 0.000500, 0.04s
2023-10-04 19:54:19,250 - INFO - epoch complete!
2023-10-04 19:54:19,250 - INFO - evaluating now!
2023-10-04 19:54:19,251 - INFO - Epoch [306/4000] train_loss: 9.4451, lr: 0.000500, 0.04s
2023-10-04 19:54:19,290 - INFO - epoch complete!
2023-10-04 19:54:19,290 - INFO - evaluating now!
2023-10-04 19:54:19,290 - INFO - Epoch [307/4000] train_loss: 9.0044, lr: 0.000500, 0.04s
2023-10-04 19:54:19,329 - INFO - epoch complete!
2023-10-04 19:54:19,329 - INFO - evaluating now!
2023-10-04 19:54:19,329 - INFO - Epoch [308/4000] train_loss: 8.4988, lr: 0.000500, 0.04s
2023-10-04 19:54:19,559 - INFO - Saved model at 308
2023-10-04 19:54:19,560 - INFO - Val loss decrease from 8.5816 to 8.4988, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch308.tar
2023-10-04 19:54:19,599 - INFO - epoch complete!
2023-10-04 19:54:19,599 - INFO - evaluating now!
2023-10-04 19:54:19,599 - INFO - Epoch [309/4000] train_loss: 9.1469, lr: 0.000500, 0.04s
2023-10-04 19:54:19,638 - INFO - epoch complete!
2023-10-04 19:54:19,638 - INFO - evaluating now!
2023-10-04 19:54:19,638 - INFO - Epoch [310/4000] train_loss: 9.1063, lr: 0.000500, 0.04s
2023-10-04 19:54:19,677 - INFO - epoch complete!
2023-10-04 19:54:19,677 - INFO - evaluating now!
2023-10-04 19:54:19,677 - INFO - Epoch [311/4000] train_loss: 9.7249, lr: 0.000500, 0.04s
2023-10-04 19:54:19,716 - INFO - epoch complete!
2023-10-04 19:54:19,717 - INFO - evaluating now!
2023-10-04 19:54:19,717 - INFO - Epoch [312/4000] train_loss: 9.6433, lr: 0.000500, 0.04s
2023-10-04 19:54:19,755 - INFO - epoch complete!
2023-10-04 19:54:19,756 - INFO - evaluating now!
2023-10-04 19:54:19,756 - INFO - Epoch [313/4000] train_loss: 9.6322, lr: 0.000500, 0.04s
2023-10-04 19:54:19,794 - INFO - epoch complete!
2023-10-04 19:54:19,795 - INFO - evaluating now!
2023-10-04 19:54:19,795 - INFO - Epoch [314/4000] train_loss: 9.4516, lr: 0.000500, 0.04s
2023-10-04 19:54:19,834 - INFO - epoch complete!
2023-10-04 19:54:19,834 - INFO - evaluating now!
2023-10-04 19:54:19,834 - INFO - Epoch [315/4000] train_loss: 8.9282, lr: 0.000500, 0.04s
2023-10-04 19:54:19,873 - INFO - epoch complete!
2023-10-04 19:54:19,873 - INFO - evaluating now!
2023-10-04 19:54:19,873 - INFO - Epoch [316/4000] train_loss: 9.2321, lr: 0.000500, 0.04s
2023-10-04 19:54:19,912 - INFO - epoch complete!
2023-10-04 19:54:19,912 - INFO - evaluating now!
2023-10-04 19:54:19,913 - INFO - Epoch [317/4000] train_loss: 8.8195, lr: 0.000500, 0.04s
2023-10-04 19:54:19,951 - INFO - epoch complete!
2023-10-04 19:54:19,952 - INFO - evaluating now!
2023-10-04 19:54:19,952 - INFO - Epoch [318/4000] train_loss: 8.8989, lr: 0.000500, 0.04s
2023-10-04 19:54:19,990 - INFO - epoch complete!
2023-10-04 19:54:19,991 - INFO - evaluating now!
2023-10-04 19:54:19,991 - INFO - Epoch [319/4000] train_loss: 8.5795, lr: 0.000500, 0.04s
2023-10-04 19:54:20,029 - INFO - epoch complete!
2023-10-04 19:54:20,030 - INFO - evaluating now!
2023-10-04 19:54:20,030 - INFO - Epoch [320/4000] train_loss: 9.2570, lr: 0.000500, 0.04s
2023-10-04 19:54:20,068 - INFO - epoch complete!
2023-10-04 19:54:20,069 - INFO - evaluating now!
2023-10-04 19:54:20,069 - INFO - Epoch [321/4000] train_loss: 8.7711, lr: 0.000500, 0.04s
2023-10-04 19:54:20,107 - INFO - epoch complete!
2023-10-04 19:54:20,108 - INFO - evaluating now!
2023-10-04 19:54:20,108 - INFO - Epoch [322/4000] train_loss: 8.8189, lr: 0.000500, 0.04s
2023-10-04 19:54:20,147 - INFO - epoch complete!
2023-10-04 19:54:20,147 - INFO - evaluating now!
2023-10-04 19:54:20,147 - INFO - Epoch [323/4000] train_loss: 9.3796, lr: 0.000500, 0.04s
2023-10-04 19:54:20,186 - INFO - epoch complete!
2023-10-04 19:54:20,186 - INFO - evaluating now!
2023-10-04 19:54:20,186 - INFO - Epoch [324/4000] train_loss: 8.4889, lr: 0.000500, 0.04s
2023-10-04 19:54:20,418 - INFO - Saved model at 324
2023-10-04 19:54:20,418 - INFO - Val loss decrease from 8.4988 to 8.4889, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch324.tar
2023-10-04 19:54:20,458 - INFO - epoch complete!
2023-10-04 19:54:20,458 - INFO - evaluating now!
2023-10-04 19:54:20,458 - INFO - Epoch [325/4000] train_loss: 8.4659, lr: 0.000500, 0.04s
2023-10-04 19:54:20,689 - INFO - Saved model at 325
2023-10-04 19:54:20,689 - INFO - Val loss decrease from 8.4889 to 8.4659, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch325.tar
2023-10-04 19:54:20,729 - INFO - epoch complete!
2023-10-04 19:54:20,730 - INFO - evaluating now!
2023-10-04 19:54:20,730 - INFO - Epoch [326/4000] train_loss: 9.3687, lr: 0.000500, 0.04s
2023-10-04 19:54:20,769 - INFO - epoch complete!
2023-10-04 19:54:20,770 - INFO - evaluating now!
2023-10-04 19:54:20,770 - INFO - Epoch [327/4000] train_loss: 8.6234, lr: 0.000500, 0.04s
2023-10-04 19:54:20,809 - INFO - epoch complete!
2023-10-04 19:54:20,809 - INFO - evaluating now!
2023-10-04 19:54:20,809 - INFO - Epoch [328/4000] train_loss: 8.8887, lr: 0.000500, 0.04s
2023-10-04 19:54:20,848 - INFO - epoch complete!
2023-10-04 19:54:20,849 - INFO - evaluating now!
2023-10-04 19:54:20,849 - INFO - Epoch [329/4000] train_loss: 9.0857, lr: 0.000500, 0.04s
2023-10-04 19:54:20,888 - INFO - epoch complete!
2023-10-04 19:54:20,888 - INFO - evaluating now!
2023-10-04 19:54:20,888 - INFO - Epoch [330/4000] train_loss: 9.4385, lr: 0.000500, 0.04s
2023-10-04 19:54:20,926 - INFO - epoch complete!
2023-10-04 19:54:20,927 - INFO - evaluating now!
2023-10-04 19:54:20,927 - INFO - Epoch [331/4000] train_loss: 8.5128, lr: 0.000500, 0.04s
2023-10-04 19:54:20,966 - INFO - epoch complete!
2023-10-04 19:54:20,966 - INFO - evaluating now!
2023-10-04 19:54:20,966 - INFO - Epoch [332/4000] train_loss: 9.0302, lr: 0.000500, 0.04s
2023-10-04 19:54:21,005 - INFO - epoch complete!
2023-10-04 19:54:21,005 - INFO - evaluating now!
2023-10-04 19:54:21,005 - INFO - Epoch [333/4000] train_loss: 8.9455, lr: 0.000500, 0.04s
2023-10-04 19:54:21,044 - INFO - epoch complete!
2023-10-04 19:54:21,045 - INFO - evaluating now!
2023-10-04 19:54:21,045 - INFO - Epoch [334/4000] train_loss: 8.3350, lr: 0.000500, 0.04s
2023-10-04 19:54:21,278 - INFO - Saved model at 334
2023-10-04 19:54:21,278 - INFO - Val loss decrease from 8.4659 to 8.3350, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch334.tar
2023-10-04 19:54:21,318 - INFO - epoch complete!
2023-10-04 19:54:21,318 - INFO - evaluating now!
2023-10-04 19:54:21,318 - INFO - Epoch [335/4000] train_loss: 8.9445, lr: 0.000500, 0.04s
2023-10-04 19:54:21,358 - INFO - epoch complete!
2023-10-04 19:54:21,358 - INFO - evaluating now!
2023-10-04 19:54:21,358 - INFO - Epoch [336/4000] train_loss: 8.7879, lr: 0.000500, 0.04s
2023-10-04 19:54:21,397 - INFO - epoch complete!
2023-10-04 19:54:21,398 - INFO - evaluating now!
2023-10-04 19:54:21,398 - INFO - Epoch [337/4000] train_loss: 9.0997, lr: 0.000500, 0.04s
2023-10-04 19:54:21,437 - INFO - epoch complete!
2023-10-04 19:54:21,437 - INFO - evaluating now!
2023-10-04 19:54:21,437 - INFO - Epoch [338/4000] train_loss: 8.3541, lr: 0.000500, 0.04s
2023-10-04 19:54:21,477 - INFO - epoch complete!
2023-10-04 19:54:21,477 - INFO - evaluating now!
2023-10-04 19:54:21,477 - INFO - Epoch [339/4000] train_loss: 8.9604, lr: 0.000500, 0.04s
2023-10-04 19:54:21,516 - INFO - epoch complete!
2023-10-04 19:54:21,516 - INFO - evaluating now!
2023-10-04 19:54:21,517 - INFO - Epoch [340/4000] train_loss: 8.6011, lr: 0.000500, 0.04s
2023-10-04 19:54:21,555 - INFO - epoch complete!
2023-10-04 19:54:21,556 - INFO - evaluating now!
2023-10-04 19:54:21,556 - INFO - Epoch [341/4000] train_loss: 8.8744, lr: 0.000500, 0.04s
2023-10-04 19:54:21,595 - INFO - epoch complete!
2023-10-04 19:54:21,595 - INFO - evaluating now!
2023-10-04 19:54:21,595 - INFO - Epoch [342/4000] train_loss: 8.3200, lr: 0.000500, 0.04s
2023-10-04 19:54:21,826 - INFO - Saved model at 342
2023-10-04 19:54:21,826 - INFO - Val loss decrease from 8.3350 to 8.3200, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch342.tar
2023-10-04 19:54:21,866 - INFO - epoch complete!
2023-10-04 19:54:21,867 - INFO - evaluating now!
2023-10-04 19:54:21,867 - INFO - Epoch [343/4000] train_loss: 8.4634, lr: 0.000500, 0.04s
2023-10-04 19:54:21,907 - INFO - epoch complete!
2023-10-04 19:54:21,908 - INFO - evaluating now!
2023-10-04 19:54:21,908 - INFO - Epoch [344/4000] train_loss: 8.5841, lr: 0.000500, 0.04s
2023-10-04 19:54:21,949 - INFO - epoch complete!
2023-10-04 19:54:21,949 - INFO - evaluating now!
2023-10-04 19:54:21,949 - INFO - Epoch [345/4000] train_loss: 8.4780, lr: 0.000500, 0.04s
2023-10-04 19:54:21,989 - INFO - epoch complete!
2023-10-04 19:54:21,989 - INFO - evaluating now!
2023-10-04 19:54:21,989 - INFO - Epoch [346/4000] train_loss: 8.9669, lr: 0.000500, 0.04s
2023-10-04 19:54:22,028 - INFO - epoch complete!
2023-10-04 19:54:22,028 - INFO - evaluating now!
2023-10-04 19:54:22,028 - INFO - Epoch [347/4000] train_loss: 8.4854, lr: 0.000500, 0.04s
2023-10-04 19:54:22,067 - INFO - epoch complete!
2023-10-04 19:54:22,067 - INFO - evaluating now!
2023-10-04 19:54:22,067 - INFO - Epoch [348/4000] train_loss: 9.3781, lr: 0.000500, 0.04s
2023-10-04 19:54:22,106 - INFO - epoch complete!
2023-10-04 19:54:22,106 - INFO - evaluating now!
2023-10-04 19:54:22,107 - INFO - Epoch [349/4000] train_loss: 8.6595, lr: 0.000500, 0.04s
2023-10-04 19:54:22,145 - INFO - epoch complete!
2023-10-04 19:54:22,146 - INFO - evaluating now!
2023-10-04 19:54:22,146 - INFO - Epoch [350/4000] train_loss: 8.2813, lr: 0.000500, 0.04s
2023-10-04 19:54:22,376 - INFO - Saved model at 350
2023-10-04 19:54:22,376 - INFO - Val loss decrease from 8.3200 to 8.2813, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch350.tar
2023-10-04 19:54:22,416 - INFO - epoch complete!
2023-10-04 19:54:22,417 - INFO - evaluating now!
2023-10-04 19:54:22,417 - INFO - Epoch [351/4000] train_loss: 8.7227, lr: 0.000500, 0.04s
2023-10-04 19:54:22,456 - INFO - epoch complete!
2023-10-04 19:54:22,457 - INFO - evaluating now!
2023-10-04 19:54:22,457 - INFO - Epoch [352/4000] train_loss: 8.5261, lr: 0.000500, 0.04s
2023-10-04 19:54:22,496 - INFO - epoch complete!
2023-10-04 19:54:22,496 - INFO - evaluating now!
2023-10-04 19:54:22,497 - INFO - Epoch [353/4000] train_loss: 8.7020, lr: 0.000500, 0.04s
2023-10-04 19:54:22,536 - INFO - epoch complete!
2023-10-04 19:54:22,536 - INFO - evaluating now!
2023-10-04 19:54:22,536 - INFO - Epoch [354/4000] train_loss: 8.3493, lr: 0.000500, 0.04s
2023-10-04 19:54:22,576 - INFO - epoch complete!
2023-10-04 19:54:22,576 - INFO - evaluating now!
2023-10-04 19:54:22,576 - INFO - Epoch [355/4000] train_loss: 9.0286, lr: 0.000500, 0.04s
2023-10-04 19:54:22,617 - INFO - epoch complete!
2023-10-04 19:54:22,617 - INFO - evaluating now!
2023-10-04 19:54:22,617 - INFO - Epoch [356/4000] train_loss: 9.1620, lr: 0.000500, 0.04s
2023-10-04 19:54:22,657 - INFO - epoch complete!
2023-10-04 19:54:22,657 - INFO - evaluating now!
2023-10-04 19:54:22,658 - INFO - Epoch [357/4000] train_loss: 8.2830, lr: 0.000500, 0.04s
2023-10-04 19:54:22,696 - INFO - epoch complete!
2023-10-04 19:54:22,696 - INFO - evaluating now!
2023-10-04 19:54:22,696 - INFO - Epoch [358/4000] train_loss: 9.3212, lr: 0.000500, 0.04s
2023-10-04 19:54:22,735 - INFO - epoch complete!
2023-10-04 19:54:22,735 - INFO - evaluating now!
2023-10-04 19:54:22,735 - INFO - Epoch [359/4000] train_loss: 8.5549, lr: 0.000500, 0.04s
2023-10-04 19:54:22,774 - INFO - epoch complete!
2023-10-04 19:54:22,774 - INFO - evaluating now!
2023-10-04 19:54:22,774 - INFO - Epoch [360/4000] train_loss: 8.8505, lr: 0.000500, 0.04s
2023-10-04 19:54:22,813 - INFO - epoch complete!
2023-10-04 19:54:22,813 - INFO - evaluating now!
2023-10-04 19:54:22,813 - INFO - Epoch [361/4000] train_loss: 8.0932, lr: 0.000500, 0.04s
2023-10-04 19:54:23,044 - INFO - Saved model at 361
2023-10-04 19:54:23,045 - INFO - Val loss decrease from 8.2813 to 8.0932, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch361.tar
2023-10-04 19:54:23,085 - INFO - epoch complete!
2023-10-04 19:54:23,086 - INFO - evaluating now!
2023-10-04 19:54:23,086 - INFO - Epoch [362/4000] train_loss: 8.6227, lr: 0.000500, 0.04s
2023-10-04 19:54:23,125 - INFO - epoch complete!
2023-10-04 19:54:23,125 - INFO - evaluating now!
2023-10-04 19:54:23,126 - INFO - Epoch [363/4000] train_loss: 8.2305, lr: 0.000500, 0.04s
2023-10-04 19:54:23,164 - INFO - epoch complete!
2023-10-04 19:54:23,165 - INFO - evaluating now!
2023-10-04 19:54:23,165 - INFO - Epoch [364/4000] train_loss: 8.3112, lr: 0.000500, 0.04s
2023-10-04 19:54:23,203 - INFO - epoch complete!
2023-10-04 19:54:23,204 - INFO - evaluating now!
2023-10-04 19:54:23,204 - INFO - Epoch [365/4000] train_loss: 8.3158, lr: 0.000500, 0.04s
2023-10-04 19:54:23,243 - INFO - epoch complete!
2023-10-04 19:54:23,243 - INFO - evaluating now!
2023-10-04 19:54:23,243 - INFO - Epoch [366/4000] train_loss: 9.1896, lr: 0.000500, 0.04s
2023-10-04 19:54:23,282 - INFO - epoch complete!
2023-10-04 19:54:23,282 - INFO - evaluating now!
2023-10-04 19:54:23,282 - INFO - Epoch [367/4000] train_loss: 8.3746, lr: 0.000500, 0.04s
2023-10-04 19:54:23,321 - INFO - epoch complete!
2023-10-04 19:54:23,321 - INFO - evaluating now!
2023-10-04 19:54:23,321 - INFO - Epoch [368/4000] train_loss: 8.9323, lr: 0.000500, 0.04s
2023-10-04 19:54:23,360 - INFO - epoch complete!
2023-10-04 19:54:23,360 - INFO - evaluating now!
2023-10-04 19:54:23,360 - INFO - Epoch [369/4000] train_loss: 8.0976, lr: 0.000500, 0.04s
2023-10-04 19:54:23,399 - INFO - epoch complete!
2023-10-04 19:54:23,399 - INFO - evaluating now!
2023-10-04 19:54:23,399 - INFO - Epoch [370/4000] train_loss: 8.5469, lr: 0.000500, 0.04s
2023-10-04 19:54:23,438 - INFO - epoch complete!
2023-10-04 19:54:23,438 - INFO - evaluating now!
2023-10-04 19:54:23,439 - INFO - Epoch [371/4000] train_loss: 7.7202, lr: 0.000500, 0.04s
2023-10-04 19:54:23,669 - INFO - Saved model at 371
2023-10-04 19:54:23,670 - INFO - Val loss decrease from 8.0932 to 7.7202, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch371.tar
2023-10-04 19:54:23,709 - INFO - epoch complete!
2023-10-04 19:54:23,710 - INFO - evaluating now!
2023-10-04 19:54:23,710 - INFO - Epoch [372/4000] train_loss: 8.2667, lr: 0.000500, 0.04s
2023-10-04 19:54:23,749 - INFO - epoch complete!
2023-10-04 19:54:23,749 - INFO - evaluating now!
2023-10-04 19:54:23,750 - INFO - Epoch [373/4000] train_loss: 8.1730, lr: 0.000500, 0.04s
2023-10-04 19:54:23,789 - INFO - epoch complete!
2023-10-04 19:54:23,789 - INFO - evaluating now!
2023-10-04 19:54:23,789 - INFO - Epoch [374/4000] train_loss: 8.6065, lr: 0.000500, 0.04s
2023-10-04 19:54:23,828 - INFO - epoch complete!
2023-10-04 19:54:23,828 - INFO - evaluating now!
2023-10-04 19:54:23,828 - INFO - Epoch [375/4000] train_loss: 7.9688, lr: 0.000500, 0.04s
2023-10-04 19:54:23,867 - INFO - epoch complete!
2023-10-04 19:54:23,868 - INFO - evaluating now!
2023-10-04 19:54:23,868 - INFO - Epoch [376/4000] train_loss: 7.9060, lr: 0.000500, 0.04s
2023-10-04 19:54:23,907 - INFO - epoch complete!
2023-10-04 19:54:23,907 - INFO - evaluating now!
2023-10-04 19:54:23,907 - INFO - Epoch [377/4000] train_loss: 8.0272, lr: 0.000500, 0.04s
2023-10-04 19:54:23,946 - INFO - epoch complete!
2023-10-04 19:54:23,946 - INFO - evaluating now!
2023-10-04 19:54:23,946 - INFO - Epoch [378/4000] train_loss: 8.5115, lr: 0.000500, 0.04s
2023-10-04 19:54:23,985 - INFO - epoch complete!
2023-10-04 19:54:23,985 - INFO - evaluating now!
2023-10-04 19:54:23,985 - INFO - Epoch [379/4000] train_loss: 8.5617, lr: 0.000500, 0.04s
2023-10-04 19:54:24,025 - INFO - epoch complete!
2023-10-04 19:54:24,026 - INFO - evaluating now!
2023-10-04 19:54:24,026 - INFO - Epoch [380/4000] train_loss: 8.3152, lr: 0.000500, 0.04s
2023-10-04 19:54:24,065 - INFO - epoch complete!
2023-10-04 19:54:24,065 - INFO - evaluating now!
2023-10-04 19:54:24,065 - INFO - Epoch [381/4000] train_loss: 7.8801, lr: 0.000500, 0.04s
2023-10-04 19:54:24,103 - INFO - epoch complete!
2023-10-04 19:54:24,104 - INFO - evaluating now!
2023-10-04 19:54:24,104 - INFO - Epoch [382/4000] train_loss: 8.7023, lr: 0.000500, 0.04s
2023-10-04 19:54:24,142 - INFO - epoch complete!
2023-10-04 19:54:24,143 - INFO - evaluating now!
2023-10-04 19:54:24,143 - INFO - Epoch [383/4000] train_loss: 8.4450, lr: 0.000500, 0.04s
2023-10-04 19:54:24,181 - INFO - epoch complete!
2023-10-04 19:54:24,181 - INFO - evaluating now!
2023-10-04 19:54:24,181 - INFO - Epoch [384/4000] train_loss: 8.2443, lr: 0.000500, 0.04s
2023-10-04 19:54:24,220 - INFO - epoch complete!
2023-10-04 19:54:24,220 - INFO - evaluating now!
2023-10-04 19:54:24,220 - INFO - Epoch [385/4000] train_loss: 8.2801, lr: 0.000500, 0.04s
2023-10-04 19:54:24,259 - INFO - epoch complete!
2023-10-04 19:54:24,259 - INFO - evaluating now!
2023-10-04 19:54:24,259 - INFO - Epoch [386/4000] train_loss: 7.8258, lr: 0.000500, 0.04s
2023-10-04 19:54:24,298 - INFO - epoch complete!
2023-10-04 19:54:24,298 - INFO - evaluating now!
2023-10-04 19:54:24,298 - INFO - Epoch [387/4000] train_loss: 8.9427, lr: 0.000500, 0.04s
2023-10-04 19:54:24,336 - INFO - epoch complete!
2023-10-04 19:54:24,337 - INFO - evaluating now!
2023-10-04 19:54:24,337 - INFO - Epoch [388/4000] train_loss: 8.2369, lr: 0.000500, 0.04s
2023-10-04 19:54:24,375 - INFO - epoch complete!
2023-10-04 19:54:24,376 - INFO - evaluating now!
2023-10-04 19:54:24,376 - INFO - Epoch [389/4000] train_loss: 8.2020, lr: 0.000500, 0.04s
2023-10-04 19:54:24,414 - INFO - epoch complete!
2023-10-04 19:54:24,414 - INFO - evaluating now!
2023-10-04 19:54:24,414 - INFO - Epoch [390/4000] train_loss: 8.1848, lr: 0.000500, 0.04s
2023-10-04 19:54:24,453 - INFO - epoch complete!
2023-10-04 19:54:24,453 - INFO - evaluating now!
2023-10-04 19:54:24,453 - INFO - Epoch [391/4000] train_loss: 8.0176, lr: 0.000500, 0.04s
2023-10-04 19:54:24,492 - INFO - epoch complete!
2023-10-04 19:54:24,492 - INFO - evaluating now!
2023-10-04 19:54:24,492 - INFO - Epoch [392/4000] train_loss: 8.0336, lr: 0.000500, 0.04s
2023-10-04 19:54:24,531 - INFO - epoch complete!
2023-10-04 19:54:24,531 - INFO - evaluating now!
2023-10-04 19:54:24,531 - INFO - Epoch [393/4000] train_loss: 8.0525, lr: 0.000500, 0.04s
2023-10-04 19:54:24,570 - INFO - epoch complete!
2023-10-04 19:54:24,570 - INFO - evaluating now!
2023-10-04 19:54:24,570 - INFO - Epoch [394/4000] train_loss: 7.9271, lr: 0.000500, 0.04s
2023-10-04 19:54:24,609 - INFO - epoch complete!
2023-10-04 19:54:24,609 - INFO - evaluating now!
2023-10-04 19:54:24,609 - INFO - Epoch [395/4000] train_loss: 8.0556, lr: 0.000500, 0.04s
2023-10-04 19:54:24,647 - INFO - epoch complete!
2023-10-04 19:54:24,648 - INFO - evaluating now!
2023-10-04 19:54:24,648 - INFO - Epoch [396/4000] train_loss: 7.6009, lr: 0.000500, 0.04s
2023-10-04 19:54:24,878 - INFO - Saved model at 396
2023-10-04 19:54:24,878 - INFO - Val loss decrease from 7.7202 to 7.6009, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch396.tar
2023-10-04 19:54:24,918 - INFO - epoch complete!
2023-10-04 19:54:24,919 - INFO - evaluating now!
2023-10-04 19:54:24,919 - INFO - Epoch [397/4000] train_loss: 8.0450, lr: 0.000500, 0.04s
2023-10-04 19:54:24,958 - INFO - epoch complete!
2023-10-04 19:54:24,959 - INFO - evaluating now!
2023-10-04 19:54:24,959 - INFO - Epoch [398/4000] train_loss: 7.9319, lr: 0.000500, 0.04s
2023-10-04 19:54:24,998 - INFO - epoch complete!
2023-10-04 19:54:24,999 - INFO - evaluating now!
2023-10-04 19:54:24,999 - INFO - Epoch [399/4000] train_loss: 7.8786, lr: 0.000500, 0.04s
2023-10-04 19:54:25,038 - INFO - epoch complete!
2023-10-04 19:54:25,039 - INFO - evaluating now!
2023-10-04 19:54:25,039 - INFO - Epoch [400/4000] train_loss: 7.8385, lr: 0.000500, 0.04s
2023-10-04 19:54:25,078 - INFO - epoch complete!
2023-10-04 19:54:25,079 - INFO - evaluating now!
2023-10-04 19:54:25,079 - INFO - Epoch [401/4000] train_loss: 7.4773, lr: 0.000500, 0.04s
2023-10-04 19:54:25,310 - INFO - Saved model at 401
2023-10-04 19:54:25,310 - INFO - Val loss decrease from 7.6009 to 7.4773, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch401.tar
2023-10-04 19:54:25,350 - INFO - epoch complete!
2023-10-04 19:54:25,350 - INFO - evaluating now!
2023-10-04 19:54:25,350 - INFO - Epoch [402/4000] train_loss: 8.2017, lr: 0.000500, 0.04s
2023-10-04 19:54:25,390 - INFO - epoch complete!
2023-10-04 19:54:25,391 - INFO - evaluating now!
2023-10-04 19:54:25,391 - INFO - Epoch [403/4000] train_loss: 8.3185, lr: 0.000500, 0.04s
2023-10-04 19:54:25,430 - INFO - epoch complete!
2023-10-04 19:54:25,430 - INFO - evaluating now!
2023-10-04 19:54:25,431 - INFO - Epoch [404/4000] train_loss: 8.3683, lr: 0.000500, 0.04s
2023-10-04 19:54:25,470 - INFO - epoch complete!
2023-10-04 19:54:25,471 - INFO - evaluating now!
2023-10-04 19:54:25,471 - INFO - Epoch [405/4000] train_loss: 7.3571, lr: 0.000500, 0.04s
2023-10-04 19:54:25,703 - INFO - Saved model at 405
2023-10-04 19:54:25,704 - INFO - Val loss decrease from 7.4773 to 7.3571, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch405.tar
2023-10-04 19:54:25,744 - INFO - epoch complete!
2023-10-04 19:54:25,744 - INFO - evaluating now!
2023-10-04 19:54:25,744 - INFO - Epoch [406/4000] train_loss: 8.1245, lr: 0.000500, 0.04s
2023-10-04 19:54:25,784 - INFO - epoch complete!
2023-10-04 19:54:25,784 - INFO - evaluating now!
2023-10-04 19:54:25,784 - INFO - Epoch [407/4000] train_loss: 8.0587, lr: 0.000500, 0.04s
2023-10-04 19:54:25,824 - INFO - epoch complete!
2023-10-04 19:54:25,824 - INFO - evaluating now!
2023-10-04 19:54:25,825 - INFO - Epoch [408/4000] train_loss: 7.6114, lr: 0.000500, 0.04s
2023-10-04 19:54:25,865 - INFO - epoch complete!
2023-10-04 19:54:25,865 - INFO - evaluating now!
2023-10-04 19:54:25,866 - INFO - Epoch [409/4000] train_loss: 8.2225, lr: 0.000500, 0.04s
2023-10-04 19:54:25,906 - INFO - epoch complete!
2023-10-04 19:54:25,906 - INFO - evaluating now!
2023-10-04 19:54:25,906 - INFO - Epoch [410/4000] train_loss: 7.8586, lr: 0.000500, 0.04s
2023-10-04 19:54:25,945 - INFO - epoch complete!
2023-10-04 19:54:25,945 - INFO - evaluating now!
2023-10-04 19:54:25,945 - INFO - Epoch [411/4000] train_loss: 8.1819, lr: 0.000500, 0.04s
2023-10-04 19:54:25,984 - INFO - epoch complete!
2023-10-04 19:54:25,984 - INFO - evaluating now!
2023-10-04 19:54:25,984 - INFO - Epoch [412/4000] train_loss: 7.7483, lr: 0.000500, 0.04s
2023-10-04 19:54:26,023 - INFO - epoch complete!
2023-10-04 19:54:26,023 - INFO - evaluating now!
2023-10-04 19:54:26,023 - INFO - Epoch [413/4000] train_loss: 7.7705, lr: 0.000500, 0.04s
2023-10-04 19:54:26,061 - INFO - epoch complete!
2023-10-04 19:54:26,062 - INFO - evaluating now!
2023-10-04 19:54:26,062 - INFO - Epoch [414/4000] train_loss: 8.0383, lr: 0.000500, 0.04s
2023-10-04 19:54:26,100 - INFO - epoch complete!
2023-10-04 19:54:26,100 - INFO - evaluating now!
2023-10-04 19:54:26,100 - INFO - Epoch [415/4000] train_loss: 7.7833, lr: 0.000500, 0.04s
2023-10-04 19:54:26,139 - INFO - epoch complete!
2023-10-04 19:54:26,139 - INFO - evaluating now!
2023-10-04 19:54:26,140 - INFO - Epoch [416/4000] train_loss: 8.0573, lr: 0.000500, 0.04s
2023-10-04 19:54:26,180 - INFO - epoch complete!
2023-10-04 19:54:26,180 - INFO - evaluating now!
2023-10-04 19:54:26,180 - INFO - Epoch [417/4000] train_loss: 7.7122, lr: 0.000500, 0.04s
2023-10-04 19:54:26,219 - INFO - epoch complete!
2023-10-04 19:54:26,219 - INFO - evaluating now!
2023-10-04 19:54:26,219 - INFO - Epoch [418/4000] train_loss: 7.9097, lr: 0.000500, 0.04s
2023-10-04 19:54:26,258 - INFO - epoch complete!
2023-10-04 19:54:26,258 - INFO - evaluating now!
2023-10-04 19:54:26,258 - INFO - Epoch [419/4000] train_loss: 8.0380, lr: 0.000500, 0.04s
2023-10-04 19:54:26,297 - INFO - epoch complete!
2023-10-04 19:54:26,297 - INFO - evaluating now!
2023-10-04 19:54:26,297 - INFO - Epoch [420/4000] train_loss: 7.2231, lr: 0.000500, 0.04s
2023-10-04 19:54:26,527 - INFO - Saved model at 420
2023-10-04 19:54:26,527 - INFO - Val loss decrease from 7.3571 to 7.2231, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch420.tar
2023-10-04 19:54:26,567 - INFO - epoch complete!
2023-10-04 19:54:26,567 - INFO - evaluating now!
2023-10-04 19:54:26,567 - INFO - Epoch [421/4000] train_loss: 7.9315, lr: 0.000500, 0.04s
2023-10-04 19:54:26,606 - INFO - epoch complete!
2023-10-04 19:54:26,607 - INFO - evaluating now!
2023-10-04 19:54:26,607 - INFO - Epoch [422/4000] train_loss: 7.9097, lr: 0.000500, 0.04s
2023-10-04 19:54:26,645 - INFO - epoch complete!
2023-10-04 19:54:26,646 - INFO - evaluating now!
2023-10-04 19:54:26,646 - INFO - Epoch [423/4000] train_loss: 8.1576, lr: 0.000500, 0.04s
2023-10-04 19:54:26,684 - INFO - epoch complete!
2023-10-04 19:54:26,685 - INFO - evaluating now!
2023-10-04 19:54:26,685 - INFO - Epoch [424/4000] train_loss: 7.4215, lr: 0.000500, 0.04s
2023-10-04 19:54:26,724 - INFO - epoch complete!
2023-10-04 19:54:26,724 - INFO - evaluating now!
2023-10-04 19:54:26,724 - INFO - Epoch [425/4000] train_loss: 7.9372, lr: 0.000500, 0.04s
2023-10-04 19:54:26,763 - INFO - epoch complete!
2023-10-04 19:54:26,763 - INFO - evaluating now!
2023-10-04 19:54:26,763 - INFO - Epoch [426/4000] train_loss: 8.1371, lr: 0.000500, 0.04s
2023-10-04 19:54:26,802 - INFO - epoch complete!
2023-10-04 19:54:26,802 - INFO - evaluating now!
2023-10-04 19:54:26,802 - INFO - Epoch [427/4000] train_loss: 7.8217, lr: 0.000500, 0.04s
2023-10-04 19:54:26,841 - INFO - epoch complete!
2023-10-04 19:54:26,841 - INFO - evaluating now!
2023-10-04 19:54:26,841 - INFO - Epoch [428/4000] train_loss: 7.4283, lr: 0.000500, 0.04s
2023-10-04 19:54:26,880 - INFO - epoch complete!
2023-10-04 19:54:26,880 - INFO - evaluating now!
2023-10-04 19:54:26,881 - INFO - Epoch [429/4000] train_loss: 7.5951, lr: 0.000500, 0.04s
2023-10-04 19:54:26,919 - INFO - epoch complete!
2023-10-04 19:54:26,920 - INFO - evaluating now!
2023-10-04 19:54:26,920 - INFO - Epoch [430/4000] train_loss: 7.9683, lr: 0.000500, 0.04s
2023-10-04 19:54:26,958 - INFO - epoch complete!
2023-10-04 19:54:26,958 - INFO - evaluating now!
2023-10-04 19:54:26,959 - INFO - Epoch [431/4000] train_loss: 8.1189, lr: 0.000500, 0.04s
2023-10-04 19:54:26,997 - INFO - epoch complete!
2023-10-04 19:54:26,997 - INFO - evaluating now!
2023-10-04 19:54:26,997 - INFO - Epoch [432/4000] train_loss: 7.1947, lr: 0.000500, 0.04s
2023-10-04 19:54:27,228 - INFO - Saved model at 432
2023-10-04 19:54:27,228 - INFO - Val loss decrease from 7.2231 to 7.1947, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch432.tar
2023-10-04 19:54:27,269 - INFO - epoch complete!
2023-10-04 19:54:27,269 - INFO - evaluating now!
2023-10-04 19:54:27,269 - INFO - Epoch [433/4000] train_loss: 7.5309, lr: 0.000500, 0.04s
2023-10-04 19:54:27,309 - INFO - epoch complete!
2023-10-04 19:54:27,309 - INFO - evaluating now!
2023-10-04 19:54:27,309 - INFO - Epoch [434/4000] train_loss: 8.5817, lr: 0.000500, 0.04s
2023-10-04 19:54:27,349 - INFO - epoch complete!
2023-10-04 19:54:27,349 - INFO - evaluating now!
2023-10-04 19:54:27,349 - INFO - Epoch [435/4000] train_loss: 7.5938, lr: 0.000500, 0.04s
2023-10-04 19:54:27,388 - INFO - epoch complete!
2023-10-04 19:54:27,388 - INFO - evaluating now!
2023-10-04 19:54:27,389 - INFO - Epoch [436/4000] train_loss: 8.3497, lr: 0.000500, 0.04s
2023-10-04 19:54:27,428 - INFO - epoch complete!
2023-10-04 19:54:27,428 - INFO - evaluating now!
2023-10-04 19:54:27,428 - INFO - Epoch [437/4000] train_loss: 8.1039, lr: 0.000500, 0.04s
2023-10-04 19:54:27,467 - INFO - epoch complete!
2023-10-04 19:54:27,467 - INFO - evaluating now!
2023-10-04 19:54:27,468 - INFO - Epoch [438/4000] train_loss: 7.4442, lr: 0.000500, 0.04s
2023-10-04 19:54:27,506 - INFO - epoch complete!
2023-10-04 19:54:27,506 - INFO - evaluating now!
2023-10-04 19:54:27,506 - INFO - Epoch [439/4000] train_loss: 7.8151, lr: 0.000500, 0.04s
2023-10-04 19:54:27,545 - INFO - epoch complete!
2023-10-04 19:54:27,545 - INFO - evaluating now!
2023-10-04 19:54:27,545 - INFO - Epoch [440/4000] train_loss: 7.8656, lr: 0.000500, 0.04s
2023-10-04 19:54:27,584 - INFO - epoch complete!
2023-10-04 19:54:27,584 - INFO - evaluating now!
2023-10-04 19:54:27,584 - INFO - Epoch [441/4000] train_loss: 7.9310, lr: 0.000500, 0.04s
2023-10-04 19:54:27,622 - INFO - epoch complete!
2023-10-04 19:54:27,623 - INFO - evaluating now!
2023-10-04 19:54:27,623 - INFO - Epoch [442/4000] train_loss: 7.7075, lr: 0.000500, 0.04s
2023-10-04 19:54:27,661 - INFO - epoch complete!
2023-10-04 19:54:27,661 - INFO - evaluating now!
2023-10-04 19:54:27,661 - INFO - Epoch [443/4000] train_loss: 7.8530, lr: 0.000500, 0.04s
2023-10-04 19:54:27,700 - INFO - epoch complete!
2023-10-04 19:54:27,700 - INFO - evaluating now!
2023-10-04 19:54:27,700 - INFO - Epoch [444/4000] train_loss: 7.6415, lr: 0.000500, 0.04s
2023-10-04 19:54:27,739 - INFO - epoch complete!
2023-10-04 19:54:27,739 - INFO - evaluating now!
2023-10-04 19:54:27,739 - INFO - Epoch [445/4000] train_loss: 7.9869, lr: 0.000500, 0.04s
2023-10-04 19:54:27,778 - INFO - epoch complete!
2023-10-04 19:54:27,778 - INFO - evaluating now!
2023-10-04 19:54:27,778 - INFO - Epoch [446/4000] train_loss: 7.6803, lr: 0.000500, 0.04s
2023-10-04 19:54:27,816 - INFO - epoch complete!
2023-10-04 19:54:27,817 - INFO - evaluating now!
2023-10-04 19:54:27,817 - INFO - Epoch [447/4000] train_loss: 7.3926, lr: 0.000500, 0.04s
2023-10-04 19:54:27,855 - INFO - epoch complete!
2023-10-04 19:54:27,856 - INFO - evaluating now!
2023-10-04 19:54:27,856 - INFO - Epoch [448/4000] train_loss: 7.3554, lr: 0.000500, 0.04s
2023-10-04 19:54:27,894 - INFO - epoch complete!
2023-10-04 19:54:27,895 - INFO - evaluating now!
2023-10-04 19:54:27,895 - INFO - Epoch [449/4000] train_loss: 7.7544, lr: 0.000500, 0.04s
2023-10-04 19:54:27,933 - INFO - epoch complete!
2023-10-04 19:54:27,934 - INFO - evaluating now!
2023-10-04 19:54:27,934 - INFO - Epoch [450/4000] train_loss: 7.3211, lr: 0.000500, 0.04s
2023-10-04 19:54:27,972 - INFO - epoch complete!
2023-10-04 19:54:27,972 - INFO - evaluating now!
2023-10-04 19:54:27,973 - INFO - Epoch [451/4000] train_loss: 7.5029, lr: 0.000500, 0.04s
2023-10-04 19:54:28,011 - INFO - epoch complete!
2023-10-04 19:54:28,011 - INFO - evaluating now!
2023-10-04 19:54:28,011 - INFO - Epoch [452/4000] train_loss: 7.3448, lr: 0.000500, 0.04s
2023-10-04 19:54:28,050 - INFO - epoch complete!
2023-10-04 19:54:28,050 - INFO - evaluating now!
2023-10-04 19:54:28,050 - INFO - Epoch [453/4000] train_loss: 7.2571, lr: 0.000500, 0.04s
2023-10-04 19:54:28,089 - INFO - epoch complete!
2023-10-04 19:54:28,089 - INFO - evaluating now!
2023-10-04 19:54:28,089 - INFO - Epoch [454/4000] train_loss: 8.1515, lr: 0.000500, 0.04s
2023-10-04 19:54:28,128 - INFO - epoch complete!
2023-10-04 19:54:28,128 - INFO - evaluating now!
2023-10-04 19:54:28,128 - INFO - Epoch [455/4000] train_loss: 7.5488, lr: 0.000500, 0.04s
2023-10-04 19:54:28,166 - INFO - epoch complete!
2023-10-04 19:54:28,167 - INFO - evaluating now!
2023-10-04 19:54:28,167 - INFO - Epoch [456/4000] train_loss: 7.2349, lr: 0.000500, 0.04s
2023-10-04 19:54:28,206 - INFO - epoch complete!
2023-10-04 19:54:28,207 - INFO - evaluating now!
2023-10-04 19:54:28,207 - INFO - Epoch [457/4000] train_loss: 7.1126, lr: 0.000500, 0.04s
2023-10-04 19:54:28,437 - INFO - Saved model at 457
2023-10-04 19:54:28,437 - INFO - Val loss decrease from 7.1947 to 7.1126, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch457.tar
2023-10-04 19:54:28,477 - INFO - epoch complete!
2023-10-04 19:54:28,477 - INFO - evaluating now!
2023-10-04 19:54:28,477 - INFO - Epoch [458/4000] train_loss: 7.5090, lr: 0.000500, 0.04s
2023-10-04 19:54:28,516 - INFO - epoch complete!
2023-10-04 19:54:28,516 - INFO - evaluating now!
2023-10-04 19:54:28,517 - INFO - Epoch [459/4000] train_loss: 7.8923, lr: 0.000500, 0.04s
2023-10-04 19:54:28,555 - INFO - epoch complete!
2023-10-04 19:54:28,556 - INFO - evaluating now!
2023-10-04 19:54:28,556 - INFO - Epoch [460/4000] train_loss: 7.3599, lr: 0.000500, 0.04s
2023-10-04 19:54:28,594 - INFO - epoch complete!
2023-10-04 19:54:28,594 - INFO - evaluating now!
2023-10-04 19:54:28,595 - INFO - Epoch [461/4000] train_loss: 7.0660, lr: 0.000500, 0.04s
2023-10-04 19:54:28,825 - INFO - Saved model at 461
2023-10-04 19:54:28,825 - INFO - Val loss decrease from 7.1126 to 7.0660, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch461.tar
2023-10-04 19:54:28,866 - INFO - epoch complete!
2023-10-04 19:54:28,866 - INFO - evaluating now!
2023-10-04 19:54:28,866 - INFO - Epoch [462/4000] train_loss: 7.1950, lr: 0.000500, 0.04s
2023-10-04 19:54:28,906 - INFO - epoch complete!
2023-10-04 19:54:28,906 - INFO - evaluating now!
2023-10-04 19:54:28,906 - INFO - Epoch [463/4000] train_loss: 7.9343, lr: 0.000500, 0.04s
2023-10-04 19:54:28,945 - INFO - epoch complete!
2023-10-04 19:54:28,946 - INFO - evaluating now!
2023-10-04 19:54:28,946 - INFO - Epoch [464/4000] train_loss: 7.2563, lr: 0.000500, 0.04s
2023-10-04 19:54:28,985 - INFO - epoch complete!
2023-10-04 19:54:28,985 - INFO - evaluating now!
2023-10-04 19:54:28,985 - INFO - Epoch [465/4000] train_loss: 7.1420, lr: 0.000500, 0.04s
2023-10-04 19:54:29,024 - INFO - epoch complete!
2023-10-04 19:54:29,024 - INFO - evaluating now!
2023-10-04 19:54:29,025 - INFO - Epoch [466/4000] train_loss: 7.3000, lr: 0.000500, 0.04s
2023-10-04 19:54:29,064 - INFO - epoch complete!
2023-10-04 19:54:29,064 - INFO - evaluating now!
2023-10-04 19:54:29,064 - INFO - Epoch [467/4000] train_loss: 7.3464, lr: 0.000500, 0.04s
2023-10-04 19:54:29,103 - INFO - epoch complete!
2023-10-04 19:54:29,103 - INFO - evaluating now!
2023-10-04 19:54:29,104 - INFO - Epoch [468/4000] train_loss: 7.9526, lr: 0.000500, 0.04s
2023-10-04 19:54:29,142 - INFO - epoch complete!
2023-10-04 19:54:29,143 - INFO - evaluating now!
2023-10-04 19:54:29,143 - INFO - Epoch [469/4000] train_loss: 7.3690, lr: 0.000500, 0.04s
2023-10-04 19:54:29,181 - INFO - epoch complete!
2023-10-04 19:54:29,182 - INFO - evaluating now!
2023-10-04 19:54:29,182 - INFO - Epoch [470/4000] train_loss: 7.3318, lr: 0.000500, 0.04s
2023-10-04 19:54:29,220 - INFO - epoch complete!
2023-10-04 19:54:29,221 - INFO - evaluating now!
2023-10-04 19:54:29,221 - INFO - Epoch [471/4000] train_loss: 7.7817, lr: 0.000500, 0.04s
2023-10-04 19:54:29,259 - INFO - epoch complete!
2023-10-04 19:54:29,260 - INFO - evaluating now!
2023-10-04 19:54:29,260 - INFO - Epoch [472/4000] train_loss: 6.8996, lr: 0.000500, 0.04s
2023-10-04 19:54:29,491 - INFO - Saved model at 472
2023-10-04 19:54:29,491 - INFO - Val loss decrease from 7.0660 to 6.8996, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch472.tar
2023-10-04 19:54:29,531 - INFO - epoch complete!
2023-10-04 19:54:29,531 - INFO - evaluating now!
2023-10-04 19:54:29,531 - INFO - Epoch [473/4000] train_loss: 7.0875, lr: 0.000500, 0.04s
2023-10-04 19:54:29,571 - INFO - epoch complete!
2023-10-04 19:54:29,571 - INFO - evaluating now!
2023-10-04 19:54:29,571 - INFO - Epoch [474/4000] train_loss: 7.1569, lr: 0.000500, 0.04s
2023-10-04 19:54:29,611 - INFO - epoch complete!
2023-10-04 19:54:29,611 - INFO - evaluating now!
2023-10-04 19:54:29,611 - INFO - Epoch [475/4000] train_loss: 7.3949, lr: 0.000500, 0.04s
2023-10-04 19:54:29,652 - INFO - epoch complete!
2023-10-04 19:54:29,652 - INFO - evaluating now!
2023-10-04 19:54:29,652 - INFO - Epoch [476/4000] train_loss: 7.2911, lr: 0.000500, 0.04s
2023-10-04 19:54:29,692 - INFO - epoch complete!
2023-10-04 19:54:29,692 - INFO - evaluating now!
2023-10-04 19:54:29,692 - INFO - Epoch [477/4000] train_loss: 7.3661, lr: 0.000500, 0.04s
2023-10-04 19:54:29,731 - INFO - epoch complete!
2023-10-04 19:54:29,732 - INFO - evaluating now!
2023-10-04 19:54:29,732 - INFO - Epoch [478/4000] train_loss: 7.1260, lr: 0.000500, 0.04s
2023-10-04 19:54:29,771 - INFO - epoch complete!
2023-10-04 19:54:29,771 - INFO - evaluating now!
2023-10-04 19:54:29,771 - INFO - Epoch [479/4000] train_loss: 7.3002, lr: 0.000500, 0.04s
2023-10-04 19:54:29,810 - INFO - epoch complete!
2023-10-04 19:54:29,810 - INFO - evaluating now!
2023-10-04 19:54:29,810 - INFO - Epoch [480/4000] train_loss: 7.2530, lr: 0.000500, 0.04s
2023-10-04 19:54:29,849 - INFO - epoch complete!
2023-10-04 19:54:29,849 - INFO - evaluating now!
2023-10-04 19:54:29,849 - INFO - Epoch [481/4000] train_loss: 6.9500, lr: 0.000500, 0.04s
2023-10-04 19:54:29,888 - INFO - epoch complete!
2023-10-04 19:54:29,888 - INFO - evaluating now!
2023-10-04 19:54:29,889 - INFO - Epoch [482/4000] train_loss: 7.8515, lr: 0.000500, 0.04s
2023-10-04 19:54:29,927 - INFO - epoch complete!
2023-10-04 19:54:29,927 - INFO - evaluating now!
2023-10-04 19:54:29,927 - INFO - Epoch [483/4000] train_loss: 6.9923, lr: 0.000500, 0.04s
2023-10-04 19:54:29,966 - INFO - epoch complete!
2023-10-04 19:54:29,966 - INFO - evaluating now!
2023-10-04 19:54:29,966 - INFO - Epoch [484/4000] train_loss: 7.3247, lr: 0.000500, 0.04s
2023-10-04 19:54:30,005 - INFO - epoch complete!
2023-10-04 19:54:30,005 - INFO - evaluating now!
2023-10-04 19:54:30,006 - INFO - Epoch [485/4000] train_loss: 6.8062, lr: 0.000500, 0.04s
2023-10-04 19:54:30,239 - INFO - Saved model at 485
2023-10-04 19:54:30,240 - INFO - Val loss decrease from 6.8996 to 6.8062, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch485.tar
2023-10-04 19:54:30,280 - INFO - epoch complete!
2023-10-04 19:54:30,280 - INFO - evaluating now!
2023-10-04 19:54:30,280 - INFO - Epoch [486/4000] train_loss: 7.5055, lr: 0.000500, 0.04s
2023-10-04 19:54:30,320 - INFO - epoch complete!
2023-10-04 19:54:30,320 - INFO - evaluating now!
2023-10-04 19:54:30,320 - INFO - Epoch [487/4000] train_loss: 7.3380, lr: 0.000500, 0.04s
2023-10-04 19:54:30,359 - INFO - epoch complete!
2023-10-04 19:54:30,360 - INFO - evaluating now!
2023-10-04 19:54:30,360 - INFO - Epoch [488/4000] train_loss: 6.9122, lr: 0.000500, 0.04s
2023-10-04 19:54:30,399 - INFO - epoch complete!
2023-10-04 19:54:30,399 - INFO - evaluating now!
2023-10-04 19:54:30,399 - INFO - Epoch [489/4000] train_loss: 6.8737, lr: 0.000500, 0.04s
2023-10-04 19:54:30,438 - INFO - epoch complete!
2023-10-04 19:54:30,438 - INFO - evaluating now!
2023-10-04 19:54:30,438 - INFO - Epoch [490/4000] train_loss: 7.3022, lr: 0.000500, 0.04s
2023-10-04 19:54:30,477 - INFO - epoch complete!
2023-10-04 19:54:30,478 - INFO - evaluating now!
2023-10-04 19:54:30,478 - INFO - Epoch [491/4000] train_loss: 7.0425, lr: 0.000500, 0.04s
2023-10-04 19:54:30,517 - INFO - epoch complete!
2023-10-04 19:54:30,517 - INFO - evaluating now!
2023-10-04 19:54:30,517 - INFO - Epoch [492/4000] train_loss: 7.3950, lr: 0.000500, 0.04s
2023-10-04 19:54:30,556 - INFO - epoch complete!
2023-10-04 19:54:30,556 - INFO - evaluating now!
2023-10-04 19:54:30,557 - INFO - Epoch [493/4000] train_loss: 7.2625, lr: 0.000500, 0.04s
2023-10-04 19:54:30,595 - INFO - epoch complete!
2023-10-04 19:54:30,595 - INFO - evaluating now!
2023-10-04 19:54:30,596 - INFO - Epoch [494/4000] train_loss: 7.5640, lr: 0.000500, 0.04s
2023-10-04 19:54:30,634 - INFO - epoch complete!
2023-10-04 19:54:30,635 - INFO - evaluating now!
2023-10-04 19:54:30,635 - INFO - Epoch [495/4000] train_loss: 7.2910, lr: 0.000500, 0.04s
2023-10-04 19:54:30,675 - INFO - epoch complete!
2023-10-04 19:54:30,675 - INFO - evaluating now!
2023-10-04 19:54:30,675 - INFO - Epoch [496/4000] train_loss: 7.0853, lr: 0.000500, 0.04s
2023-10-04 19:54:30,714 - INFO - epoch complete!
2023-10-04 19:54:30,714 - INFO - evaluating now!
2023-10-04 19:54:30,714 - INFO - Epoch [497/4000] train_loss: 6.8998, lr: 0.000500, 0.04s
2023-10-04 19:54:30,753 - INFO - epoch complete!
2023-10-04 19:54:30,753 - INFO - evaluating now!
2023-10-04 19:54:30,754 - INFO - Epoch [498/4000] train_loss: 6.7724, lr: 0.000500, 0.04s
2023-10-04 19:54:30,984 - INFO - Saved model at 498
2023-10-04 19:54:30,984 - INFO - Val loss decrease from 6.8062 to 6.7724, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch498.tar
2023-10-04 19:54:31,025 - INFO - epoch complete!
2023-10-04 19:54:31,025 - INFO - evaluating now!
2023-10-04 19:54:31,025 - INFO - Epoch [499/4000] train_loss: 7.2613, lr: 0.000500, 0.04s
2023-10-04 19:54:31,065 - INFO - epoch complete!
2023-10-04 19:54:31,065 - INFO - evaluating now!
2023-10-04 19:54:31,065 - INFO - Epoch [500/4000] train_loss: 6.5436, lr: 0.000500, 0.04s
2023-10-04 19:54:31,296 - INFO - Saved model at 500
2023-10-04 19:54:31,296 - INFO - Val loss decrease from 6.7724 to 6.5436, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch500.tar
2023-10-04 19:54:31,336 - INFO - epoch complete!
2023-10-04 19:54:31,336 - INFO - evaluating now!
2023-10-04 19:54:31,337 - INFO - Epoch [501/4000] train_loss: 7.0207, lr: 0.000500, 0.04s
2023-10-04 19:54:31,376 - INFO - epoch complete!
2023-10-04 19:54:31,376 - INFO - evaluating now!
2023-10-04 19:54:31,376 - INFO - Epoch [502/4000] train_loss: 7.4315, lr: 0.000500, 0.04s
2023-10-04 19:54:31,416 - INFO - epoch complete!
2023-10-04 19:54:31,416 - INFO - evaluating now!
2023-10-04 19:54:31,416 - INFO - Epoch [503/4000] train_loss: 7.4389, lr: 0.000500, 0.04s
2023-10-04 19:54:31,456 - INFO - epoch complete!
2023-10-04 19:54:31,456 - INFO - evaluating now!
2023-10-04 19:54:31,456 - INFO - Epoch [504/4000] train_loss: 6.8900, lr: 0.000500, 0.04s
2023-10-04 19:54:31,495 - INFO - epoch complete!
2023-10-04 19:54:31,496 - INFO - evaluating now!
2023-10-04 19:54:31,496 - INFO - Epoch [505/4000] train_loss: 7.1798, lr: 0.000500, 0.04s
2023-10-04 19:54:31,535 - INFO - epoch complete!
2023-10-04 19:54:31,535 - INFO - evaluating now!
2023-10-04 19:54:31,535 - INFO - Epoch [506/4000] train_loss: 6.7812, lr: 0.000500, 0.04s
2023-10-04 19:54:31,574 - INFO - epoch complete!
2023-10-04 19:54:31,575 - INFO - evaluating now!
2023-10-04 19:54:31,575 - INFO - Epoch [507/4000] train_loss: 6.8789, lr: 0.000500, 0.04s
2023-10-04 19:54:31,613 - INFO - epoch complete!
2023-10-04 19:54:31,613 - INFO - evaluating now!
2023-10-04 19:54:31,613 - INFO - Epoch [508/4000] train_loss: 7.3034, lr: 0.000500, 0.04s
2023-10-04 19:54:31,652 - INFO - epoch complete!
2023-10-04 19:54:31,652 - INFO - evaluating now!
2023-10-04 19:54:31,652 - INFO - Epoch [509/4000] train_loss: 7.2020, lr: 0.000500, 0.04s
2023-10-04 19:54:31,690 - INFO - epoch complete!
2023-10-04 19:54:31,691 - INFO - evaluating now!
2023-10-04 19:54:31,691 - INFO - Epoch [510/4000] train_loss: 6.8640, lr: 0.000500, 0.04s
2023-10-04 19:54:31,729 - INFO - epoch complete!
2023-10-04 19:54:31,729 - INFO - evaluating now!
2023-10-04 19:54:31,730 - INFO - Epoch [511/4000] train_loss: 6.5751, lr: 0.000500, 0.04s
2023-10-04 19:54:31,768 - INFO - epoch complete!
2023-10-04 19:54:31,768 - INFO - evaluating now!
2023-10-04 19:54:31,768 - INFO - Epoch [512/4000] train_loss: 6.7363, lr: 0.000500, 0.04s
2023-10-04 19:54:31,807 - INFO - epoch complete!
2023-10-04 19:54:31,807 - INFO - evaluating now!
2023-10-04 19:54:31,807 - INFO - Epoch [513/4000] train_loss: 6.8198, lr: 0.000500, 0.04s
2023-10-04 19:54:31,845 - INFO - epoch complete!
2023-10-04 19:54:31,845 - INFO - evaluating now!
2023-10-04 19:54:31,846 - INFO - Epoch [514/4000] train_loss: 6.9410, lr: 0.000500, 0.04s
2023-10-04 19:54:31,884 - INFO - epoch complete!
2023-10-04 19:54:31,884 - INFO - evaluating now!
2023-10-04 19:54:31,884 - INFO - Epoch [515/4000] train_loss: 6.9040, lr: 0.000500, 0.04s
2023-10-04 19:54:31,923 - INFO - epoch complete!
2023-10-04 19:54:31,923 - INFO - evaluating now!
2023-10-04 19:54:31,923 - INFO - Epoch [516/4000] train_loss: 6.6928, lr: 0.000500, 0.04s
2023-10-04 19:54:31,962 - INFO - epoch complete!
2023-10-04 19:54:31,962 - INFO - evaluating now!
2023-10-04 19:54:31,962 - INFO - Epoch [517/4000] train_loss: 6.6435, lr: 0.000500, 0.04s
2023-10-04 19:54:32,000 - INFO - epoch complete!
2023-10-04 19:54:32,001 - INFO - evaluating now!
2023-10-04 19:54:32,001 - INFO - Epoch [518/4000] train_loss: 6.9305, lr: 0.000500, 0.04s
2023-10-04 19:54:32,039 - INFO - epoch complete!
2023-10-04 19:54:32,039 - INFO - evaluating now!
2023-10-04 19:54:32,039 - INFO - Epoch [519/4000] train_loss: 7.1595, lr: 0.000500, 0.04s
2023-10-04 19:54:32,078 - INFO - epoch complete!
2023-10-04 19:54:32,078 - INFO - evaluating now!
2023-10-04 19:54:32,078 - INFO - Epoch [520/4000] train_loss: 6.2170, lr: 0.000500, 0.04s
2023-10-04 19:54:32,311 - INFO - Saved model at 520
2023-10-04 19:54:32,311 - INFO - Val loss decrease from 6.5436 to 6.2170, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch520.tar
2023-10-04 19:54:32,350 - INFO - epoch complete!
2023-10-04 19:54:32,351 - INFO - evaluating now!
2023-10-04 19:54:32,351 - INFO - Epoch [521/4000] train_loss: 7.0180, lr: 0.000500, 0.04s
2023-10-04 19:54:32,389 - INFO - epoch complete!
2023-10-04 19:54:32,389 - INFO - evaluating now!
2023-10-04 19:54:32,390 - INFO - Epoch [522/4000] train_loss: 7.2815, lr: 0.000500, 0.04s
2023-10-04 19:54:32,428 - INFO - epoch complete!
2023-10-04 19:54:32,428 - INFO - evaluating now!
2023-10-04 19:54:32,428 - INFO - Epoch [523/4000] train_loss: 6.7034, lr: 0.000500, 0.04s
2023-10-04 19:54:32,467 - INFO - epoch complete!
2023-10-04 19:54:32,467 - INFO - evaluating now!
2023-10-04 19:54:32,467 - INFO - Epoch [524/4000] train_loss: 7.0679, lr: 0.000500, 0.04s
2023-10-04 19:54:32,506 - INFO - epoch complete!
2023-10-04 19:54:32,506 - INFO - evaluating now!
2023-10-04 19:54:32,506 - INFO - Epoch [525/4000] train_loss: 6.6503, lr: 0.000500, 0.04s
2023-10-04 19:54:32,545 - INFO - epoch complete!
2023-10-04 19:54:32,545 - INFO - evaluating now!
2023-10-04 19:54:32,545 - INFO - Epoch [526/4000] train_loss: 6.9271, lr: 0.000500, 0.04s
2023-10-04 19:54:32,583 - INFO - epoch complete!
2023-10-04 19:54:32,584 - INFO - evaluating now!
2023-10-04 19:54:32,584 - INFO - Epoch [527/4000] train_loss: 6.6847, lr: 0.000500, 0.04s
2023-10-04 19:54:32,622 - INFO - epoch complete!
2023-10-04 19:54:32,622 - INFO - evaluating now!
2023-10-04 19:54:32,622 - INFO - Epoch [528/4000] train_loss: 6.5830, lr: 0.000500, 0.04s
2023-10-04 19:54:32,661 - INFO - epoch complete!
2023-10-04 19:54:32,661 - INFO - evaluating now!
2023-10-04 19:54:32,661 - INFO - Epoch [529/4000] train_loss: 6.7364, lr: 0.000500, 0.04s
2023-10-04 19:54:32,700 - INFO - epoch complete!
2023-10-04 19:54:32,700 - INFO - evaluating now!
2023-10-04 19:54:32,700 - INFO - Epoch [530/4000] train_loss: 7.1250, lr: 0.000500, 0.04s
2023-10-04 19:54:32,738 - INFO - epoch complete!
2023-10-04 19:54:32,739 - INFO - evaluating now!
2023-10-04 19:54:32,739 - INFO - Epoch [531/4000] train_loss: 6.8996, lr: 0.000500, 0.04s
2023-10-04 19:54:32,777 - INFO - epoch complete!
2023-10-04 19:54:32,777 - INFO - evaluating now!
2023-10-04 19:54:32,778 - INFO - Epoch [532/4000] train_loss: 6.8227, lr: 0.000500, 0.04s
2023-10-04 19:54:32,816 - INFO - epoch complete!
2023-10-04 19:54:32,816 - INFO - evaluating now!
2023-10-04 19:54:32,816 - INFO - Epoch [533/4000] train_loss: 7.0622, lr: 0.000500, 0.04s
2023-10-04 19:54:32,855 - INFO - epoch complete!
2023-10-04 19:54:32,855 - INFO - evaluating now!
2023-10-04 19:54:32,855 - INFO - Epoch [534/4000] train_loss: 6.7489, lr: 0.000500, 0.04s
2023-10-04 19:54:32,894 - INFO - epoch complete!
2023-10-04 19:54:32,894 - INFO - evaluating now!
2023-10-04 19:54:32,894 - INFO - Epoch [535/4000] train_loss: 6.8759, lr: 0.000500, 0.04s
2023-10-04 19:54:32,934 - INFO - epoch complete!
2023-10-04 19:54:32,934 - INFO - evaluating now!
2023-10-04 19:54:32,934 - INFO - Epoch [536/4000] train_loss: 6.6716, lr: 0.000500, 0.04s
2023-10-04 19:54:32,973 - INFO - epoch complete!
2023-10-04 19:54:32,973 - INFO - evaluating now!
2023-10-04 19:54:32,973 - INFO - Epoch [537/4000] train_loss: 7.1106, lr: 0.000500, 0.04s
2023-10-04 19:54:33,011 - INFO - epoch complete!
2023-10-04 19:54:33,012 - INFO - evaluating now!
2023-10-04 19:54:33,012 - INFO - Epoch [538/4000] train_loss: 6.6166, lr: 0.000500, 0.04s
2023-10-04 19:54:33,050 - INFO - epoch complete!
2023-10-04 19:54:33,050 - INFO - evaluating now!
2023-10-04 19:54:33,050 - INFO - Epoch [539/4000] train_loss: 7.1532, lr: 0.000500, 0.04s
2023-10-04 19:54:33,089 - INFO - epoch complete!
2023-10-04 19:54:33,089 - INFO - evaluating now!
2023-10-04 19:54:33,089 - INFO - Epoch [540/4000] train_loss: 6.5821, lr: 0.000500, 0.04s
2023-10-04 19:54:33,128 - INFO - epoch complete!
2023-10-04 19:54:33,128 - INFO - evaluating now!
2023-10-04 19:54:33,128 - INFO - Epoch [541/4000] train_loss: 7.0924, lr: 0.000500, 0.04s
2023-10-04 19:54:33,167 - INFO - epoch complete!
2023-10-04 19:54:33,167 - INFO - evaluating now!
2023-10-04 19:54:33,167 - INFO - Epoch [542/4000] train_loss: 6.9800, lr: 0.000500, 0.04s
2023-10-04 19:54:33,205 - INFO - epoch complete!
2023-10-04 19:54:33,206 - INFO - evaluating now!
2023-10-04 19:54:33,206 - INFO - Epoch [543/4000] train_loss: 6.6615, lr: 0.000500, 0.04s
2023-10-04 19:54:33,244 - INFO - epoch complete!
2023-10-04 19:54:33,244 - INFO - evaluating now!
2023-10-04 19:54:33,244 - INFO - Epoch [544/4000] train_loss: 6.4607, lr: 0.000500, 0.04s
2023-10-04 19:54:33,283 - INFO - epoch complete!
2023-10-04 19:54:33,283 - INFO - evaluating now!
2023-10-04 19:54:33,283 - INFO - Epoch [545/4000] train_loss: 6.7912, lr: 0.000500, 0.04s
2023-10-04 19:54:33,321 - INFO - epoch complete!
2023-10-04 19:54:33,321 - INFO - evaluating now!
2023-10-04 19:54:33,322 - INFO - Epoch [546/4000] train_loss: 6.9944, lr: 0.000500, 0.04s
2023-10-04 19:54:33,360 - INFO - epoch complete!
2023-10-04 19:54:33,360 - INFO - evaluating now!
2023-10-04 19:54:33,360 - INFO - Epoch [547/4000] train_loss: 6.7832, lr: 0.000500, 0.04s
2023-10-04 19:54:33,399 - INFO - epoch complete!
2023-10-04 19:54:33,399 - INFO - evaluating now!
2023-10-04 19:54:33,399 - INFO - Epoch [548/4000] train_loss: 6.9183, lr: 0.000500, 0.04s
2023-10-04 19:54:33,438 - INFO - epoch complete!
2023-10-04 19:54:33,439 - INFO - evaluating now!
2023-10-04 19:54:33,439 - INFO - Epoch [549/4000] train_loss: 6.9425, lr: 0.000500, 0.04s
2023-10-04 19:54:33,478 - INFO - epoch complete!
2023-10-04 19:54:33,479 - INFO - evaluating now!
2023-10-04 19:54:33,479 - INFO - Epoch [550/4000] train_loss: 6.8534, lr: 0.000500, 0.04s
2023-10-04 19:54:33,518 - INFO - epoch complete!
2023-10-04 19:54:33,518 - INFO - evaluating now!
2023-10-04 19:54:33,518 - INFO - Epoch [551/4000] train_loss: 6.4770, lr: 0.000500, 0.04s
2023-10-04 19:54:33,557 - INFO - epoch complete!
2023-10-04 19:54:33,557 - INFO - evaluating now!
2023-10-04 19:54:33,557 - INFO - Epoch [552/4000] train_loss: 6.4559, lr: 0.000500, 0.04s
2023-10-04 19:54:33,595 - INFO - epoch complete!
2023-10-04 19:54:33,596 - INFO - evaluating now!
2023-10-04 19:54:33,596 - INFO - Epoch [553/4000] train_loss: 7.6016, lr: 0.000500, 0.04s
2023-10-04 19:54:33,634 - INFO - epoch complete!
2023-10-04 19:54:33,635 - INFO - evaluating now!
2023-10-04 19:54:33,635 - INFO - Epoch [554/4000] train_loss: 6.8166, lr: 0.000500, 0.04s
2023-10-04 19:54:33,673 - INFO - epoch complete!
2023-10-04 19:54:33,673 - INFO - evaluating now!
2023-10-04 19:54:33,674 - INFO - Epoch [555/4000] train_loss: 6.8697, lr: 0.000500, 0.04s
2023-10-04 19:54:33,712 - INFO - epoch complete!
2023-10-04 19:54:33,712 - INFO - evaluating now!
2023-10-04 19:54:33,712 - INFO - Epoch [556/4000] train_loss: 7.0205, lr: 0.000500, 0.04s
2023-10-04 19:54:33,751 - INFO - epoch complete!
2023-10-04 19:54:33,751 - INFO - evaluating now!
2023-10-04 19:54:33,751 - INFO - Epoch [557/4000] train_loss: 6.4388, lr: 0.000500, 0.04s
2023-10-04 19:54:33,789 - INFO - epoch complete!
2023-10-04 19:54:33,790 - INFO - evaluating now!
2023-10-04 19:54:33,790 - INFO - Epoch [558/4000] train_loss: 7.1217, lr: 0.000500, 0.04s
2023-10-04 19:54:33,828 - INFO - epoch complete!
2023-10-04 19:54:33,828 - INFO - evaluating now!
2023-10-04 19:54:33,828 - INFO - Epoch [559/4000] train_loss: 6.8726, lr: 0.000500, 0.04s
2023-10-04 19:54:33,867 - INFO - epoch complete!
2023-10-04 19:54:33,867 - INFO - evaluating now!
2023-10-04 19:54:33,867 - INFO - Epoch [560/4000] train_loss: 6.8249, lr: 0.000500, 0.04s
2023-10-04 19:54:33,906 - INFO - epoch complete!
2023-10-04 19:54:33,906 - INFO - evaluating now!
2023-10-04 19:54:33,906 - INFO - Epoch [561/4000] train_loss: 6.7100, lr: 0.000500, 0.04s
2023-10-04 19:54:33,947 - INFO - epoch complete!
2023-10-04 19:54:33,948 - INFO - evaluating now!
2023-10-04 19:54:33,948 - INFO - Epoch [562/4000] train_loss: 6.6337, lr: 0.000500, 0.04s
2023-10-04 19:54:33,986 - INFO - epoch complete!
2023-10-04 19:54:33,986 - INFO - evaluating now!
2023-10-04 19:54:33,987 - INFO - Epoch [563/4000] train_loss: 6.6369, lr: 0.000500, 0.04s
2023-10-04 19:54:34,025 - INFO - epoch complete!
2023-10-04 19:54:34,025 - INFO - evaluating now!
2023-10-04 19:54:34,025 - INFO - Epoch [564/4000] train_loss: 6.5672, lr: 0.000500, 0.04s
2023-10-04 19:54:34,064 - INFO - epoch complete!
2023-10-04 19:54:34,064 - INFO - evaluating now!
2023-10-04 19:54:34,064 - INFO - Epoch [565/4000] train_loss: 6.5198, lr: 0.000500, 0.04s
2023-10-04 19:54:34,103 - INFO - epoch complete!
2023-10-04 19:54:34,103 - INFO - evaluating now!
2023-10-04 19:54:34,103 - INFO - Epoch [566/4000] train_loss: 6.5495, lr: 0.000500, 0.04s
2023-10-04 19:54:34,142 - INFO - epoch complete!
2023-10-04 19:54:34,142 - INFO - evaluating now!
2023-10-04 19:54:34,142 - INFO - Epoch [567/4000] train_loss: 5.8848, lr: 0.000500, 0.04s
2023-10-04 19:54:34,502 - INFO - Saved model at 567
2023-10-04 19:54:34,502 - INFO - Val loss decrease from 6.2170 to 5.8848, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch567.tar
2023-10-04 19:54:34,542 - INFO - epoch complete!
2023-10-04 19:54:34,542 - INFO - evaluating now!
2023-10-04 19:54:34,542 - INFO - Epoch [568/4000] train_loss: 6.5531, lr: 0.000500, 0.04s
2023-10-04 19:54:34,582 - INFO - epoch complete!
2023-10-04 19:54:34,582 - INFO - evaluating now!
2023-10-04 19:54:34,582 - INFO - Epoch [569/4000] train_loss: 6.7294, lr: 0.000500, 0.04s
2023-10-04 19:54:34,622 - INFO - epoch complete!
2023-10-04 19:54:34,622 - INFO - evaluating now!
2023-10-04 19:54:34,622 - INFO - Epoch [570/4000] train_loss: 6.6014, lr: 0.000500, 0.04s
2023-10-04 19:54:34,661 - INFO - epoch complete!
2023-10-04 19:54:34,661 - INFO - evaluating now!
2023-10-04 19:54:34,662 - INFO - Epoch [571/4000] train_loss: 6.6058, lr: 0.000500, 0.04s
2023-10-04 19:54:34,700 - INFO - epoch complete!
2023-10-04 19:54:34,701 - INFO - evaluating now!
2023-10-04 19:54:34,701 - INFO - Epoch [572/4000] train_loss: 6.3079, lr: 0.000500, 0.04s
2023-10-04 19:54:34,739 - INFO - epoch complete!
2023-10-04 19:54:34,739 - INFO - evaluating now!
2023-10-04 19:54:34,740 - INFO - Epoch [573/4000] train_loss: 6.6576, lr: 0.000500, 0.04s
2023-10-04 19:54:34,778 - INFO - epoch complete!
2023-10-04 19:54:34,778 - INFO - evaluating now!
2023-10-04 19:54:34,779 - INFO - Epoch [574/4000] train_loss: 6.6470, lr: 0.000500, 0.04s
2023-10-04 19:54:34,817 - INFO - epoch complete!
2023-10-04 19:54:34,817 - INFO - evaluating now!
2023-10-04 19:54:34,817 - INFO - Epoch [575/4000] train_loss: 6.5858, lr: 0.000500, 0.04s
2023-10-04 19:54:34,856 - INFO - epoch complete!
2023-10-04 19:54:34,856 - INFO - evaluating now!
2023-10-04 19:54:34,856 - INFO - Epoch [576/4000] train_loss: 6.9579, lr: 0.000500, 0.04s
2023-10-04 19:54:34,895 - INFO - epoch complete!
2023-10-04 19:54:34,895 - INFO - evaluating now!
2023-10-04 19:54:34,895 - INFO - Epoch [577/4000] train_loss: 6.8015, lr: 0.000500, 0.04s
2023-10-04 19:54:34,934 - INFO - epoch complete!
2023-10-04 19:54:34,934 - INFO - evaluating now!
2023-10-04 19:54:34,934 - INFO - Epoch [578/4000] train_loss: 6.6158, lr: 0.000500, 0.04s
2023-10-04 19:54:34,973 - INFO - epoch complete!
2023-10-04 19:54:34,973 - INFO - evaluating now!
2023-10-04 19:54:34,973 - INFO - Epoch [579/4000] train_loss: 5.9892, lr: 0.000500, 0.04s
2023-10-04 19:54:35,012 - INFO - epoch complete!
2023-10-04 19:54:35,012 - INFO - evaluating now!
2023-10-04 19:54:35,012 - INFO - Epoch [580/4000] train_loss: 6.5470, lr: 0.000500, 0.04s
2023-10-04 19:54:35,050 - INFO - epoch complete!
2023-10-04 19:54:35,051 - INFO - evaluating now!
2023-10-04 19:54:35,051 - INFO - Epoch [581/4000] train_loss: 6.3230, lr: 0.000500, 0.04s
2023-10-04 19:54:35,089 - INFO - epoch complete!
2023-10-04 19:54:35,089 - INFO - evaluating now!
2023-10-04 19:54:35,090 - INFO - Epoch [582/4000] train_loss: 6.5224, lr: 0.000500, 0.04s
2023-10-04 19:54:35,128 - INFO - epoch complete!
2023-10-04 19:54:35,128 - INFO - evaluating now!
2023-10-04 19:54:35,128 - INFO - Epoch [583/4000] train_loss: 6.1822, lr: 0.000500, 0.04s
2023-10-04 19:54:35,167 - INFO - epoch complete!
2023-10-04 19:54:35,167 - INFO - evaluating now!
2023-10-04 19:54:35,168 - INFO - Epoch [584/4000] train_loss: 6.3583, lr: 0.000500, 0.04s
2023-10-04 19:54:35,206 - INFO - epoch complete!
2023-10-04 19:54:35,206 - INFO - evaluating now!
2023-10-04 19:54:35,206 - INFO - Epoch [585/4000] train_loss: 6.3371, lr: 0.000500, 0.04s
2023-10-04 19:54:35,245 - INFO - epoch complete!
2023-10-04 19:54:35,245 - INFO - evaluating now!
2023-10-04 19:54:35,245 - INFO - Epoch [586/4000] train_loss: 6.0874, lr: 0.000500, 0.04s
2023-10-04 19:54:35,283 - INFO - epoch complete!
2023-10-04 19:54:35,284 - INFO - evaluating now!
2023-10-04 19:54:35,284 - INFO - Epoch [587/4000] train_loss: 6.4814, lr: 0.000500, 0.04s
2023-10-04 19:54:35,322 - INFO - epoch complete!
2023-10-04 19:54:35,322 - INFO - evaluating now!
2023-10-04 19:54:35,322 - INFO - Epoch [588/4000] train_loss: 6.5656, lr: 0.000500, 0.04s
2023-10-04 19:54:35,361 - INFO - epoch complete!
2023-10-04 19:54:35,361 - INFO - evaluating now!
2023-10-04 19:54:35,361 - INFO - Epoch [589/4000] train_loss: 6.4833, lr: 0.000500, 0.04s
2023-10-04 19:54:35,400 - INFO - epoch complete!
2023-10-04 19:54:35,400 - INFO - evaluating now!
2023-10-04 19:54:35,400 - INFO - Epoch [590/4000] train_loss: 6.6159, lr: 0.000500, 0.04s
2023-10-04 19:54:35,439 - INFO - epoch complete!
2023-10-04 19:54:35,439 - INFO - evaluating now!
2023-10-04 19:54:35,439 - INFO - Epoch [591/4000] train_loss: 6.3537, lr: 0.000500, 0.04s
2023-10-04 19:54:35,478 - INFO - epoch complete!
2023-10-04 19:54:35,478 - INFO - evaluating now!
2023-10-04 19:54:35,478 - INFO - Epoch [592/4000] train_loss: 6.6371, lr: 0.000500, 0.04s
2023-10-04 19:54:35,517 - INFO - epoch complete!
2023-10-04 19:54:35,517 - INFO - evaluating now!
2023-10-04 19:54:35,517 - INFO - Epoch [593/4000] train_loss: 6.6842, lr: 0.000500, 0.04s
2023-10-04 19:54:35,556 - INFO - epoch complete!
2023-10-04 19:54:35,556 - INFO - evaluating now!
2023-10-04 19:54:35,556 - INFO - Epoch [594/4000] train_loss: 6.9437, lr: 0.000500, 0.04s
2023-10-04 19:54:35,595 - INFO - epoch complete!
2023-10-04 19:54:35,595 - INFO - evaluating now!
2023-10-04 19:54:35,595 - INFO - Epoch [595/4000] train_loss: 6.5769, lr: 0.000500, 0.04s
2023-10-04 19:54:35,633 - INFO - epoch complete!
2023-10-04 19:54:35,634 - INFO - evaluating now!
2023-10-04 19:54:35,634 - INFO - Epoch [596/4000] train_loss: 6.7142, lr: 0.000500, 0.04s
2023-10-04 19:54:35,672 - INFO - epoch complete!
2023-10-04 19:54:35,673 - INFO - evaluating now!
2023-10-04 19:54:35,673 - INFO - Epoch [597/4000] train_loss: 6.7683, lr: 0.000500, 0.04s
2023-10-04 19:54:35,711 - INFO - epoch complete!
2023-10-04 19:54:35,712 - INFO - evaluating now!
2023-10-04 19:54:35,712 - INFO - Epoch [598/4000] train_loss: 6.3358, lr: 0.000500, 0.04s
2023-10-04 19:54:35,750 - INFO - epoch complete!
2023-10-04 19:54:35,750 - INFO - evaluating now!
2023-10-04 19:54:35,751 - INFO - Epoch [599/4000] train_loss: 6.1442, lr: 0.000500, 0.04s
2023-10-04 19:54:35,789 - INFO - epoch complete!
2023-10-04 19:54:35,789 - INFO - evaluating now!
2023-10-04 19:54:35,789 - INFO - Epoch [600/4000] train_loss: 6.2551, lr: 0.000500, 0.04s
2023-10-04 19:54:35,828 - INFO - epoch complete!
2023-10-04 19:54:35,828 - INFO - evaluating now!
2023-10-04 19:54:35,828 - INFO - Epoch [601/4000] train_loss: 6.5763, lr: 0.000500, 0.04s
2023-10-04 19:54:35,866 - INFO - epoch complete!
2023-10-04 19:54:35,867 - INFO - evaluating now!
2023-10-04 19:54:35,867 - INFO - Epoch [602/4000] train_loss: 6.4518, lr: 0.000500, 0.04s
2023-10-04 19:54:35,905 - INFO - epoch complete!
2023-10-04 19:54:35,906 - INFO - evaluating now!
2023-10-04 19:54:35,906 - INFO - Epoch [603/4000] train_loss: 6.3853, lr: 0.000500, 0.04s
2023-10-04 19:54:35,944 - INFO - epoch complete!
2023-10-04 19:54:35,945 - INFO - evaluating now!
2023-10-04 19:54:35,945 - INFO - Epoch [604/4000] train_loss: 6.6057, lr: 0.000500, 0.04s
2023-10-04 19:54:35,983 - INFO - epoch complete!
2023-10-04 19:54:35,984 - INFO - evaluating now!
2023-10-04 19:54:35,984 - INFO - Epoch [605/4000] train_loss: 6.4045, lr: 0.000500, 0.04s
2023-10-04 19:54:36,022 - INFO - epoch complete!
2023-10-04 19:54:36,022 - INFO - evaluating now!
2023-10-04 19:54:36,022 - INFO - Epoch [606/4000] train_loss: 6.3359, lr: 0.000500, 0.04s
2023-10-04 19:54:36,061 - INFO - epoch complete!
2023-10-04 19:54:36,061 - INFO - evaluating now!
2023-10-04 19:54:36,061 - INFO - Epoch [607/4000] train_loss: 6.3567, lr: 0.000500, 0.04s
2023-10-04 19:54:36,100 - INFO - epoch complete!
2023-10-04 19:54:36,101 - INFO - evaluating now!
2023-10-04 19:54:36,101 - INFO - Epoch [608/4000] train_loss: 6.3052, lr: 0.000500, 0.04s
2023-10-04 19:54:36,140 - INFO - epoch complete!
2023-10-04 19:54:36,140 - INFO - evaluating now!
2023-10-04 19:54:36,140 - INFO - Epoch [609/4000] train_loss: 6.2155, lr: 0.000500, 0.04s
2023-10-04 19:54:36,179 - INFO - epoch complete!
2023-10-04 19:54:36,179 - INFO - evaluating now!
2023-10-04 19:54:36,179 - INFO - Epoch [610/4000] train_loss: 6.3471, lr: 0.000500, 0.04s
2023-10-04 19:54:36,218 - INFO - epoch complete!
2023-10-04 19:54:36,218 - INFO - evaluating now!
2023-10-04 19:54:36,218 - INFO - Epoch [611/4000] train_loss: 6.2220, lr: 0.000500, 0.04s
2023-10-04 19:54:36,257 - INFO - epoch complete!
2023-10-04 19:54:36,258 - INFO - evaluating now!
2023-10-04 19:54:36,258 - INFO - Epoch [612/4000] train_loss: 6.4030, lr: 0.000500, 0.04s
2023-10-04 19:54:36,297 - INFO - epoch complete!
2023-10-04 19:54:36,297 - INFO - evaluating now!
2023-10-04 19:54:36,297 - INFO - Epoch [613/4000] train_loss: 6.1250, lr: 0.000500, 0.04s
2023-10-04 19:54:36,336 - INFO - epoch complete!
2023-10-04 19:54:36,336 - INFO - evaluating now!
2023-10-04 19:54:36,336 - INFO - Epoch [614/4000] train_loss: 6.1275, lr: 0.000500, 0.04s
2023-10-04 19:54:36,375 - INFO - epoch complete!
2023-10-04 19:54:36,376 - INFO - evaluating now!
2023-10-04 19:54:36,376 - INFO - Epoch [615/4000] train_loss: 6.1053, lr: 0.000500, 0.04s
2023-10-04 19:54:36,415 - INFO - epoch complete!
2023-10-04 19:54:36,415 - INFO - evaluating now!
2023-10-04 19:54:36,415 - INFO - Epoch [616/4000] train_loss: 6.4259, lr: 0.000500, 0.04s
2023-10-04 19:54:36,454 - INFO - epoch complete!
2023-10-04 19:54:36,455 - INFO - evaluating now!
2023-10-04 19:54:36,455 - INFO - Epoch [617/4000] train_loss: 6.7391, lr: 0.000500, 0.04s
2023-10-04 19:54:36,494 - INFO - epoch complete!
2023-10-04 19:54:36,494 - INFO - evaluating now!
2023-10-04 19:54:36,494 - INFO - Epoch [618/4000] train_loss: 6.0968, lr: 0.000500, 0.04s
2023-10-04 19:54:36,533 - INFO - epoch complete!
2023-10-04 19:54:36,533 - INFO - evaluating now!
2023-10-04 19:54:36,534 - INFO - Epoch [619/4000] train_loss: 6.0581, lr: 0.000500, 0.04s
2023-10-04 19:54:36,572 - INFO - epoch complete!
2023-10-04 19:54:36,573 - INFO - evaluating now!
2023-10-04 19:54:36,573 - INFO - Epoch [620/4000] train_loss: 6.7460, lr: 0.000500, 0.04s
2023-10-04 19:54:36,612 - INFO - epoch complete!
2023-10-04 19:54:36,612 - INFO - evaluating now!
2023-10-04 19:54:36,612 - INFO - Epoch [621/4000] train_loss: 5.9507, lr: 0.000500, 0.04s
2023-10-04 19:54:36,651 - INFO - epoch complete!
2023-10-04 19:54:36,652 - INFO - evaluating now!
2023-10-04 19:54:36,652 - INFO - Epoch [622/4000] train_loss: 6.7278, lr: 0.000500, 0.04s
2023-10-04 19:54:36,690 - INFO - epoch complete!
2023-10-04 19:54:36,691 - INFO - evaluating now!
2023-10-04 19:54:36,691 - INFO - Epoch [623/4000] train_loss: 6.4739, lr: 0.000500, 0.04s
2023-10-04 19:54:36,730 - INFO - epoch complete!
2023-10-04 19:54:36,730 - INFO - evaluating now!
2023-10-04 19:54:36,730 - INFO - Epoch [624/4000] train_loss: 6.5115, lr: 0.000500, 0.04s
2023-10-04 19:54:36,769 - INFO - epoch complete!
2023-10-04 19:54:36,769 - INFO - evaluating now!
2023-10-04 19:54:36,769 - INFO - Epoch [625/4000] train_loss: 6.1449, lr: 0.000500, 0.04s
2023-10-04 19:54:36,808 - INFO - epoch complete!
2023-10-04 19:54:36,808 - INFO - evaluating now!
2023-10-04 19:54:36,809 - INFO - Epoch [626/4000] train_loss: 6.2466, lr: 0.000500, 0.04s
2023-10-04 19:54:36,848 - INFO - epoch complete!
2023-10-04 19:54:36,848 - INFO - evaluating now!
2023-10-04 19:54:36,848 - INFO - Epoch [627/4000] train_loss: 6.1077, lr: 0.000500, 0.04s
2023-10-04 19:54:36,887 - INFO - epoch complete!
2023-10-04 19:54:36,887 - INFO - evaluating now!
2023-10-04 19:54:36,887 - INFO - Epoch [628/4000] train_loss: 6.4592, lr: 0.000500, 0.04s
2023-10-04 19:54:36,926 - INFO - epoch complete!
2023-10-04 19:54:36,926 - INFO - evaluating now!
2023-10-04 19:54:36,926 - INFO - Epoch [629/4000] train_loss: 6.0364, lr: 0.000500, 0.04s
2023-10-04 19:54:36,966 - INFO - epoch complete!
2023-10-04 19:54:36,966 - INFO - evaluating now!
2023-10-04 19:54:36,966 - INFO - Epoch [630/4000] train_loss: 6.1424, lr: 0.000500, 0.04s
2023-10-04 19:54:37,005 - INFO - epoch complete!
2023-10-04 19:54:37,005 - INFO - evaluating now!
2023-10-04 19:54:37,005 - INFO - Epoch [631/4000] train_loss: 6.4017, lr: 0.000500, 0.04s
2023-10-04 19:54:37,045 - INFO - epoch complete!
2023-10-04 19:54:37,045 - INFO - evaluating now!
2023-10-04 19:54:37,045 - INFO - Epoch [632/4000] train_loss: 6.1134, lr: 0.000500, 0.04s
2023-10-04 19:54:37,084 - INFO - epoch complete!
2023-10-04 19:54:37,084 - INFO - evaluating now!
2023-10-04 19:54:37,084 - INFO - Epoch [633/4000] train_loss: 6.2539, lr: 0.000500, 0.04s
2023-10-04 19:54:37,123 - INFO - epoch complete!
2023-10-04 19:54:37,123 - INFO - evaluating now!
2023-10-04 19:54:37,123 - INFO - Epoch [634/4000] train_loss: 6.0738, lr: 0.000500, 0.04s
2023-10-04 19:54:37,162 - INFO - epoch complete!
2023-10-04 19:54:37,162 - INFO - evaluating now!
2023-10-04 19:54:37,163 - INFO - Epoch [635/4000] train_loss: 6.7184, lr: 0.000500, 0.04s
2023-10-04 19:54:37,202 - INFO - epoch complete!
2023-10-04 19:54:37,203 - INFO - evaluating now!
2023-10-04 19:54:37,203 - INFO - Epoch [636/4000] train_loss: 5.9089, lr: 0.000500, 0.04s
2023-10-04 19:54:37,242 - INFO - epoch complete!
2023-10-04 19:54:37,242 - INFO - evaluating now!
2023-10-04 19:54:37,242 - INFO - Epoch [637/4000] train_loss: 6.0488, lr: 0.000500, 0.04s
2023-10-04 19:54:37,281 - INFO - epoch complete!
2023-10-04 19:54:37,281 - INFO - evaluating now!
2023-10-04 19:54:37,282 - INFO - Epoch [638/4000] train_loss: 5.6224, lr: 0.000500, 0.04s
2023-10-04 19:54:37,512 - INFO - Saved model at 638
2023-10-04 19:54:37,512 - INFO - Val loss decrease from 5.8848 to 5.6224, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch638.tar
2023-10-04 19:54:37,552 - INFO - epoch complete!
2023-10-04 19:54:37,552 - INFO - evaluating now!
2023-10-04 19:54:37,552 - INFO - Epoch [639/4000] train_loss: 6.4781, lr: 0.000500, 0.04s
2023-10-04 19:54:37,591 - INFO - epoch complete!
2023-10-04 19:54:37,592 - INFO - evaluating now!
2023-10-04 19:54:37,592 - INFO - Epoch [640/4000] train_loss: 6.0139, lr: 0.000500, 0.04s
2023-10-04 19:54:37,631 - INFO - epoch complete!
2023-10-04 19:54:37,631 - INFO - evaluating now!
2023-10-04 19:54:37,631 - INFO - Epoch [641/4000] train_loss: 5.6735, lr: 0.000500, 0.04s
2023-10-04 19:54:37,670 - INFO - epoch complete!
2023-10-04 19:54:37,670 - INFO - evaluating now!
2023-10-04 19:54:37,670 - INFO - Epoch [642/4000] train_loss: 6.4812, lr: 0.000500, 0.04s
2023-10-04 19:54:37,708 - INFO - epoch complete!
2023-10-04 19:54:37,709 - INFO - evaluating now!
2023-10-04 19:54:37,709 - INFO - Epoch [643/4000] train_loss: 6.2576, lr: 0.000500, 0.04s
2023-10-04 19:54:37,748 - INFO - epoch complete!
2023-10-04 19:54:37,748 - INFO - evaluating now!
2023-10-04 19:54:37,748 - INFO - Epoch [644/4000] train_loss: 6.3032, lr: 0.000500, 0.04s
2023-10-04 19:54:37,786 - INFO - epoch complete!
2023-10-04 19:54:37,787 - INFO - evaluating now!
2023-10-04 19:54:37,787 - INFO - Epoch [645/4000] train_loss: 6.1263, lr: 0.000500, 0.04s
2023-10-04 19:54:37,825 - INFO - epoch complete!
2023-10-04 19:54:37,825 - INFO - evaluating now!
2023-10-04 19:54:37,825 - INFO - Epoch [646/4000] train_loss: 6.2305, lr: 0.000500, 0.04s
2023-10-04 19:54:37,864 - INFO - epoch complete!
2023-10-04 19:54:37,864 - INFO - evaluating now!
2023-10-04 19:54:37,864 - INFO - Epoch [647/4000] train_loss: 6.4047, lr: 0.000500, 0.04s
2023-10-04 19:54:37,902 - INFO - epoch complete!
2023-10-04 19:54:37,902 - INFO - evaluating now!
2023-10-04 19:54:37,903 - INFO - Epoch [648/4000] train_loss: 6.0080, lr: 0.000500, 0.04s
2023-10-04 19:54:37,941 - INFO - epoch complete!
2023-10-04 19:54:37,941 - INFO - evaluating now!
2023-10-04 19:54:37,941 - INFO - Epoch [649/4000] train_loss: 6.1658, lr: 0.000500, 0.04s
2023-10-04 19:54:37,980 - INFO - epoch complete!
2023-10-04 19:54:37,980 - INFO - evaluating now!
2023-10-04 19:54:37,980 - INFO - Epoch [650/4000] train_loss: 6.3860, lr: 0.000500, 0.04s
2023-10-04 19:54:38,018 - INFO - epoch complete!
2023-10-04 19:54:38,018 - INFO - evaluating now!
2023-10-04 19:54:38,018 - INFO - Epoch [651/4000] train_loss: 6.2721, lr: 0.000500, 0.04s
2023-10-04 19:54:38,057 - INFO - epoch complete!
2023-10-04 19:54:38,057 - INFO - evaluating now!
2023-10-04 19:54:38,057 - INFO - Epoch [652/4000] train_loss: 6.4024, lr: 0.000500, 0.04s
2023-10-04 19:54:38,096 - INFO - epoch complete!
2023-10-04 19:54:38,096 - INFO - evaluating now!
2023-10-04 19:54:38,096 - INFO - Epoch [653/4000] train_loss: 6.3378, lr: 0.000500, 0.04s
2023-10-04 19:54:38,135 - INFO - epoch complete!
2023-10-04 19:54:38,135 - INFO - evaluating now!
2023-10-04 19:54:38,135 - INFO - Epoch [654/4000] train_loss: 5.8594, lr: 0.000500, 0.04s
2023-10-04 19:54:38,173 - INFO - epoch complete!
2023-10-04 19:54:38,174 - INFO - evaluating now!
2023-10-04 19:54:38,174 - INFO - Epoch [655/4000] train_loss: 6.2049, lr: 0.000500, 0.04s
2023-10-04 19:54:38,212 - INFO - epoch complete!
2023-10-04 19:54:38,212 - INFO - evaluating now!
2023-10-04 19:54:38,213 - INFO - Epoch [656/4000] train_loss: 5.9030, lr: 0.000500, 0.04s
2023-10-04 19:54:38,251 - INFO - epoch complete!
2023-10-04 19:54:38,251 - INFO - evaluating now!
2023-10-04 19:54:38,251 - INFO - Epoch [657/4000] train_loss: 5.8087, lr: 0.000500, 0.04s
2023-10-04 19:54:38,290 - INFO - epoch complete!
2023-10-04 19:54:38,290 - INFO - evaluating now!
2023-10-04 19:54:38,290 - INFO - Epoch [658/4000] train_loss: 6.1794, lr: 0.000500, 0.04s
2023-10-04 19:54:38,328 - INFO - epoch complete!
2023-10-04 19:54:38,328 - INFO - evaluating now!
2023-10-04 19:54:38,329 - INFO - Epoch [659/4000] train_loss: 5.8393, lr: 0.000500, 0.04s
2023-10-04 19:54:38,367 - INFO - epoch complete!
2023-10-04 19:54:38,367 - INFO - evaluating now!
2023-10-04 19:54:38,367 - INFO - Epoch [660/4000] train_loss: 6.2397, lr: 0.000500, 0.04s
2023-10-04 19:54:38,406 - INFO - epoch complete!
2023-10-04 19:54:38,406 - INFO - evaluating now!
2023-10-04 19:54:38,406 - INFO - Epoch [661/4000] train_loss: 6.3177, lr: 0.000500, 0.04s
2023-10-04 19:54:38,445 - INFO - epoch complete!
2023-10-04 19:54:38,445 - INFO - evaluating now!
2023-10-04 19:54:38,445 - INFO - Epoch [662/4000] train_loss: 6.3829, lr: 0.000500, 0.04s
2023-10-04 19:54:38,484 - INFO - epoch complete!
2023-10-04 19:54:38,484 - INFO - evaluating now!
2023-10-04 19:54:38,484 - INFO - Epoch [663/4000] train_loss: 6.2823, lr: 0.000500, 0.04s
2023-10-04 19:54:38,523 - INFO - epoch complete!
2023-10-04 19:54:38,523 - INFO - evaluating now!
2023-10-04 19:54:38,523 - INFO - Epoch [664/4000] train_loss: 6.1561, lr: 0.000500, 0.04s
2023-10-04 19:54:38,561 - INFO - epoch complete!
2023-10-04 19:54:38,562 - INFO - evaluating now!
2023-10-04 19:54:38,562 - INFO - Epoch [665/4000] train_loss: 6.1116, lr: 0.000500, 0.04s
2023-10-04 19:54:38,600 - INFO - epoch complete!
2023-10-04 19:54:38,601 - INFO - evaluating now!
2023-10-04 19:54:38,601 - INFO - Epoch [666/4000] train_loss: 6.0656, lr: 0.000500, 0.04s
2023-10-04 19:54:38,639 - INFO - epoch complete!
2023-10-04 19:54:38,639 - INFO - evaluating now!
2023-10-04 19:54:38,639 - INFO - Epoch [667/4000] train_loss: 5.6884, lr: 0.000500, 0.04s
2023-10-04 19:54:38,678 - INFO - epoch complete!
2023-10-04 19:54:38,678 - INFO - evaluating now!
2023-10-04 19:54:38,678 - INFO - Epoch [668/4000] train_loss: 6.1909, lr: 0.000500, 0.04s
2023-10-04 19:54:38,717 - INFO - epoch complete!
2023-10-04 19:54:38,717 - INFO - evaluating now!
2023-10-04 19:54:38,717 - INFO - Epoch [669/4000] train_loss: 5.9120, lr: 0.000500, 0.04s
2023-10-04 19:54:38,756 - INFO - epoch complete!
2023-10-04 19:54:38,756 - INFO - evaluating now!
2023-10-04 19:54:38,756 - INFO - Epoch [670/4000] train_loss: 5.9755, lr: 0.000500, 0.04s
2023-10-04 19:54:38,794 - INFO - epoch complete!
2023-10-04 19:54:38,795 - INFO - evaluating now!
2023-10-04 19:54:38,795 - INFO - Epoch [671/4000] train_loss: 5.8794, lr: 0.000500, 0.04s
2023-10-04 19:54:38,833 - INFO - epoch complete!
2023-10-04 19:54:38,833 - INFO - evaluating now!
2023-10-04 19:54:38,833 - INFO - Epoch [672/4000] train_loss: 5.9564, lr: 0.000500, 0.04s
2023-10-04 19:54:38,872 - INFO - epoch complete!
2023-10-04 19:54:38,872 - INFO - evaluating now!
2023-10-04 19:54:38,872 - INFO - Epoch [673/4000] train_loss: 6.0605, lr: 0.000500, 0.04s
2023-10-04 19:54:38,911 - INFO - epoch complete!
2023-10-04 19:54:38,911 - INFO - evaluating now!
2023-10-04 19:54:38,911 - INFO - Epoch [674/4000] train_loss: 6.1995, lr: 0.000500, 0.04s
2023-10-04 19:54:38,949 - INFO - epoch complete!
2023-10-04 19:54:38,950 - INFO - evaluating now!
2023-10-04 19:54:38,950 - INFO - Epoch [675/4000] train_loss: 5.6334, lr: 0.000500, 0.04s
2023-10-04 19:54:38,988 - INFO - epoch complete!
2023-10-04 19:54:38,988 - INFO - evaluating now!
2023-10-04 19:54:38,988 - INFO - Epoch [676/4000] train_loss: 6.1722, lr: 0.000500, 0.04s
2023-10-04 19:54:39,027 - INFO - epoch complete!
2023-10-04 19:54:39,027 - INFO - evaluating now!
2023-10-04 19:54:39,027 - INFO - Epoch [677/4000] train_loss: 5.6797, lr: 0.000500, 0.04s
2023-10-04 19:54:39,066 - INFO - epoch complete!
2023-10-04 19:54:39,066 - INFO - evaluating now!
2023-10-04 19:54:39,066 - INFO - Epoch [678/4000] train_loss: 5.7466, lr: 0.000500, 0.04s
2023-10-04 19:54:39,104 - INFO - epoch complete!
2023-10-04 19:54:39,105 - INFO - evaluating now!
2023-10-04 19:54:39,105 - INFO - Epoch [679/4000] train_loss: 5.9336, lr: 0.000500, 0.04s
2023-10-04 19:54:39,143 - INFO - epoch complete!
2023-10-04 19:54:39,143 - INFO - evaluating now!
2023-10-04 19:54:39,144 - INFO - Epoch [680/4000] train_loss: 5.6238, lr: 0.000500, 0.04s
2023-10-04 19:54:39,182 - INFO - epoch complete!
2023-10-04 19:54:39,182 - INFO - evaluating now!
2023-10-04 19:54:39,182 - INFO - Epoch [681/4000] train_loss: 5.7770, lr: 0.000500, 0.04s
2023-10-04 19:54:39,221 - INFO - epoch complete!
2023-10-04 19:54:39,221 - INFO - evaluating now!
2023-10-04 19:54:39,221 - INFO - Epoch [682/4000] train_loss: 6.1576, lr: 0.000500, 0.04s
2023-10-04 19:54:39,260 - INFO - epoch complete!
2023-10-04 19:54:39,260 - INFO - evaluating now!
2023-10-04 19:54:39,260 - INFO - Epoch [683/4000] train_loss: 5.8835, lr: 0.000500, 0.04s
2023-10-04 19:54:39,298 - INFO - epoch complete!
2023-10-04 19:54:39,299 - INFO - evaluating now!
2023-10-04 19:54:39,299 - INFO - Epoch [684/4000] train_loss: 5.8671, lr: 0.000500, 0.04s
2023-10-04 19:54:39,337 - INFO - epoch complete!
2023-10-04 19:54:39,337 - INFO - evaluating now!
2023-10-04 19:54:39,338 - INFO - Epoch [685/4000] train_loss: 5.8588, lr: 0.000500, 0.04s
2023-10-04 19:54:39,376 - INFO - epoch complete!
2023-10-04 19:54:39,376 - INFO - evaluating now!
2023-10-04 19:54:39,376 - INFO - Epoch [686/4000] train_loss: 5.9216, lr: 0.000500, 0.04s
2023-10-04 19:54:39,414 - INFO - epoch complete!
2023-10-04 19:54:39,415 - INFO - evaluating now!
2023-10-04 19:54:39,415 - INFO - Epoch [687/4000] train_loss: 5.6983, lr: 0.000500, 0.04s
2023-10-04 19:54:39,453 - INFO - epoch complete!
2023-10-04 19:54:39,453 - INFO - evaluating now!
2023-10-04 19:54:39,454 - INFO - Epoch [688/4000] train_loss: 5.5685, lr: 0.000500, 0.04s
2023-10-04 19:54:39,684 - INFO - Saved model at 688
2023-10-04 19:54:39,684 - INFO - Val loss decrease from 5.6224 to 5.5685, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch688.tar
2023-10-04 19:54:39,725 - INFO - epoch complete!
2023-10-04 19:54:39,726 - INFO - evaluating now!
2023-10-04 19:54:39,726 - INFO - Epoch [689/4000] train_loss: 6.0120, lr: 0.000500, 0.04s
2023-10-04 19:54:39,765 - INFO - epoch complete!
2023-10-04 19:54:39,766 - INFO - evaluating now!
2023-10-04 19:54:39,766 - INFO - Epoch [690/4000] train_loss: 5.8190, lr: 0.000500, 0.04s
2023-10-04 19:54:39,806 - INFO - epoch complete!
2023-10-04 19:54:39,806 - INFO - evaluating now!
2023-10-04 19:54:39,806 - INFO - Epoch [691/4000] train_loss: 6.2545, lr: 0.000500, 0.04s
2023-10-04 19:54:39,845 - INFO - epoch complete!
2023-10-04 19:54:39,846 - INFO - evaluating now!
2023-10-04 19:54:39,846 - INFO - Epoch [692/4000] train_loss: 6.1916, lr: 0.000500, 0.04s
2023-10-04 19:54:39,885 - INFO - epoch complete!
2023-10-04 19:54:39,886 - INFO - evaluating now!
2023-10-04 19:54:39,886 - INFO - Epoch [693/4000] train_loss: 6.0579, lr: 0.000500, 0.04s
2023-10-04 19:54:39,924 - INFO - epoch complete!
2023-10-04 19:54:39,925 - INFO - evaluating now!
2023-10-04 19:54:39,925 - INFO - Epoch [694/4000] train_loss: 5.8764, lr: 0.000500, 0.04s
2023-10-04 19:54:39,963 - INFO - epoch complete!
2023-10-04 19:54:39,963 - INFO - evaluating now!
2023-10-04 19:54:39,964 - INFO - Epoch [695/4000] train_loss: 6.2060, lr: 0.000500, 0.04s
2023-10-04 19:54:40,002 - INFO - epoch complete!
2023-10-04 19:54:40,002 - INFO - evaluating now!
2023-10-04 19:54:40,002 - INFO - Epoch [696/4000] train_loss: 5.8169, lr: 0.000500, 0.04s
2023-10-04 19:54:40,041 - INFO - epoch complete!
2023-10-04 19:54:40,041 - INFO - evaluating now!
2023-10-04 19:54:40,041 - INFO - Epoch [697/4000] train_loss: 5.8112, lr: 0.000500, 0.04s
2023-10-04 19:54:40,079 - INFO - epoch complete!
2023-10-04 19:54:40,080 - INFO - evaluating now!
2023-10-04 19:54:40,080 - INFO - Epoch [698/4000] train_loss: 6.0033, lr: 0.000500, 0.04s
2023-10-04 19:54:40,118 - INFO - epoch complete!
2023-10-04 19:54:40,118 - INFO - evaluating now!
2023-10-04 19:54:40,119 - INFO - Epoch [699/4000] train_loss: 5.9254, lr: 0.000500, 0.04s
2023-10-04 19:54:40,157 - INFO - epoch complete!
2023-10-04 19:54:40,157 - INFO - evaluating now!
2023-10-04 19:54:40,157 - INFO - Epoch [700/4000] train_loss: 6.0912, lr: 0.000500, 0.04s
2023-10-04 19:54:40,196 - INFO - epoch complete!
2023-10-04 19:54:40,196 - INFO - evaluating now!
2023-10-04 19:54:40,196 - INFO - Epoch [701/4000] train_loss: 5.8707, lr: 0.000500, 0.04s
2023-10-04 19:54:40,234 - INFO - epoch complete!
2023-10-04 19:54:40,235 - INFO - evaluating now!
2023-10-04 19:54:40,235 - INFO - Epoch [702/4000] train_loss: 5.9343, lr: 0.000500, 0.04s
2023-10-04 19:54:40,273 - INFO - epoch complete!
2023-10-04 19:54:40,273 - INFO - evaluating now!
2023-10-04 19:54:40,273 - INFO - Epoch [703/4000] train_loss: 5.5530, lr: 0.000500, 0.04s
2023-10-04 19:54:40,504 - INFO - Saved model at 703
2023-10-04 19:54:40,504 - INFO - Val loss decrease from 5.5685 to 5.5530, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch703.tar
2023-10-04 19:54:40,543 - INFO - epoch complete!
2023-10-04 19:54:40,543 - INFO - evaluating now!
2023-10-04 19:54:40,544 - INFO - Epoch [704/4000] train_loss: 5.5689, lr: 0.000500, 0.04s
2023-10-04 19:54:40,583 - INFO - epoch complete!
2023-10-04 19:54:40,583 - INFO - evaluating now!
2023-10-04 19:54:40,583 - INFO - Epoch [705/4000] train_loss: 5.9190, lr: 0.000500, 0.04s
2023-10-04 19:54:40,621 - INFO - epoch complete!
2023-10-04 19:54:40,622 - INFO - evaluating now!
2023-10-04 19:54:40,622 - INFO - Epoch [706/4000] train_loss: 5.8941, lr: 0.000500, 0.04s
2023-10-04 19:54:40,660 - INFO - epoch complete!
2023-10-04 19:54:40,661 - INFO - evaluating now!
2023-10-04 19:54:40,661 - INFO - Epoch [707/4000] train_loss: 6.1640, lr: 0.000500, 0.04s
2023-10-04 19:54:40,699 - INFO - epoch complete!
2023-10-04 19:54:40,699 - INFO - evaluating now!
2023-10-04 19:54:40,699 - INFO - Epoch [708/4000] train_loss: 6.0898, lr: 0.000500, 0.04s
2023-10-04 19:54:40,738 - INFO - epoch complete!
2023-10-04 19:54:40,738 - INFO - evaluating now!
2023-10-04 19:54:40,738 - INFO - Epoch [709/4000] train_loss: 5.6249, lr: 0.000500, 0.04s
2023-10-04 19:54:40,776 - INFO - epoch complete!
2023-10-04 19:54:40,777 - INFO - evaluating now!
2023-10-04 19:54:40,777 - INFO - Epoch [710/4000] train_loss: 6.1631, lr: 0.000500, 0.04s
2023-10-04 19:54:40,815 - INFO - epoch complete!
2023-10-04 19:54:40,815 - INFO - evaluating now!
2023-10-04 19:54:40,815 - INFO - Epoch [711/4000] train_loss: 5.8351, lr: 0.000500, 0.04s
2023-10-04 19:54:40,854 - INFO - epoch complete!
2023-10-04 19:54:40,854 - INFO - evaluating now!
2023-10-04 19:54:40,854 - INFO - Epoch [712/4000] train_loss: 6.0661, lr: 0.000500, 0.04s
2023-10-04 19:54:40,892 - INFO - epoch complete!
2023-10-04 19:54:40,893 - INFO - evaluating now!
2023-10-04 19:54:40,893 - INFO - Epoch [713/4000] train_loss: 6.0262, lr: 0.000500, 0.04s
2023-10-04 19:54:40,932 - INFO - epoch complete!
2023-10-04 19:54:40,933 - INFO - evaluating now!
2023-10-04 19:54:40,933 - INFO - Epoch [714/4000] train_loss: 5.9706, lr: 0.000500, 0.04s
2023-10-04 19:54:40,973 - INFO - epoch complete!
2023-10-04 19:54:40,973 - INFO - evaluating now!
2023-10-04 19:54:40,973 - INFO - Epoch [715/4000] train_loss: 5.8986, lr: 0.000500, 0.04s
2023-10-04 19:54:41,011 - INFO - epoch complete!
2023-10-04 19:54:41,012 - INFO - evaluating now!
2023-10-04 19:54:41,012 - INFO - Epoch [716/4000] train_loss: 5.7164, lr: 0.000500, 0.04s
2023-10-04 19:54:41,050 - INFO - epoch complete!
2023-10-04 19:54:41,050 - INFO - evaluating now!
2023-10-04 19:54:41,050 - INFO - Epoch [717/4000] train_loss: 6.0264, lr: 0.000500, 0.04s
2023-10-04 19:54:41,089 - INFO - epoch complete!
2023-10-04 19:54:41,090 - INFO - evaluating now!
2023-10-04 19:54:41,090 - INFO - Epoch [718/4000] train_loss: 5.3726, lr: 0.000500, 0.04s
2023-10-04 19:54:41,320 - INFO - Saved model at 718
2023-10-04 19:54:41,320 - INFO - Val loss decrease from 5.5530 to 5.3726, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch718.tar
2023-10-04 19:54:41,360 - INFO - epoch complete!
2023-10-04 19:54:41,361 - INFO - evaluating now!
2023-10-04 19:54:41,361 - INFO - Epoch [719/4000] train_loss: 5.4680, lr: 0.000500, 0.04s
2023-10-04 19:54:41,401 - INFO - epoch complete!
2023-10-04 19:54:41,401 - INFO - evaluating now!
2023-10-04 19:54:41,401 - INFO - Epoch [720/4000] train_loss: 5.9115, lr: 0.000500, 0.04s
2023-10-04 19:54:41,440 - INFO - epoch complete!
2023-10-04 19:54:41,440 - INFO - evaluating now!
2023-10-04 19:54:41,441 - INFO - Epoch [721/4000] train_loss: 5.9872, lr: 0.000500, 0.04s
2023-10-04 19:54:41,480 - INFO - epoch complete!
2023-10-04 19:54:41,480 - INFO - evaluating now!
2023-10-04 19:54:41,480 - INFO - Epoch [722/4000] train_loss: 5.5616, lr: 0.000500, 0.04s
2023-10-04 19:54:41,519 - INFO - epoch complete!
2023-10-04 19:54:41,519 - INFO - evaluating now!
2023-10-04 19:54:41,519 - INFO - Epoch [723/4000] train_loss: 5.5308, lr: 0.000500, 0.04s
2023-10-04 19:54:41,558 - INFO - epoch complete!
2023-10-04 19:54:41,558 - INFO - evaluating now!
2023-10-04 19:54:41,558 - INFO - Epoch [724/4000] train_loss: 5.8246, lr: 0.000500, 0.04s
2023-10-04 19:54:41,597 - INFO - epoch complete!
2023-10-04 19:54:41,597 - INFO - evaluating now!
2023-10-04 19:54:41,597 - INFO - Epoch [725/4000] train_loss: 6.0273, lr: 0.000500, 0.04s
2023-10-04 19:54:41,636 - INFO - epoch complete!
2023-10-04 19:54:41,636 - INFO - evaluating now!
2023-10-04 19:54:41,636 - INFO - Epoch [726/4000] train_loss: 5.5302, lr: 0.000500, 0.04s
2023-10-04 19:54:41,677 - INFO - epoch complete!
2023-10-04 19:54:41,678 - INFO - evaluating now!
2023-10-04 19:54:41,678 - INFO - Epoch [727/4000] train_loss: 5.5936, lr: 0.000500, 0.04s
2023-10-04 19:54:41,717 - INFO - epoch complete!
2023-10-04 19:54:41,717 - INFO - evaluating now!
2023-10-04 19:54:41,717 - INFO - Epoch [728/4000] train_loss: 5.8828, lr: 0.000500, 0.04s
2023-10-04 19:54:41,756 - INFO - epoch complete!
2023-10-04 19:54:41,756 - INFO - evaluating now!
2023-10-04 19:54:41,756 - INFO - Epoch [729/4000] train_loss: 5.7147, lr: 0.000500, 0.04s
2023-10-04 19:54:41,795 - INFO - epoch complete!
2023-10-04 19:54:41,795 - INFO - evaluating now!
2023-10-04 19:54:41,796 - INFO - Epoch [730/4000] train_loss: 5.6525, lr: 0.000500, 0.04s
2023-10-04 19:54:41,834 - INFO - epoch complete!
2023-10-04 19:54:41,834 - INFO - evaluating now!
2023-10-04 19:54:41,834 - INFO - Epoch [731/4000] train_loss: 5.6065, lr: 0.000500, 0.04s
2023-10-04 19:54:41,873 - INFO - epoch complete!
2023-10-04 19:54:41,873 - INFO - evaluating now!
2023-10-04 19:54:41,874 - INFO - Epoch [732/4000] train_loss: 5.6491, lr: 0.000500, 0.04s
2023-10-04 19:54:41,912 - INFO - epoch complete!
2023-10-04 19:54:41,913 - INFO - evaluating now!
2023-10-04 19:54:41,913 - INFO - Epoch [733/4000] train_loss: 5.8673, lr: 0.000500, 0.04s
2023-10-04 19:54:41,951 - INFO - epoch complete!
2023-10-04 19:54:41,952 - INFO - evaluating now!
2023-10-04 19:54:41,952 - INFO - Epoch [734/4000] train_loss: 5.8361, lr: 0.000500, 0.04s
2023-10-04 19:54:41,990 - INFO - epoch complete!
2023-10-04 19:54:41,991 - INFO - evaluating now!
2023-10-04 19:54:41,991 - INFO - Epoch [735/4000] train_loss: 5.7305, lr: 0.000500, 0.04s
2023-10-04 19:54:42,029 - INFO - epoch complete!
2023-10-04 19:54:42,030 - INFO - evaluating now!
2023-10-04 19:54:42,030 - INFO - Epoch [736/4000] train_loss: 5.6882, lr: 0.000500, 0.04s
2023-10-04 19:54:42,069 - INFO - epoch complete!
2023-10-04 19:54:42,069 - INFO - evaluating now!
2023-10-04 19:54:42,069 - INFO - Epoch [737/4000] train_loss: 5.8876, lr: 0.000500, 0.04s
2023-10-04 19:54:42,108 - INFO - epoch complete!
2023-10-04 19:54:42,108 - INFO - evaluating now!
2023-10-04 19:54:42,108 - INFO - Epoch [738/4000] train_loss: 5.6868, lr: 0.000500, 0.04s
2023-10-04 19:54:42,147 - INFO - epoch complete!
2023-10-04 19:54:42,148 - INFO - evaluating now!
2023-10-04 19:54:42,148 - INFO - Epoch [739/4000] train_loss: 5.9525, lr: 0.000500, 0.04s
2023-10-04 19:54:42,186 - INFO - epoch complete!
2023-10-04 19:54:42,187 - INFO - evaluating now!
2023-10-04 19:54:42,187 - INFO - Epoch [740/4000] train_loss: 5.7550, lr: 0.000500, 0.04s
2023-10-04 19:54:42,225 - INFO - epoch complete!
2023-10-04 19:54:42,226 - INFO - evaluating now!
2023-10-04 19:54:42,226 - INFO - Epoch [741/4000] train_loss: 6.0337, lr: 0.000500, 0.04s
2023-10-04 19:54:42,264 - INFO - epoch complete!
2023-10-04 19:54:42,265 - INFO - evaluating now!
2023-10-04 19:54:42,265 - INFO - Epoch [742/4000] train_loss: 5.8828, lr: 0.000500, 0.04s
2023-10-04 19:54:42,304 - INFO - epoch complete!
2023-10-04 19:54:42,304 - INFO - evaluating now!
2023-10-04 19:54:42,304 - INFO - Epoch [743/4000] train_loss: 5.3599, lr: 0.000500, 0.04s
2023-10-04 19:54:42,534 - INFO - Saved model at 743
2023-10-04 19:54:42,535 - INFO - Val loss decrease from 5.3726 to 5.3599, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch743.tar
2023-10-04 19:54:42,574 - INFO - epoch complete!
2023-10-04 19:54:42,575 - INFO - evaluating now!
2023-10-04 19:54:42,575 - INFO - Epoch [744/4000] train_loss: 5.9528, lr: 0.000500, 0.04s
2023-10-04 19:54:42,614 - INFO - epoch complete!
2023-10-04 19:54:42,614 - INFO - evaluating now!
2023-10-04 19:54:42,614 - INFO - Epoch [745/4000] train_loss: 6.1608, lr: 0.000500, 0.04s
2023-10-04 19:54:42,654 - INFO - epoch complete!
2023-10-04 19:54:42,654 - INFO - evaluating now!
2023-10-04 19:54:42,654 - INFO - Epoch [746/4000] train_loss: 5.4400, lr: 0.000500, 0.04s
2023-10-04 19:54:42,693 - INFO - epoch complete!
2023-10-04 19:54:42,694 - INFO - evaluating now!
2023-10-04 19:54:42,694 - INFO - Epoch [747/4000] train_loss: 5.7447, lr: 0.000500, 0.04s
2023-10-04 19:54:42,732 - INFO - epoch complete!
2023-10-04 19:54:42,732 - INFO - evaluating now!
2023-10-04 19:54:42,733 - INFO - Epoch [748/4000] train_loss: 5.3724, lr: 0.000500, 0.04s
2023-10-04 19:54:42,771 - INFO - epoch complete!
2023-10-04 19:54:42,772 - INFO - evaluating now!
2023-10-04 19:54:42,772 - INFO - Epoch [749/4000] train_loss: 5.8670, lr: 0.000500, 0.04s
2023-10-04 19:54:42,810 - INFO - epoch complete!
2023-10-04 19:54:42,810 - INFO - evaluating now!
2023-10-04 19:54:42,810 - INFO - Epoch [750/4000] train_loss: 5.7940, lr: 0.000500, 0.04s
2023-10-04 19:54:42,849 - INFO - epoch complete!
2023-10-04 19:54:42,849 - INFO - evaluating now!
2023-10-04 19:54:42,849 - INFO - Epoch [751/4000] train_loss: 5.7448, lr: 0.000500, 0.04s
2023-10-04 19:54:42,888 - INFO - epoch complete!
2023-10-04 19:54:42,889 - INFO - evaluating now!
2023-10-04 19:54:42,889 - INFO - Epoch [752/4000] train_loss: 5.5426, lr: 0.000500, 0.04s
2023-10-04 19:54:42,927 - INFO - epoch complete!
2023-10-04 19:54:42,928 - INFO - evaluating now!
2023-10-04 19:54:42,928 - INFO - Epoch [753/4000] train_loss: 5.5046, lr: 0.000500, 0.04s
2023-10-04 19:54:42,966 - INFO - epoch complete!
2023-10-04 19:54:42,966 - INFO - evaluating now!
2023-10-04 19:54:42,967 - INFO - Epoch [754/4000] train_loss: 5.8761, lr: 0.000500, 0.04s
2023-10-04 19:54:43,005 - INFO - epoch complete!
2023-10-04 19:54:43,005 - INFO - evaluating now!
2023-10-04 19:54:43,006 - INFO - Epoch [755/4000] train_loss: 5.5515, lr: 0.000500, 0.04s
2023-10-04 19:54:43,045 - INFO - epoch complete!
2023-10-04 19:54:43,045 - INFO - evaluating now!
2023-10-04 19:54:43,045 - INFO - Epoch [756/4000] train_loss: 5.6994, lr: 0.000500, 0.04s
2023-10-04 19:54:43,084 - INFO - epoch complete!
2023-10-04 19:54:43,084 - INFO - evaluating now!
2023-10-04 19:54:43,084 - INFO - Epoch [757/4000] train_loss: 5.8699, lr: 0.000500, 0.04s
2023-10-04 19:54:43,123 - INFO - epoch complete!
2023-10-04 19:54:43,123 - INFO - evaluating now!
2023-10-04 19:54:43,123 - INFO - Epoch [758/4000] train_loss: 5.6946, lr: 0.000500, 0.04s
2023-10-04 19:54:43,162 - INFO - epoch complete!
2023-10-04 19:54:43,162 - INFO - evaluating now!
2023-10-04 19:54:43,162 - INFO - Epoch [759/4000] train_loss: 5.6113, lr: 0.000500, 0.04s
2023-10-04 19:54:43,201 - INFO - epoch complete!
2023-10-04 19:54:43,201 - INFO - evaluating now!
2023-10-04 19:54:43,201 - INFO - Epoch [760/4000] train_loss: 5.5782, lr: 0.000500, 0.04s
2023-10-04 19:54:43,240 - INFO - epoch complete!
2023-10-04 19:54:43,240 - INFO - evaluating now!
2023-10-04 19:54:43,241 - INFO - Epoch [761/4000] train_loss: 6.0723, lr: 0.000500, 0.04s
2023-10-04 19:54:43,280 - INFO - epoch complete!
2023-10-04 19:54:43,280 - INFO - evaluating now!
2023-10-04 19:54:43,280 - INFO - Epoch [762/4000] train_loss: 5.4448, lr: 0.000500, 0.04s
2023-10-04 19:54:43,319 - INFO - epoch complete!
2023-10-04 19:54:43,319 - INFO - evaluating now!
2023-10-04 19:54:43,319 - INFO - Epoch [763/4000] train_loss: 5.5827, lr: 0.000500, 0.04s
2023-10-04 19:54:43,358 - INFO - epoch complete!
2023-10-04 19:54:43,358 - INFO - evaluating now!
2023-10-04 19:54:43,358 - INFO - Epoch [764/4000] train_loss: 5.5645, lr: 0.000500, 0.04s
2023-10-04 19:54:43,397 - INFO - epoch complete!
2023-10-04 19:54:43,397 - INFO - evaluating now!
2023-10-04 19:54:43,397 - INFO - Epoch [765/4000] train_loss: 5.5346, lr: 0.000500, 0.04s
2023-10-04 19:54:43,436 - INFO - epoch complete!
2023-10-04 19:54:43,436 - INFO - evaluating now!
2023-10-04 19:54:43,437 - INFO - Epoch [766/4000] train_loss: 5.6104, lr: 0.000500, 0.04s
2023-10-04 19:54:43,476 - INFO - epoch complete!
2023-10-04 19:54:43,476 - INFO - evaluating now!
2023-10-04 19:54:43,476 - INFO - Epoch [767/4000] train_loss: 5.4217, lr: 0.000500, 0.04s
2023-10-04 19:54:43,515 - INFO - epoch complete!
2023-10-04 19:54:43,515 - INFO - evaluating now!
2023-10-04 19:54:43,515 - INFO - Epoch [768/4000] train_loss: 5.2041, lr: 0.000500, 0.04s
2023-10-04 19:54:43,745 - INFO - Saved model at 768
2023-10-04 19:54:43,746 - INFO - Val loss decrease from 5.3599 to 5.2041, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch768.tar
2023-10-04 19:54:43,786 - INFO - epoch complete!
2023-10-04 19:54:43,786 - INFO - evaluating now!
2023-10-04 19:54:43,786 - INFO - Epoch [769/4000] train_loss: 5.4762, lr: 0.000500, 0.04s
2023-10-04 19:54:43,826 - INFO - epoch complete!
2023-10-04 19:54:43,826 - INFO - evaluating now!
2023-10-04 19:54:43,826 - INFO - Epoch [770/4000] train_loss: 5.2943, lr: 0.000500, 0.04s
2023-10-04 19:54:43,866 - INFO - epoch complete!
2023-10-04 19:54:43,866 - INFO - evaluating now!
2023-10-04 19:54:43,866 - INFO - Epoch [771/4000] train_loss: 5.3001, lr: 0.000500, 0.04s
2023-10-04 19:54:43,905 - INFO - epoch complete!
2023-10-04 19:54:43,905 - INFO - evaluating now!
2023-10-04 19:54:43,906 - INFO - Epoch [772/4000] train_loss: 5.6589, lr: 0.000500, 0.04s
2023-10-04 19:54:43,944 - INFO - epoch complete!
2023-10-04 19:54:43,945 - INFO - evaluating now!
2023-10-04 19:54:43,945 - INFO - Epoch [773/4000] train_loss: 5.5600, lr: 0.000500, 0.04s
2023-10-04 19:54:43,984 - INFO - epoch complete!
2023-10-04 19:54:43,984 - INFO - evaluating now!
2023-10-04 19:54:43,984 - INFO - Epoch [774/4000] train_loss: 5.4155, lr: 0.000500, 0.04s
2023-10-04 19:54:44,023 - INFO - epoch complete!
2023-10-04 19:54:44,023 - INFO - evaluating now!
2023-10-04 19:54:44,023 - INFO - Epoch [775/4000] train_loss: 5.3364, lr: 0.000500, 0.04s
2023-10-04 19:54:44,062 - INFO - epoch complete!
2023-10-04 19:54:44,062 - INFO - evaluating now!
2023-10-04 19:54:44,062 - INFO - Epoch [776/4000] train_loss: 5.3539, lr: 0.000500, 0.04s
2023-10-04 19:54:44,101 - INFO - epoch complete!
2023-10-04 19:54:44,101 - INFO - evaluating now!
2023-10-04 19:54:44,101 - INFO - Epoch [777/4000] train_loss: 5.1781, lr: 0.000500, 0.04s
2023-10-04 19:54:44,332 - INFO - Saved model at 777
2023-10-04 19:54:44,332 - INFO - Val loss decrease from 5.2041 to 5.1781, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch777.tar
2023-10-04 19:54:44,372 - INFO - epoch complete!
2023-10-04 19:54:44,372 - INFO - evaluating now!
2023-10-04 19:54:44,372 - INFO - Epoch [778/4000] train_loss: 5.4738, lr: 0.000500, 0.04s
2023-10-04 19:54:44,412 - INFO - epoch complete!
2023-10-04 19:54:44,412 - INFO - evaluating now!
2023-10-04 19:54:44,412 - INFO - Epoch [779/4000] train_loss: 5.5427, lr: 0.000500, 0.04s
2023-10-04 19:54:44,452 - INFO - epoch complete!
2023-10-04 19:54:44,452 - INFO - evaluating now!
2023-10-04 19:54:44,452 - INFO - Epoch [780/4000] train_loss: 5.2138, lr: 0.000500, 0.04s
2023-10-04 19:54:44,491 - INFO - epoch complete!
2023-10-04 19:54:44,491 - INFO - evaluating now!
2023-10-04 19:54:44,491 - INFO - Epoch [781/4000] train_loss: 5.9198, lr: 0.000500, 0.04s
2023-10-04 19:54:44,530 - INFO - epoch complete!
2023-10-04 19:54:44,530 - INFO - evaluating now!
2023-10-04 19:54:44,530 - INFO - Epoch [782/4000] train_loss: 5.2464, lr: 0.000500, 0.04s
2023-10-04 19:54:44,569 - INFO - epoch complete!
2023-10-04 19:54:44,569 - INFO - evaluating now!
2023-10-04 19:54:44,570 - INFO - Epoch [783/4000] train_loss: 5.3770, lr: 0.000500, 0.04s
2023-10-04 19:54:44,608 - INFO - epoch complete!
2023-10-04 19:54:44,608 - INFO - evaluating now!
2023-10-04 19:54:44,608 - INFO - Epoch [784/4000] train_loss: 5.3488, lr: 0.000500, 0.04s
2023-10-04 19:54:44,647 - INFO - epoch complete!
2023-10-04 19:54:44,647 - INFO - evaluating now!
2023-10-04 19:54:44,648 - INFO - Epoch [785/4000] train_loss: 5.3829, lr: 0.000500, 0.04s
2023-10-04 19:54:44,686 - INFO - epoch complete!
2023-10-04 19:54:44,687 - INFO - evaluating now!
2023-10-04 19:54:44,687 - INFO - Epoch [786/4000] train_loss: 5.7646, lr: 0.000500, 0.04s
2023-10-04 19:54:44,725 - INFO - epoch complete!
2023-10-04 19:54:44,726 - INFO - evaluating now!
2023-10-04 19:54:44,726 - INFO - Epoch [787/4000] train_loss: 5.6035, lr: 0.000500, 0.04s
2023-10-04 19:54:44,765 - INFO - epoch complete!
2023-10-04 19:54:44,765 - INFO - evaluating now!
2023-10-04 19:54:44,765 - INFO - Epoch [788/4000] train_loss: 5.4580, lr: 0.000500, 0.04s
2023-10-04 19:54:44,805 - INFO - epoch complete!
2023-10-04 19:54:44,806 - INFO - evaluating now!
2023-10-04 19:54:44,806 - INFO - Epoch [789/4000] train_loss: 5.3813, lr: 0.000500, 0.04s
2023-10-04 19:54:44,846 - INFO - epoch complete!
2023-10-04 19:54:44,846 - INFO - evaluating now!
2023-10-04 19:54:44,846 - INFO - Epoch [790/4000] train_loss: 5.4504, lr: 0.000500, 0.04s
2023-10-04 19:54:44,888 - INFO - epoch complete!
2023-10-04 19:54:44,888 - INFO - evaluating now!
2023-10-04 19:54:44,889 - INFO - Epoch [791/4000] train_loss: 5.9071, lr: 0.000500, 0.04s
2023-10-04 19:54:44,928 - INFO - epoch complete!
2023-10-04 19:54:44,928 - INFO - evaluating now!
2023-10-04 19:54:44,928 - INFO - Epoch [792/4000] train_loss: 5.4697, lr: 0.000500, 0.04s
2023-10-04 19:54:44,967 - INFO - epoch complete!
2023-10-04 19:54:44,967 - INFO - evaluating now!
2023-10-04 19:54:44,967 - INFO - Epoch [793/4000] train_loss: 5.4832, lr: 0.000500, 0.04s
2023-10-04 19:54:45,006 - INFO - epoch complete!
2023-10-04 19:54:45,006 - INFO - evaluating now!
2023-10-04 19:54:45,007 - INFO - Epoch [794/4000] train_loss: 5.8185, lr: 0.000500, 0.04s
2023-10-04 19:54:45,045 - INFO - epoch complete!
2023-10-04 19:54:45,046 - INFO - evaluating now!
2023-10-04 19:54:45,046 - INFO - Epoch [795/4000] train_loss: 5.4014, lr: 0.000500, 0.04s
2023-10-04 19:54:45,085 - INFO - epoch complete!
2023-10-04 19:54:45,085 - INFO - evaluating now!
2023-10-04 19:54:45,085 - INFO - Epoch [796/4000] train_loss: 5.5974, lr: 0.000500, 0.04s
2023-10-04 19:54:45,124 - INFO - epoch complete!
2023-10-04 19:54:45,124 - INFO - evaluating now!
2023-10-04 19:54:45,124 - INFO - Epoch [797/4000] train_loss: 5.8720, lr: 0.000500, 0.04s
2023-10-04 19:54:45,163 - INFO - epoch complete!
2023-10-04 19:54:45,163 - INFO - evaluating now!
2023-10-04 19:54:45,163 - INFO - Epoch [798/4000] train_loss: 5.4784, lr: 0.000500, 0.04s
2023-10-04 19:54:45,202 - INFO - epoch complete!
2023-10-04 19:54:45,202 - INFO - evaluating now!
2023-10-04 19:54:45,202 - INFO - Epoch [799/4000] train_loss: 5.5049, lr: 0.000500, 0.04s
2023-10-04 19:54:45,241 - INFO - epoch complete!
2023-10-04 19:54:45,241 - INFO - evaluating now!
2023-10-04 19:54:45,241 - INFO - Epoch [800/4000] train_loss: 5.6385, lr: 0.000500, 0.04s
2023-10-04 19:54:45,280 - INFO - epoch complete!
2023-10-04 19:54:45,280 - INFO - evaluating now!
2023-10-04 19:54:45,281 - INFO - Epoch [801/4000] train_loss: 5.7192, lr: 0.000500, 0.04s
2023-10-04 19:54:45,319 - INFO - epoch complete!
2023-10-04 19:54:45,320 - INFO - evaluating now!
2023-10-04 19:54:45,320 - INFO - Epoch [802/4000] train_loss: 5.5424, lr: 0.000500, 0.04s
2023-10-04 19:54:45,358 - INFO - epoch complete!
2023-10-04 19:54:45,358 - INFO - evaluating now!
2023-10-04 19:54:45,359 - INFO - Epoch [803/4000] train_loss: 5.7575, lr: 0.000500, 0.04s
2023-10-04 19:54:45,397 - INFO - epoch complete!
2023-10-04 19:54:45,398 - INFO - evaluating now!
2023-10-04 19:54:45,398 - INFO - Epoch [804/4000] train_loss: 5.3750, lr: 0.000500, 0.04s
2023-10-04 19:54:45,436 - INFO - epoch complete!
2023-10-04 19:54:45,437 - INFO - evaluating now!
2023-10-04 19:54:45,437 - INFO - Epoch [805/4000] train_loss: 5.6696, lr: 0.000500, 0.04s
2023-10-04 19:54:45,476 - INFO - epoch complete!
2023-10-04 19:54:45,476 - INFO - evaluating now!
2023-10-04 19:54:45,476 - INFO - Epoch [806/4000] train_loss: 5.6653, lr: 0.000500, 0.04s
2023-10-04 19:54:45,515 - INFO - epoch complete!
2023-10-04 19:54:45,515 - INFO - evaluating now!
2023-10-04 19:54:45,515 - INFO - Epoch [807/4000] train_loss: 5.4724, lr: 0.000500, 0.04s
2023-10-04 19:54:45,554 - INFO - epoch complete!
2023-10-04 19:54:45,554 - INFO - evaluating now!
2023-10-04 19:54:45,554 - INFO - Epoch [808/4000] train_loss: 5.7260, lr: 0.000500, 0.04s
2023-10-04 19:54:45,593 - INFO - epoch complete!
2023-10-04 19:54:45,594 - INFO - evaluating now!
2023-10-04 19:54:45,594 - INFO - Epoch [809/4000] train_loss: 5.3215, lr: 0.000500, 0.04s
2023-10-04 19:54:45,632 - INFO - epoch complete!
2023-10-04 19:54:45,633 - INFO - evaluating now!
2023-10-04 19:54:45,633 - INFO - Epoch [810/4000] train_loss: 5.2681, lr: 0.000500, 0.04s
2023-10-04 19:54:45,671 - INFO - epoch complete!
2023-10-04 19:54:45,672 - INFO - evaluating now!
2023-10-04 19:54:45,672 - INFO - Epoch [811/4000] train_loss: 5.3983, lr: 0.000500, 0.04s
2023-10-04 19:54:45,711 - INFO - epoch complete!
2023-10-04 19:54:45,711 - INFO - evaluating now!
2023-10-04 19:54:45,711 - INFO - Epoch [812/4000] train_loss: 5.3253, lr: 0.000500, 0.04s
2023-10-04 19:54:45,750 - INFO - epoch complete!
2023-10-04 19:54:45,750 - INFO - evaluating now!
2023-10-04 19:54:45,750 - INFO - Epoch [813/4000] train_loss: 5.1559, lr: 0.000500, 0.04s
2023-10-04 19:54:46,000 - INFO - Saved model at 813
2023-10-04 19:54:46,000 - INFO - Val loss decrease from 5.1781 to 5.1559, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch813.tar
2023-10-04 19:54:46,040 - INFO - epoch complete!
2023-10-04 19:54:46,040 - INFO - evaluating now!
2023-10-04 19:54:46,040 - INFO - Epoch [814/4000] train_loss: 5.6037, lr: 0.000500, 0.04s
2023-10-04 19:54:46,079 - INFO - epoch complete!
2023-10-04 19:54:46,079 - INFO - evaluating now!
2023-10-04 19:54:46,079 - INFO - Epoch [815/4000] train_loss: 5.3932, lr: 0.000500, 0.04s
2023-10-04 19:54:46,118 - INFO - epoch complete!
2023-10-04 19:54:46,118 - INFO - evaluating now!
2023-10-04 19:54:46,118 - INFO - Epoch [816/4000] train_loss: 5.2223, lr: 0.000500, 0.04s
2023-10-04 19:54:46,156 - INFO - epoch complete!
2023-10-04 19:54:46,157 - INFO - evaluating now!
2023-10-04 19:54:46,157 - INFO - Epoch [817/4000] train_loss: 5.3430, lr: 0.000500, 0.04s
2023-10-04 19:54:46,195 - INFO - epoch complete!
2023-10-04 19:54:46,195 - INFO - evaluating now!
2023-10-04 19:54:46,195 - INFO - Epoch [818/4000] train_loss: 5.2900, lr: 0.000500, 0.04s
2023-10-04 19:54:46,233 - INFO - epoch complete!
2023-10-04 19:54:46,234 - INFO - evaluating now!
2023-10-04 19:54:46,234 - INFO - Epoch [819/4000] train_loss: 5.1281, lr: 0.000500, 0.04s
2023-10-04 19:54:46,478 - INFO - Saved model at 819
2023-10-04 19:54:46,479 - INFO - Val loss decrease from 5.1559 to 5.1281, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch819.tar
2023-10-04 19:54:46,519 - INFO - epoch complete!
2023-10-04 19:54:46,519 - INFO - evaluating now!
2023-10-04 19:54:46,520 - INFO - Epoch [820/4000] train_loss: 5.3994, lr: 0.000500, 0.04s
2023-10-04 19:54:46,559 - INFO - epoch complete!
2023-10-04 19:54:46,559 - INFO - evaluating now!
2023-10-04 19:54:46,559 - INFO - Epoch [821/4000] train_loss: 5.4489, lr: 0.000500, 0.04s
2023-10-04 19:54:46,598 - INFO - epoch complete!
2023-10-04 19:54:46,599 - INFO - evaluating now!
2023-10-04 19:54:46,599 - INFO - Epoch [822/4000] train_loss: 5.0460, lr: 0.000500, 0.04s
2023-10-04 19:54:46,829 - INFO - Saved model at 822
2023-10-04 19:54:46,829 - INFO - Val loss decrease from 5.1281 to 5.0460, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch822.tar
2023-10-04 19:54:46,869 - INFO - epoch complete!
2023-10-04 19:54:46,870 - INFO - evaluating now!
2023-10-04 19:54:46,870 - INFO - Epoch [823/4000] train_loss: 5.4935, lr: 0.000500, 0.04s
2023-10-04 19:54:46,909 - INFO - epoch complete!
2023-10-04 19:54:46,910 - INFO - evaluating now!
2023-10-04 19:54:46,910 - INFO - Epoch [824/4000] train_loss: 5.1526, lr: 0.000500, 0.04s
2023-10-04 19:54:46,949 - INFO - epoch complete!
2023-10-04 19:54:46,950 - INFO - evaluating now!
2023-10-04 19:54:46,950 - INFO - Epoch [825/4000] train_loss: 5.4849, lr: 0.000500, 0.04s
2023-10-04 19:54:46,989 - INFO - epoch complete!
2023-10-04 19:54:46,989 - INFO - evaluating now!
2023-10-04 19:54:46,990 - INFO - Epoch [826/4000] train_loss: 5.6128, lr: 0.000500, 0.04s
2023-10-04 19:54:47,029 - INFO - epoch complete!
2023-10-04 19:54:47,029 - INFO - evaluating now!
2023-10-04 19:54:47,029 - INFO - Epoch [827/4000] train_loss: 5.6553, lr: 0.000500, 0.04s
2023-10-04 19:54:47,068 - INFO - epoch complete!
2023-10-04 19:54:47,068 - INFO - evaluating now!
2023-10-04 19:54:47,069 - INFO - Epoch [828/4000] train_loss: 5.3880, lr: 0.000500, 0.04s
2023-10-04 19:54:47,108 - INFO - epoch complete!
2023-10-04 19:54:47,108 - INFO - evaluating now!
2023-10-04 19:54:47,108 - INFO - Epoch [829/4000] train_loss: 5.4333, lr: 0.000500, 0.04s
2023-10-04 19:54:47,147 - INFO - epoch complete!
2023-10-04 19:54:47,147 - INFO - evaluating now!
2023-10-04 19:54:47,148 - INFO - Epoch [830/4000] train_loss: 5.2612, lr: 0.000500, 0.04s
2023-10-04 19:54:47,186 - INFO - epoch complete!
2023-10-04 19:54:47,186 - INFO - evaluating now!
2023-10-04 19:54:47,186 - INFO - Epoch [831/4000] train_loss: 5.1699, lr: 0.000500, 0.04s
2023-10-04 19:54:47,225 - INFO - epoch complete!
2023-10-04 19:54:47,225 - INFO - evaluating now!
2023-10-04 19:54:47,225 - INFO - Epoch [832/4000] train_loss: 5.4679, lr: 0.000500, 0.04s
2023-10-04 19:54:47,264 - INFO - epoch complete!
2023-10-04 19:54:47,264 - INFO - evaluating now!
2023-10-04 19:54:47,264 - INFO - Epoch [833/4000] train_loss: 5.3230, lr: 0.000500, 0.04s
2023-10-04 19:54:47,303 - INFO - epoch complete!
2023-10-04 19:54:47,303 - INFO - evaluating now!
2023-10-04 19:54:47,303 - INFO - Epoch [834/4000] train_loss: 5.1386, lr: 0.000500, 0.04s
2023-10-04 19:54:47,341 - INFO - epoch complete!
2023-10-04 19:54:47,342 - INFO - evaluating now!
2023-10-04 19:54:47,342 - INFO - Epoch [835/4000] train_loss: 5.2698, lr: 0.000500, 0.04s
2023-10-04 19:54:47,380 - INFO - epoch complete!
2023-10-04 19:54:47,381 - INFO - evaluating now!
2023-10-04 19:54:47,381 - INFO - Epoch [836/4000] train_loss: 5.1750, lr: 0.000500, 0.04s
2023-10-04 19:54:47,419 - INFO - epoch complete!
2023-10-04 19:54:47,420 - INFO - evaluating now!
2023-10-04 19:54:47,420 - INFO - Epoch [837/4000] train_loss: 5.2896, lr: 0.000500, 0.04s
2023-10-04 19:54:47,458 - INFO - epoch complete!
2023-10-04 19:54:47,459 - INFO - evaluating now!
2023-10-04 19:54:47,459 - INFO - Epoch [838/4000] train_loss: 5.4600, lr: 0.000500, 0.04s
2023-10-04 19:54:47,497 - INFO - epoch complete!
2023-10-04 19:54:47,498 - INFO - evaluating now!
2023-10-04 19:54:47,498 - INFO - Epoch [839/4000] train_loss: 5.6787, lr: 0.000500, 0.04s
2023-10-04 19:54:47,536 - INFO - epoch complete!
2023-10-04 19:54:47,536 - INFO - evaluating now!
2023-10-04 19:54:47,537 - INFO - Epoch [840/4000] train_loss: 5.6563, lr: 0.000500, 0.04s
2023-10-04 19:54:47,575 - INFO - epoch complete!
2023-10-04 19:54:47,575 - INFO - evaluating now!
2023-10-04 19:54:47,575 - INFO - Epoch [841/4000] train_loss: 5.5154, lr: 0.000500, 0.04s
2023-10-04 19:54:47,614 - INFO - epoch complete!
2023-10-04 19:54:47,614 - INFO - evaluating now!
2023-10-04 19:54:47,614 - INFO - Epoch [842/4000] train_loss: 5.4267, lr: 0.000500, 0.04s
2023-10-04 19:54:47,653 - INFO - epoch complete!
2023-10-04 19:54:47,653 - INFO - evaluating now!
2023-10-04 19:54:47,653 - INFO - Epoch [843/4000] train_loss: 5.2799, lr: 0.000500, 0.04s
2023-10-04 19:54:47,692 - INFO - epoch complete!
2023-10-04 19:54:47,692 - INFO - evaluating now!
2023-10-04 19:54:47,692 - INFO - Epoch [844/4000] train_loss: 5.1922, lr: 0.000500, 0.04s
2023-10-04 19:54:47,731 - INFO - epoch complete!
2023-10-04 19:54:47,731 - INFO - evaluating now!
2023-10-04 19:54:47,731 - INFO - Epoch [845/4000] train_loss: 4.8466, lr: 0.000500, 0.04s
2023-10-04 19:54:47,961 - INFO - Saved model at 845
2023-10-04 19:54:47,962 - INFO - Val loss decrease from 5.0460 to 4.8466, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch845.tar
2023-10-04 19:54:48,002 - INFO - epoch complete!
2023-10-04 19:54:48,002 - INFO - evaluating now!
2023-10-04 19:54:48,002 - INFO - Epoch [846/4000] train_loss: 5.5573, lr: 0.000500, 0.04s
2023-10-04 19:54:48,042 - INFO - epoch complete!
2023-10-04 19:54:48,042 - INFO - evaluating now!
2023-10-04 19:54:48,042 - INFO - Epoch [847/4000] train_loss: 5.3130, lr: 0.000500, 0.04s
2023-10-04 19:54:48,081 - INFO - epoch complete!
2023-10-04 19:54:48,081 - INFO - evaluating now!
2023-10-04 19:54:48,081 - INFO - Epoch [848/4000] train_loss: 5.0013, lr: 0.000500, 0.04s
2023-10-04 19:54:48,120 - INFO - epoch complete!
2023-10-04 19:54:48,120 - INFO - evaluating now!
2023-10-04 19:54:48,120 - INFO - Epoch [849/4000] train_loss: 4.9919, lr: 0.000500, 0.04s
2023-10-04 19:54:48,159 - INFO - epoch complete!
2023-10-04 19:54:48,160 - INFO - evaluating now!
2023-10-04 19:54:48,160 - INFO - Epoch [850/4000] train_loss: 5.3936, lr: 0.000500, 0.04s
2023-10-04 19:54:48,198 - INFO - epoch complete!
2023-10-04 19:54:48,198 - INFO - evaluating now!
2023-10-04 19:54:48,199 - INFO - Epoch [851/4000] train_loss: 5.1419, lr: 0.000500, 0.04s
2023-10-04 19:54:48,237 - INFO - epoch complete!
2023-10-04 19:54:48,237 - INFO - evaluating now!
2023-10-04 19:54:48,238 - INFO - Epoch [852/4000] train_loss: 5.2014, lr: 0.000500, 0.04s
2023-10-04 19:54:48,276 - INFO - epoch complete!
2023-10-04 19:54:48,277 - INFO - evaluating now!
2023-10-04 19:54:48,277 - INFO - Epoch [853/4000] train_loss: 5.3048, lr: 0.000500, 0.04s
2023-10-04 19:54:48,315 - INFO - epoch complete!
2023-10-04 19:54:48,316 - INFO - evaluating now!
2023-10-04 19:54:48,316 - INFO - Epoch [854/4000] train_loss: 5.1767, lr: 0.000500, 0.04s
2023-10-04 19:54:48,354 - INFO - epoch complete!
2023-10-04 19:54:48,354 - INFO - evaluating now!
2023-10-04 19:54:48,355 - INFO - Epoch [855/4000] train_loss: 5.3277, lr: 0.000500, 0.04s
2023-10-04 19:54:48,393 - INFO - epoch complete!
2023-10-04 19:54:48,394 - INFO - evaluating now!
2023-10-04 19:54:48,394 - INFO - Epoch [856/4000] train_loss: 5.1780, lr: 0.000500, 0.04s
2023-10-04 19:54:48,434 - INFO - epoch complete!
2023-10-04 19:54:48,434 - INFO - evaluating now!
2023-10-04 19:54:48,434 - INFO - Epoch [857/4000] train_loss: 5.2866, lr: 0.000500, 0.04s
2023-10-04 19:54:48,473 - INFO - epoch complete!
2023-10-04 19:54:48,473 - INFO - evaluating now!
2023-10-04 19:54:48,474 - INFO - Epoch [858/4000] train_loss: 5.2463, lr: 0.000500, 0.04s
2023-10-04 19:54:48,512 - INFO - epoch complete!
2023-10-04 19:54:48,513 - INFO - evaluating now!
2023-10-04 19:54:48,513 - INFO - Epoch [859/4000] train_loss: 5.2211, lr: 0.000500, 0.04s
2023-10-04 19:54:48,551 - INFO - epoch complete!
2023-10-04 19:54:48,552 - INFO - evaluating now!
2023-10-04 19:54:48,552 - INFO - Epoch [860/4000] train_loss: 5.2722, lr: 0.000500, 0.04s
2023-10-04 19:54:48,591 - INFO - epoch complete!
2023-10-04 19:54:48,591 - INFO - evaluating now!
2023-10-04 19:54:48,591 - INFO - Epoch [861/4000] train_loss: 5.1811, lr: 0.000500, 0.04s
2023-10-04 19:54:48,630 - INFO - epoch complete!
2023-10-04 19:54:48,630 - INFO - evaluating now!
2023-10-04 19:54:48,631 - INFO - Epoch [862/4000] train_loss: 5.4421, lr: 0.000500, 0.04s
2023-10-04 19:54:48,670 - INFO - epoch complete!
2023-10-04 19:54:48,670 - INFO - evaluating now!
2023-10-04 19:54:48,670 - INFO - Epoch [863/4000] train_loss: 5.1834, lr: 0.000500, 0.04s
2023-10-04 19:54:48,710 - INFO - epoch complete!
2023-10-04 19:54:48,711 - INFO - evaluating now!
2023-10-04 19:54:48,711 - INFO - Epoch [864/4000] train_loss: 5.1054, lr: 0.000500, 0.04s
2023-10-04 19:54:48,750 - INFO - epoch complete!
2023-10-04 19:54:48,751 - INFO - evaluating now!
2023-10-04 19:54:48,751 - INFO - Epoch [865/4000] train_loss: 5.0066, lr: 0.000500, 0.04s
2023-10-04 19:54:48,790 - INFO - epoch complete!
2023-10-04 19:54:48,790 - INFO - evaluating now!
2023-10-04 19:54:48,790 - INFO - Epoch [866/4000] train_loss: 5.2557, lr: 0.000500, 0.04s
2023-10-04 19:54:48,829 - INFO - epoch complete!
2023-10-04 19:54:48,829 - INFO - evaluating now!
2023-10-04 19:54:48,829 - INFO - Epoch [867/4000] train_loss: 5.4639, lr: 0.000500, 0.04s
2023-10-04 19:54:48,868 - INFO - epoch complete!
2023-10-04 19:54:48,868 - INFO - evaluating now!
2023-10-04 19:54:48,868 - INFO - Epoch [868/4000] train_loss: 5.2479, lr: 0.000500, 0.04s
2023-10-04 19:54:48,907 - INFO - epoch complete!
2023-10-04 19:54:48,908 - INFO - evaluating now!
2023-10-04 19:54:48,908 - INFO - Epoch [869/4000] train_loss: 4.8681, lr: 0.000500, 0.04s
2023-10-04 19:54:48,946 - INFO - epoch complete!
2023-10-04 19:54:48,947 - INFO - evaluating now!
2023-10-04 19:54:48,947 - INFO - Epoch [870/4000] train_loss: 5.1386, lr: 0.000500, 0.04s
2023-10-04 19:54:48,985 - INFO - epoch complete!
2023-10-04 19:54:48,986 - INFO - evaluating now!
2023-10-04 19:54:48,986 - INFO - Epoch [871/4000] train_loss: 4.9300, lr: 0.000500, 0.04s
2023-10-04 19:54:49,024 - INFO - epoch complete!
2023-10-04 19:54:49,025 - INFO - evaluating now!
2023-10-04 19:54:49,025 - INFO - Epoch [872/4000] train_loss: 4.9295, lr: 0.000500, 0.04s
2023-10-04 19:54:49,063 - INFO - epoch complete!
2023-10-04 19:54:49,064 - INFO - evaluating now!
2023-10-04 19:54:49,064 - INFO - Epoch [873/4000] train_loss: 5.1401, lr: 0.000500, 0.04s
2023-10-04 19:54:49,103 - INFO - epoch complete!
2023-10-04 19:54:49,103 - INFO - evaluating now!
2023-10-04 19:54:49,103 - INFO - Epoch [874/4000] train_loss: 4.9877, lr: 0.000500, 0.04s
2023-10-04 19:54:49,142 - INFO - epoch complete!
2023-10-04 19:54:49,142 - INFO - evaluating now!
2023-10-04 19:54:49,143 - INFO - Epoch [875/4000] train_loss: 5.3367, lr: 0.000500, 0.04s
2023-10-04 19:54:49,181 - INFO - epoch complete!
2023-10-04 19:54:49,182 - INFO - evaluating now!
2023-10-04 19:54:49,182 - INFO - Epoch [876/4000] train_loss: 5.1139, lr: 0.000500, 0.04s
2023-10-04 19:54:49,220 - INFO - epoch complete!
2023-10-04 19:54:49,221 - INFO - evaluating now!
2023-10-04 19:54:49,221 - INFO - Epoch [877/4000] train_loss: 5.2033, lr: 0.000500, 0.04s
2023-10-04 19:54:49,260 - INFO - epoch complete!
2023-10-04 19:54:49,260 - INFO - evaluating now!
2023-10-04 19:54:49,260 - INFO - Epoch [878/4000] train_loss: 5.0525, lr: 0.000500, 0.04s
2023-10-04 19:54:49,299 - INFO - epoch complete!
2023-10-04 19:54:49,299 - INFO - evaluating now!
2023-10-04 19:54:49,299 - INFO - Epoch [879/4000] train_loss: 5.1272, lr: 0.000500, 0.04s
2023-10-04 19:54:49,338 - INFO - epoch complete!
2023-10-04 19:54:49,338 - INFO - evaluating now!
2023-10-04 19:54:49,338 - INFO - Epoch [880/4000] train_loss: 5.2231, lr: 0.000500, 0.04s
2023-10-04 19:54:49,377 - INFO - epoch complete!
2023-10-04 19:54:49,377 - INFO - evaluating now!
2023-10-04 19:54:49,377 - INFO - Epoch [881/4000] train_loss: 5.1735, lr: 0.000500, 0.04s
2023-10-04 19:54:49,416 - INFO - epoch complete!
2023-10-04 19:54:49,416 - INFO - evaluating now!
2023-10-04 19:54:49,417 - INFO - Epoch [882/4000] train_loss: 5.0386, lr: 0.000500, 0.04s
2023-10-04 19:54:49,455 - INFO - epoch complete!
2023-10-04 19:54:49,456 - INFO - evaluating now!
2023-10-04 19:54:49,456 - INFO - Epoch [883/4000] train_loss: 5.3054, lr: 0.000500, 0.04s
2023-10-04 19:54:49,495 - INFO - epoch complete!
2023-10-04 19:54:49,495 - INFO - evaluating now!
2023-10-04 19:54:49,495 - INFO - Epoch [884/4000] train_loss: 5.1239, lr: 0.000500, 0.04s
2023-10-04 19:54:49,534 - INFO - epoch complete!
2023-10-04 19:54:49,535 - INFO - evaluating now!
2023-10-04 19:54:49,535 - INFO - Epoch [885/4000] train_loss: 5.3891, lr: 0.000500, 0.04s
2023-10-04 19:54:49,573 - INFO - epoch complete!
2023-10-04 19:54:49,574 - INFO - evaluating now!
2023-10-04 19:54:49,574 - INFO - Epoch [886/4000] train_loss: 4.8642, lr: 0.000500, 0.04s
2023-10-04 19:54:49,612 - INFO - epoch complete!
2023-10-04 19:54:49,613 - INFO - evaluating now!
2023-10-04 19:54:49,613 - INFO - Epoch [887/4000] train_loss: 5.0218, lr: 0.000500, 0.04s
2023-10-04 19:54:49,652 - INFO - epoch complete!
2023-10-04 19:54:49,652 - INFO - evaluating now!
2023-10-04 19:54:49,652 - INFO - Epoch [888/4000] train_loss: 4.8690, lr: 0.000500, 0.04s
2023-10-04 19:54:49,691 - INFO - epoch complete!
2023-10-04 19:54:49,691 - INFO - evaluating now!
2023-10-04 19:54:49,691 - INFO - Epoch [889/4000] train_loss: 5.0386, lr: 0.000500, 0.04s
2023-10-04 19:54:49,729 - INFO - epoch complete!
2023-10-04 19:54:49,730 - INFO - evaluating now!
2023-10-04 19:54:49,730 - INFO - Epoch [890/4000] train_loss: 4.9125, lr: 0.000500, 0.04s
2023-10-04 19:54:49,769 - INFO - epoch complete!
2023-10-04 19:54:49,769 - INFO - evaluating now!
2023-10-04 19:54:49,769 - INFO - Epoch [891/4000] train_loss: 5.2115, lr: 0.000500, 0.04s
2023-10-04 19:54:49,807 - INFO - epoch complete!
2023-10-04 19:54:49,808 - INFO - evaluating now!
2023-10-04 19:54:49,808 - INFO - Epoch [892/4000] train_loss: 4.8843, lr: 0.000500, 0.04s
2023-10-04 19:54:49,846 - INFO - epoch complete!
2023-10-04 19:54:49,847 - INFO - evaluating now!
2023-10-04 19:54:49,847 - INFO - Epoch [893/4000] train_loss: 4.9719, lr: 0.000500, 0.04s
2023-10-04 19:54:49,885 - INFO - epoch complete!
2023-10-04 19:54:49,885 - INFO - evaluating now!
2023-10-04 19:54:49,886 - INFO - Epoch [894/4000] train_loss: 5.2005, lr: 0.000500, 0.04s
2023-10-04 19:54:49,924 - INFO - epoch complete!
2023-10-04 19:54:49,925 - INFO - evaluating now!
2023-10-04 19:54:49,925 - INFO - Epoch [895/4000] train_loss: 5.1286, lr: 0.000500, 0.04s
2023-10-04 19:54:49,963 - INFO - epoch complete!
2023-10-04 19:54:49,964 - INFO - evaluating now!
2023-10-04 19:54:49,964 - INFO - Epoch [896/4000] train_loss: 5.2800, lr: 0.000500, 0.04s
2023-10-04 19:54:50,002 - INFO - epoch complete!
2023-10-04 19:54:50,002 - INFO - evaluating now!
2023-10-04 19:54:50,002 - INFO - Epoch [897/4000] train_loss: 5.1912, lr: 0.000500, 0.04s
2023-10-04 19:54:50,041 - INFO - epoch complete!
2023-10-04 19:54:50,041 - INFO - evaluating now!
2023-10-04 19:54:50,041 - INFO - Epoch [898/4000] train_loss: 5.0877, lr: 0.000500, 0.04s
2023-10-04 19:54:50,080 - INFO - epoch complete!
2023-10-04 19:54:50,080 - INFO - evaluating now!
2023-10-04 19:54:50,080 - INFO - Epoch [899/4000] train_loss: 5.1560, lr: 0.000500, 0.04s
2023-10-04 19:54:50,119 - INFO - epoch complete!
2023-10-04 19:54:50,119 - INFO - evaluating now!
2023-10-04 19:54:50,119 - INFO - Epoch [900/4000] train_loss: 5.0096, lr: 0.000500, 0.04s
2023-10-04 19:54:50,158 - INFO - epoch complete!
2023-10-04 19:54:50,158 - INFO - evaluating now!
2023-10-04 19:54:50,158 - INFO - Epoch [901/4000] train_loss: 5.2443, lr: 0.000500, 0.04s
2023-10-04 19:54:50,197 - INFO - epoch complete!
2023-10-04 19:54:50,197 - INFO - evaluating now!
2023-10-04 19:54:50,197 - INFO - Epoch [902/4000] train_loss: 5.0057, lr: 0.000500, 0.04s
2023-10-04 19:54:50,235 - INFO - epoch complete!
2023-10-04 19:54:50,236 - INFO - evaluating now!
2023-10-04 19:54:50,236 - INFO - Epoch [903/4000] train_loss: 5.0831, lr: 0.000500, 0.04s
2023-10-04 19:54:50,274 - INFO - epoch complete!
2023-10-04 19:54:50,275 - INFO - evaluating now!
2023-10-04 19:54:50,275 - INFO - Epoch [904/4000] train_loss: 5.0266, lr: 0.000500, 0.04s
2023-10-04 19:54:50,313 - INFO - epoch complete!
2023-10-04 19:54:50,313 - INFO - evaluating now!
2023-10-04 19:54:50,313 - INFO - Epoch [905/4000] train_loss: 5.5780, lr: 0.000500, 0.04s
2023-10-04 19:54:50,352 - INFO - epoch complete!
2023-10-04 19:54:50,352 - INFO - evaluating now!
2023-10-04 19:54:50,352 - INFO - Epoch [906/4000] train_loss: 5.2979, lr: 0.000500, 0.04s
2023-10-04 19:54:50,391 - INFO - epoch complete!
2023-10-04 19:54:50,391 - INFO - evaluating now!
2023-10-04 19:54:50,391 - INFO - Epoch [907/4000] train_loss: 5.6544, lr: 0.000500, 0.04s
2023-10-04 19:54:50,430 - INFO - epoch complete!
2023-10-04 19:54:50,430 - INFO - evaluating now!
2023-10-04 19:54:50,430 - INFO - Epoch [908/4000] train_loss: 5.5104, lr: 0.000500, 0.04s
2023-10-04 19:54:50,469 - INFO - epoch complete!
2023-10-04 19:54:50,469 - INFO - evaluating now!
2023-10-04 19:54:50,469 - INFO - Epoch [909/4000] train_loss: 5.1596, lr: 0.000500, 0.04s
2023-10-04 19:54:50,508 - INFO - epoch complete!
2023-10-04 19:54:50,508 - INFO - evaluating now!
2023-10-04 19:54:50,508 - INFO - Epoch [910/4000] train_loss: 4.8737, lr: 0.000500, 0.04s
2023-10-04 19:54:50,547 - INFO - epoch complete!
2023-10-04 19:54:50,547 - INFO - evaluating now!
2023-10-04 19:54:50,547 - INFO - Epoch [911/4000] train_loss: 5.4042, lr: 0.000500, 0.04s
2023-10-04 19:54:50,586 - INFO - epoch complete!
2023-10-04 19:54:50,586 - INFO - evaluating now!
2023-10-04 19:54:50,586 - INFO - Epoch [912/4000] train_loss: 4.7846, lr: 0.000500, 0.04s
2023-10-04 19:54:50,816 - INFO - Saved model at 912
2023-10-04 19:54:50,816 - INFO - Val loss decrease from 4.8466 to 4.7846, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch912.tar
2023-10-04 19:54:50,856 - INFO - epoch complete!
2023-10-04 19:54:50,857 - INFO - evaluating now!
2023-10-04 19:54:50,857 - INFO - Epoch [913/4000] train_loss: 5.0148, lr: 0.000500, 0.04s
2023-10-04 19:54:50,897 - INFO - epoch complete!
2023-10-04 19:54:50,897 - INFO - evaluating now!
2023-10-04 19:54:50,897 - INFO - Epoch [914/4000] train_loss: 4.9380, lr: 0.000500, 0.04s
2023-10-04 19:54:50,937 - INFO - epoch complete!
2023-10-04 19:54:50,937 - INFO - evaluating now!
2023-10-04 19:54:50,937 - INFO - Epoch [915/4000] train_loss: 4.9873, lr: 0.000500, 0.04s
2023-10-04 19:54:50,977 - INFO - epoch complete!
2023-10-04 19:54:50,977 - INFO - evaluating now!
2023-10-04 19:54:50,978 - INFO - Epoch [916/4000] train_loss: 5.3020, lr: 0.000500, 0.04s
2023-10-04 19:54:51,017 - INFO - epoch complete!
2023-10-04 19:54:51,018 - INFO - evaluating now!
2023-10-04 19:54:51,018 - INFO - Epoch [917/4000] train_loss: 5.0954, lr: 0.000500, 0.04s
2023-10-04 19:54:51,058 - INFO - epoch complete!
2023-10-04 19:54:51,058 - INFO - evaluating now!
2023-10-04 19:54:51,058 - INFO - Epoch [918/4000] train_loss: 4.8104, lr: 0.000500, 0.04s
2023-10-04 19:54:51,098 - INFO - epoch complete!
2023-10-04 19:54:51,098 - INFO - evaluating now!
2023-10-04 19:54:51,098 - INFO - Epoch [919/4000] train_loss: 5.2474, lr: 0.000500, 0.04s
2023-10-04 19:54:51,138 - INFO - epoch complete!
2023-10-04 19:54:51,138 - INFO - evaluating now!
2023-10-04 19:54:51,138 - INFO - Epoch [920/4000] train_loss: 5.1118, lr: 0.000500, 0.04s
2023-10-04 19:54:51,178 - INFO - epoch complete!
2023-10-04 19:54:51,178 - INFO - evaluating now!
2023-10-04 19:54:51,179 - INFO - Epoch [921/4000] train_loss: 5.3017, lr: 0.000500, 0.04s
2023-10-04 19:54:51,218 - INFO - epoch complete!
2023-10-04 19:54:51,218 - INFO - evaluating now!
2023-10-04 19:54:51,218 - INFO - Epoch [922/4000] train_loss: 5.0094, lr: 0.000500, 0.04s
2023-10-04 19:54:51,259 - INFO - epoch complete!
2023-10-04 19:54:51,259 - INFO - evaluating now!
2023-10-04 19:54:51,259 - INFO - Epoch [923/4000] train_loss: 4.9943, lr: 0.000500, 0.04s
2023-10-04 19:54:51,299 - INFO - epoch complete!
2023-10-04 19:54:51,299 - INFO - evaluating now!
2023-10-04 19:54:51,299 - INFO - Epoch [924/4000] train_loss: 4.8985, lr: 0.000500, 0.04s
2023-10-04 19:54:51,337 - INFO - epoch complete!
2023-10-04 19:54:51,338 - INFO - evaluating now!
2023-10-04 19:54:51,338 - INFO - Epoch [925/4000] train_loss: 5.2049, lr: 0.000500, 0.04s
2023-10-04 19:54:51,376 - INFO - epoch complete!
2023-10-04 19:54:51,377 - INFO - evaluating now!
2023-10-04 19:54:51,377 - INFO - Epoch [926/4000] train_loss: 4.8174, lr: 0.000500, 0.04s
2023-10-04 19:54:51,415 - INFO - epoch complete!
2023-10-04 19:54:51,415 - INFO - evaluating now!
2023-10-04 19:54:51,415 - INFO - Epoch [927/4000] train_loss: 5.1260, lr: 0.000500, 0.04s
2023-10-04 19:54:51,454 - INFO - epoch complete!
2023-10-04 19:54:51,454 - INFO - evaluating now!
2023-10-04 19:54:51,454 - INFO - Epoch [928/4000] train_loss: 4.9148, lr: 0.000500, 0.04s
2023-10-04 19:54:51,493 - INFO - epoch complete!
2023-10-04 19:54:51,493 - INFO - evaluating now!
2023-10-04 19:54:51,493 - INFO - Epoch [929/4000] train_loss: 4.8309, lr: 0.000500, 0.04s
2023-10-04 19:54:51,532 - INFO - epoch complete!
2023-10-04 19:54:51,532 - INFO - evaluating now!
2023-10-04 19:54:51,532 - INFO - Epoch [930/4000] train_loss: 5.0478, lr: 0.000500, 0.04s
2023-10-04 19:54:51,571 - INFO - epoch complete!
2023-10-04 19:54:51,571 - INFO - evaluating now!
2023-10-04 19:54:51,571 - INFO - Epoch [931/4000] train_loss: 4.6950, lr: 0.000500, 0.04s
2023-10-04 19:54:51,801 - INFO - Saved model at 931
2023-10-04 19:54:51,801 - INFO - Val loss decrease from 4.7846 to 4.6950, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch931.tar
2023-10-04 19:54:51,841 - INFO - epoch complete!
2023-10-04 19:54:51,842 - INFO - evaluating now!
2023-10-04 19:54:51,842 - INFO - Epoch [932/4000] train_loss: 4.9615, lr: 0.000500, 0.04s
2023-10-04 19:54:51,881 - INFO - epoch complete!
2023-10-04 19:54:51,882 - INFO - evaluating now!
2023-10-04 19:54:51,882 - INFO - Epoch [933/4000] train_loss: 4.7013, lr: 0.000500, 0.04s
2023-10-04 19:54:51,921 - INFO - epoch complete!
2023-10-04 19:54:51,921 - INFO - evaluating now!
2023-10-04 19:54:51,921 - INFO - Epoch [934/4000] train_loss: 4.7075, lr: 0.000500, 0.04s
2023-10-04 19:54:51,960 - INFO - epoch complete!
2023-10-04 19:54:51,961 - INFO - evaluating now!
2023-10-04 19:54:51,961 - INFO - Epoch [935/4000] train_loss: 5.1892, lr: 0.000500, 0.04s
2023-10-04 19:54:52,000 - INFO - epoch complete!
2023-10-04 19:54:52,000 - INFO - evaluating now!
2023-10-04 19:54:52,000 - INFO - Epoch [936/4000] train_loss: 4.9917, lr: 0.000500, 0.04s
2023-10-04 19:54:52,040 - INFO - epoch complete!
2023-10-04 19:54:52,040 - INFO - evaluating now!
2023-10-04 19:54:52,040 - INFO - Epoch [937/4000] train_loss: 4.9071, lr: 0.000500, 0.04s
2023-10-04 19:54:52,079 - INFO - epoch complete!
2023-10-04 19:54:52,080 - INFO - evaluating now!
2023-10-04 19:54:52,080 - INFO - Epoch [938/4000] train_loss: 5.0945, lr: 0.000500, 0.04s
2023-10-04 19:54:52,119 - INFO - epoch complete!
2023-10-04 19:54:52,119 - INFO - evaluating now!
2023-10-04 19:54:52,119 - INFO - Epoch [939/4000] train_loss: 4.9479, lr: 0.000500, 0.04s
2023-10-04 19:54:52,159 - INFO - epoch complete!
2023-10-04 19:54:52,159 - INFO - evaluating now!
2023-10-04 19:54:52,159 - INFO - Epoch [940/4000] train_loss: 5.1017, lr: 0.000500, 0.04s
2023-10-04 19:54:52,198 - INFO - epoch complete!
2023-10-04 19:54:52,199 - INFO - evaluating now!
2023-10-04 19:54:52,199 - INFO - Epoch [941/4000] train_loss: 4.7050, lr: 0.000500, 0.04s
2023-10-04 19:54:52,238 - INFO - epoch complete!
2023-10-04 19:54:52,238 - INFO - evaluating now!
2023-10-04 19:54:52,238 - INFO - Epoch [942/4000] train_loss: 4.7591, lr: 0.000500, 0.04s
2023-10-04 19:54:52,277 - INFO - epoch complete!
2023-10-04 19:54:52,277 - INFO - evaluating now!
2023-10-04 19:54:52,278 - INFO - Epoch [943/4000] train_loss: 4.9765, lr: 0.000500, 0.04s
2023-10-04 19:54:52,316 - INFO - epoch complete!
2023-10-04 19:54:52,316 - INFO - evaluating now!
2023-10-04 19:54:52,317 - INFO - Epoch [944/4000] train_loss: 4.6460, lr: 0.000500, 0.04s
2023-10-04 19:54:52,548 - INFO - Saved model at 944
2023-10-04 19:54:52,548 - INFO - Val loss decrease from 4.6950 to 4.6460, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch944.tar
2023-10-04 19:54:52,589 - INFO - epoch complete!
2023-10-04 19:54:52,589 - INFO - evaluating now!
2023-10-04 19:54:52,589 - INFO - Epoch [945/4000] train_loss: 5.1819, lr: 0.000500, 0.04s
2023-10-04 19:54:52,629 - INFO - epoch complete!
2023-10-04 19:54:52,629 - INFO - evaluating now!
2023-10-04 19:54:52,629 - INFO - Epoch [946/4000] train_loss: 5.0612, lr: 0.000500, 0.04s
2023-10-04 19:54:52,668 - INFO - epoch complete!
2023-10-04 19:54:52,669 - INFO - evaluating now!
2023-10-04 19:54:52,669 - INFO - Epoch [947/4000] train_loss: 5.2284, lr: 0.000500, 0.04s
2023-10-04 19:54:52,708 - INFO - epoch complete!
2023-10-04 19:54:52,708 - INFO - evaluating now!
2023-10-04 19:54:52,708 - INFO - Epoch [948/4000] train_loss: 4.8903, lr: 0.000500, 0.04s
2023-10-04 19:54:52,746 - INFO - epoch complete!
2023-10-04 19:54:52,747 - INFO - evaluating now!
2023-10-04 19:54:52,747 - INFO - Epoch [949/4000] train_loss: 5.0828, lr: 0.000500, 0.04s
2023-10-04 19:54:52,785 - INFO - epoch complete!
2023-10-04 19:54:52,786 - INFO - evaluating now!
2023-10-04 19:54:52,786 - INFO - Epoch [950/4000] train_loss: 4.7131, lr: 0.000500, 0.04s
2023-10-04 19:54:52,824 - INFO - epoch complete!
2023-10-04 19:54:52,824 - INFO - evaluating now!
2023-10-04 19:54:52,825 - INFO - Epoch [951/4000] train_loss: 4.9738, lr: 0.000500, 0.04s
2023-10-04 19:54:52,863 - INFO - epoch complete!
2023-10-04 19:54:52,863 - INFO - evaluating now!
2023-10-04 19:54:52,863 - INFO - Epoch [952/4000] train_loss: 4.7121, lr: 0.000500, 0.04s
2023-10-04 19:54:52,902 - INFO - epoch complete!
2023-10-04 19:54:52,902 - INFO - evaluating now!
2023-10-04 19:54:52,902 - INFO - Epoch [953/4000] train_loss: 5.1784, lr: 0.000500, 0.04s
2023-10-04 19:54:52,940 - INFO - epoch complete!
2023-10-04 19:54:52,941 - INFO - evaluating now!
2023-10-04 19:54:52,941 - INFO - Epoch [954/4000] train_loss: 5.0048, lr: 0.000500, 0.04s
2023-10-04 19:54:52,979 - INFO - epoch complete!
2023-10-04 19:54:52,979 - INFO - evaluating now!
2023-10-04 19:54:52,979 - INFO - Epoch [955/4000] train_loss: 4.8689, lr: 0.000500, 0.04s
2023-10-04 19:54:53,018 - INFO - epoch complete!
2023-10-04 19:54:53,018 - INFO - evaluating now!
2023-10-04 19:54:53,018 - INFO - Epoch [956/4000] train_loss: 5.1065, lr: 0.000500, 0.04s
2023-10-04 19:54:53,056 - INFO - epoch complete!
2023-10-04 19:54:53,057 - INFO - evaluating now!
2023-10-04 19:54:53,057 - INFO - Epoch [957/4000] train_loss: 4.8401, lr: 0.000500, 0.04s
2023-10-04 19:54:53,095 - INFO - epoch complete!
2023-10-04 19:54:53,095 - INFO - evaluating now!
2023-10-04 19:54:53,096 - INFO - Epoch [958/4000] train_loss: 4.5246, lr: 0.000500, 0.04s
2023-10-04 19:54:53,326 - INFO - Saved model at 958
2023-10-04 19:54:53,326 - INFO - Val loss decrease from 4.6460 to 4.5246, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch958.tar
2023-10-04 19:54:53,366 - INFO - epoch complete!
2023-10-04 19:54:53,366 - INFO - evaluating now!
2023-10-04 19:54:53,366 - INFO - Epoch [959/4000] train_loss: 4.7721, lr: 0.000500, 0.04s
2023-10-04 19:54:53,406 - INFO - epoch complete!
2023-10-04 19:54:53,406 - INFO - evaluating now!
2023-10-04 19:54:53,407 - INFO - Epoch [960/4000] train_loss: 4.6580, lr: 0.000500, 0.04s
2023-10-04 19:54:53,446 - INFO - epoch complete!
2023-10-04 19:54:53,447 - INFO - evaluating now!
2023-10-04 19:54:53,447 - INFO - Epoch [961/4000] train_loss: 5.0570, lr: 0.000500, 0.04s
2023-10-04 19:54:53,487 - INFO - epoch complete!
2023-10-04 19:54:53,487 - INFO - evaluating now!
2023-10-04 19:54:53,487 - INFO - Epoch [962/4000] train_loss: 4.8314, lr: 0.000500, 0.04s
2023-10-04 19:54:53,527 - INFO - epoch complete!
2023-10-04 19:54:53,527 - INFO - evaluating now!
2023-10-04 19:54:53,527 - INFO - Epoch [963/4000] train_loss: 4.6944, lr: 0.000500, 0.04s
2023-10-04 19:54:53,567 - INFO - epoch complete!
2023-10-04 19:54:53,567 - INFO - evaluating now!
2023-10-04 19:54:53,567 - INFO - Epoch [964/4000] train_loss: 5.2314, lr: 0.000500, 0.04s
2023-10-04 19:54:53,606 - INFO - epoch complete!
2023-10-04 19:54:53,606 - INFO - evaluating now!
2023-10-04 19:54:53,607 - INFO - Epoch [965/4000] train_loss: 4.7214, lr: 0.000500, 0.04s
2023-10-04 19:54:53,646 - INFO - epoch complete!
2023-10-04 19:54:53,646 - INFO - evaluating now!
2023-10-04 19:54:53,646 - INFO - Epoch [966/4000] train_loss: 4.8342, lr: 0.000500, 0.04s
2023-10-04 19:54:53,684 - INFO - epoch complete!
2023-10-04 19:54:53,685 - INFO - evaluating now!
2023-10-04 19:54:53,685 - INFO - Epoch [967/4000] train_loss: 5.1606, lr: 0.000500, 0.04s
2023-10-04 19:54:53,723 - INFO - epoch complete!
2023-10-04 19:54:53,723 - INFO - evaluating now!
2023-10-04 19:54:53,724 - INFO - Epoch [968/4000] train_loss: 4.6665, lr: 0.000500, 0.04s
2023-10-04 19:54:53,762 - INFO - epoch complete!
2023-10-04 19:54:53,762 - INFO - evaluating now!
2023-10-04 19:54:53,762 - INFO - Epoch [969/4000] train_loss: 4.7502, lr: 0.000500, 0.04s
2023-10-04 19:54:53,801 - INFO - epoch complete!
2023-10-04 19:54:53,801 - INFO - evaluating now!
2023-10-04 19:54:53,801 - INFO - Epoch [970/4000] train_loss: 4.6812, lr: 0.000500, 0.04s
2023-10-04 19:54:53,839 - INFO - epoch complete!
2023-10-04 19:54:53,840 - INFO - evaluating now!
2023-10-04 19:54:53,840 - INFO - Epoch [971/4000] train_loss: 4.9923, lr: 0.000500, 0.04s
2023-10-04 19:54:53,878 - INFO - epoch complete!
2023-10-04 19:54:53,878 - INFO - evaluating now!
2023-10-04 19:54:53,878 - INFO - Epoch [972/4000] train_loss: 4.9016, lr: 0.000500, 0.04s
2023-10-04 19:54:53,917 - INFO - epoch complete!
2023-10-04 19:54:53,917 - INFO - evaluating now!
2023-10-04 19:54:53,917 - INFO - Epoch [973/4000] train_loss: 4.6327, lr: 0.000500, 0.04s
2023-10-04 19:54:53,956 - INFO - epoch complete!
2023-10-04 19:54:53,956 - INFO - evaluating now!
2023-10-04 19:54:53,956 - INFO - Epoch [974/4000] train_loss: 4.8198, lr: 0.000500, 0.04s
2023-10-04 19:54:53,995 - INFO - epoch complete!
2023-10-04 19:54:53,995 - INFO - evaluating now!
2023-10-04 19:54:53,995 - INFO - Epoch [975/4000] train_loss: 5.0172, lr: 0.000500, 0.04s
2023-10-04 19:54:54,034 - INFO - epoch complete!
2023-10-04 19:54:54,034 - INFO - evaluating now!
2023-10-04 19:54:54,034 - INFO - Epoch [976/4000] train_loss: 4.8258, lr: 0.000500, 0.04s
2023-10-04 19:54:54,072 - INFO - epoch complete!
2023-10-04 19:54:54,073 - INFO - evaluating now!
2023-10-04 19:54:54,073 - INFO - Epoch [977/4000] train_loss: 5.1035, lr: 0.000500, 0.04s
2023-10-04 19:54:54,111 - INFO - epoch complete!
2023-10-04 19:54:54,112 - INFO - evaluating now!
2023-10-04 19:54:54,112 - INFO - Epoch [978/4000] train_loss: 4.8097, lr: 0.000500, 0.04s
2023-10-04 19:54:54,150 - INFO - epoch complete!
2023-10-04 19:54:54,150 - INFO - evaluating now!
2023-10-04 19:54:54,151 - INFO - Epoch [979/4000] train_loss: 5.0432, lr: 0.000500, 0.04s
2023-10-04 19:54:54,189 - INFO - epoch complete!
2023-10-04 19:54:54,189 - INFO - evaluating now!
2023-10-04 19:54:54,189 - INFO - Epoch [980/4000] train_loss: 4.9193, lr: 0.000500, 0.04s
2023-10-04 19:54:54,228 - INFO - epoch complete!
2023-10-04 19:54:54,228 - INFO - evaluating now!
2023-10-04 19:54:54,228 - INFO - Epoch [981/4000] train_loss: 4.8843, lr: 0.000500, 0.04s
2023-10-04 19:54:54,267 - INFO - epoch complete!
2023-10-04 19:54:54,267 - INFO - evaluating now!
2023-10-04 19:54:54,267 - INFO - Epoch [982/4000] train_loss: 4.7858, lr: 0.000500, 0.04s
2023-10-04 19:54:54,305 - INFO - epoch complete!
2023-10-04 19:54:54,306 - INFO - evaluating now!
2023-10-04 19:54:54,306 - INFO - Epoch [983/4000] train_loss: 4.9436, lr: 0.000500, 0.04s
2023-10-04 19:54:54,344 - INFO - epoch complete!
2023-10-04 19:54:54,345 - INFO - evaluating now!
2023-10-04 19:54:54,345 - INFO - Epoch [984/4000] train_loss: 4.9233, lr: 0.000500, 0.04s
2023-10-04 19:54:54,383 - INFO - epoch complete!
2023-10-04 19:54:54,383 - INFO - evaluating now!
2023-10-04 19:54:54,383 - INFO - Epoch [985/4000] train_loss: 4.8330, lr: 0.000500, 0.04s
2023-10-04 19:54:54,422 - INFO - epoch complete!
2023-10-04 19:54:54,422 - INFO - evaluating now!
2023-10-04 19:54:54,422 - INFO - Epoch [986/4000] train_loss: 4.6794, lr: 0.000500, 0.04s
2023-10-04 19:54:54,461 - INFO - epoch complete!
2023-10-04 19:54:54,461 - INFO - evaluating now!
2023-10-04 19:54:54,461 - INFO - Epoch [987/4000] train_loss: 4.7073, lr: 0.000500, 0.04s
2023-10-04 19:54:54,500 - INFO - epoch complete!
2023-10-04 19:54:54,500 - INFO - evaluating now!
2023-10-04 19:54:54,500 - INFO - Epoch [988/4000] train_loss: 4.9518, lr: 0.000500, 0.04s
2023-10-04 19:54:54,540 - INFO - epoch complete!
2023-10-04 19:54:54,540 - INFO - evaluating now!
2023-10-04 19:54:54,541 - INFO - Epoch [989/4000] train_loss: 4.9199, lr: 0.000500, 0.04s
2023-10-04 19:54:54,579 - INFO - epoch complete!
2023-10-04 19:54:54,579 - INFO - evaluating now!
2023-10-04 19:54:54,579 - INFO - Epoch [990/4000] train_loss: 4.9270, lr: 0.000500, 0.04s
2023-10-04 19:54:54,618 - INFO - epoch complete!
2023-10-04 19:54:54,618 - INFO - evaluating now!
2023-10-04 19:54:54,618 - INFO - Epoch [991/4000] train_loss: 5.0896, lr: 0.000500, 0.04s
2023-10-04 19:54:54,657 - INFO - epoch complete!
2023-10-04 19:54:54,657 - INFO - evaluating now!
2023-10-04 19:54:54,657 - INFO - Epoch [992/4000] train_loss: 4.4771, lr: 0.000500, 0.04s
2023-10-04 19:54:54,887 - INFO - Saved model at 992
2023-10-04 19:54:54,888 - INFO - Val loss decrease from 4.5246 to 4.4771, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch992.tar
2023-10-04 19:54:54,927 - INFO - epoch complete!
2023-10-04 19:54:54,928 - INFO - evaluating now!
2023-10-04 19:54:54,928 - INFO - Epoch [993/4000] train_loss: 4.9585, lr: 0.000500, 0.04s
2023-10-04 19:54:54,967 - INFO - epoch complete!
2023-10-04 19:54:54,968 - INFO - evaluating now!
2023-10-04 19:54:54,968 - INFO - Epoch [994/4000] train_loss: 4.7261, lr: 0.000500, 0.04s
2023-10-04 19:54:55,006 - INFO - epoch complete!
2023-10-04 19:54:55,007 - INFO - evaluating now!
2023-10-04 19:54:55,007 - INFO - Epoch [995/4000] train_loss: 4.8264, lr: 0.000500, 0.04s
2023-10-04 19:54:55,045 - INFO - epoch complete!
2023-10-04 19:54:55,046 - INFO - evaluating now!
2023-10-04 19:54:55,046 - INFO - Epoch [996/4000] train_loss: 4.5688, lr: 0.000500, 0.04s
2023-10-04 19:54:55,084 - INFO - epoch complete!
2023-10-04 19:54:55,084 - INFO - evaluating now!
2023-10-04 19:54:55,085 - INFO - Epoch [997/4000] train_loss: 4.7501, lr: 0.000500, 0.04s
2023-10-04 19:54:55,123 - INFO - epoch complete!
2023-10-04 19:54:55,124 - INFO - evaluating now!
2023-10-04 19:54:55,124 - INFO - Epoch [998/4000] train_loss: 4.4555, lr: 0.000500, 0.04s
2023-10-04 19:54:55,354 - INFO - Saved model at 998
2023-10-04 19:54:55,354 - INFO - Val loss decrease from 4.4771 to 4.4555, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch998.tar
2023-10-04 19:54:55,395 - INFO - epoch complete!
2023-10-04 19:54:55,395 - INFO - evaluating now!
2023-10-04 19:54:55,396 - INFO - Epoch [999/4000] train_loss: 4.9101, lr: 0.000500, 0.04s
2023-10-04 19:54:55,435 - INFO - epoch complete!
2023-10-04 19:54:55,435 - INFO - evaluating now!
2023-10-04 19:54:55,435 - INFO - Epoch [1000/4000] train_loss: 4.5656, lr: 0.000500, 0.04s
2023-10-04 19:54:55,475 - INFO - epoch complete!
2023-10-04 19:54:55,475 - INFO - evaluating now!
2023-10-04 19:54:55,475 - INFO - Epoch [1001/4000] train_loss: 5.0329, lr: 0.000500, 0.04s
2023-10-04 19:54:55,514 - INFO - epoch complete!
2023-10-04 19:54:55,514 - INFO - evaluating now!
2023-10-04 19:54:55,514 - INFO - Epoch [1002/4000] train_loss: 4.9200, lr: 0.000500, 0.04s
2023-10-04 19:54:55,553 - INFO - epoch complete!
2023-10-04 19:54:55,553 - INFO - evaluating now!
2023-10-04 19:54:55,554 - INFO - Epoch [1003/4000] train_loss: 4.8972, lr: 0.000500, 0.04s
2023-10-04 19:54:55,592 - INFO - epoch complete!
2023-10-04 19:54:55,592 - INFO - evaluating now!
2023-10-04 19:54:55,593 - INFO - Epoch [1004/4000] train_loss: 4.8415, lr: 0.000500, 0.04s
2023-10-04 19:54:55,631 - INFO - epoch complete!
2023-10-04 19:54:55,632 - INFO - evaluating now!
2023-10-04 19:54:55,632 - INFO - Epoch [1005/4000] train_loss: 4.6614, lr: 0.000500, 0.04s
2023-10-04 19:54:55,670 - INFO - epoch complete!
2023-10-04 19:54:55,671 - INFO - evaluating now!
2023-10-04 19:54:55,671 - INFO - Epoch [1006/4000] train_loss: 4.5985, lr: 0.000500, 0.04s
2023-10-04 19:54:55,710 - INFO - epoch complete!
2023-10-04 19:54:55,710 - INFO - evaluating now!
2023-10-04 19:54:55,710 - INFO - Epoch [1007/4000] train_loss: 4.7342, lr: 0.000500, 0.04s
2023-10-04 19:54:55,749 - INFO - epoch complete!
2023-10-04 19:54:55,749 - INFO - evaluating now!
2023-10-04 19:54:55,749 - INFO - Epoch [1008/4000] train_loss: 4.7909, lr: 0.000500, 0.04s
2023-10-04 19:54:55,788 - INFO - epoch complete!
2023-10-04 19:54:55,788 - INFO - evaluating now!
2023-10-04 19:54:55,789 - INFO - Epoch [1009/4000] train_loss: 4.6275, lr: 0.000500, 0.04s
2023-10-04 19:54:55,827 - INFO - epoch complete!
2023-10-04 19:54:55,828 - INFO - evaluating now!
2023-10-04 19:54:55,828 - INFO - Epoch [1010/4000] train_loss: 5.1048, lr: 0.000500, 0.04s
2023-10-04 19:54:55,866 - INFO - epoch complete!
2023-10-04 19:54:55,867 - INFO - evaluating now!
2023-10-04 19:54:55,867 - INFO - Epoch [1011/4000] train_loss: 4.8215, lr: 0.000500, 0.04s
2023-10-04 19:54:55,906 - INFO - epoch complete!
2023-10-04 19:54:55,906 - INFO - evaluating now!
2023-10-04 19:54:55,906 - INFO - Epoch [1012/4000] train_loss: 4.8820, lr: 0.000500, 0.04s
2023-10-04 19:54:55,945 - INFO - epoch complete!
2023-10-04 19:54:55,945 - INFO - evaluating now!
2023-10-04 19:54:55,946 - INFO - Epoch [1013/4000] train_loss: 4.7267, lr: 0.000500, 0.04s
2023-10-04 19:54:55,984 - INFO - epoch complete!
2023-10-04 19:54:55,984 - INFO - evaluating now!
2023-10-04 19:54:55,984 - INFO - Epoch [1014/4000] train_loss: 4.9103, lr: 0.000500, 0.04s
2023-10-04 19:54:56,023 - INFO - epoch complete!
2023-10-04 19:54:56,023 - INFO - evaluating now!
2023-10-04 19:54:56,023 - INFO - Epoch [1015/4000] train_loss: 5.0179, lr: 0.000500, 0.04s
2023-10-04 19:54:56,062 - INFO - epoch complete!
2023-10-04 19:54:56,062 - INFO - evaluating now!
2023-10-04 19:54:56,062 - INFO - Epoch [1016/4000] train_loss: 4.6851, lr: 0.000500, 0.04s
2023-10-04 19:54:56,101 - INFO - epoch complete!
2023-10-04 19:54:56,101 - INFO - evaluating now!
2023-10-04 19:54:56,102 - INFO - Epoch [1017/4000] train_loss: 4.6384, lr: 0.000500, 0.04s
2023-10-04 19:54:56,140 - INFO - epoch complete!
2023-10-04 19:54:56,140 - INFO - evaluating now!
2023-10-04 19:54:56,141 - INFO - Epoch [1018/4000] train_loss: 4.8355, lr: 0.000500, 0.04s
2023-10-04 19:54:56,180 - INFO - epoch complete!
2023-10-04 19:54:56,181 - INFO - evaluating now!
2023-10-04 19:54:56,181 - INFO - Epoch [1019/4000] train_loss: 4.9079, lr: 0.000500, 0.04s
2023-10-04 19:54:56,220 - INFO - epoch complete!
2023-10-04 19:54:56,221 - INFO - evaluating now!
2023-10-04 19:54:56,221 - INFO - Epoch [1020/4000] train_loss: 4.5865, lr: 0.000500, 0.04s
2023-10-04 19:54:56,260 - INFO - epoch complete!
2023-10-04 19:54:56,260 - INFO - evaluating now!
2023-10-04 19:54:56,260 - INFO - Epoch [1021/4000] train_loss: 4.8223, lr: 0.000500, 0.04s
2023-10-04 19:54:56,299 - INFO - epoch complete!
2023-10-04 19:54:56,299 - INFO - evaluating now!
2023-10-04 19:54:56,299 - INFO - Epoch [1022/4000] train_loss: 4.6643, lr: 0.000500, 0.04s
2023-10-04 19:54:56,338 - INFO - epoch complete!
2023-10-04 19:54:56,338 - INFO - evaluating now!
2023-10-04 19:54:56,338 - INFO - Epoch [1023/4000] train_loss: 4.5114, lr: 0.000500, 0.04s
2023-10-04 19:54:56,377 - INFO - epoch complete!
2023-10-04 19:54:56,377 - INFO - evaluating now!
2023-10-04 19:54:56,377 - INFO - Epoch [1024/4000] train_loss: 4.9295, lr: 0.000500, 0.04s
2023-10-04 19:54:56,415 - INFO - epoch complete!
2023-10-04 19:54:56,416 - INFO - evaluating now!
2023-10-04 19:54:56,416 - INFO - Epoch [1025/4000] train_loss: 4.5969, lr: 0.000500, 0.04s
2023-10-04 19:54:56,454 - INFO - epoch complete!
2023-10-04 19:54:56,454 - INFO - evaluating now!
2023-10-04 19:54:56,455 - INFO - Epoch [1026/4000] train_loss: 4.4703, lr: 0.000500, 0.04s
2023-10-04 19:54:56,493 - INFO - epoch complete!
2023-10-04 19:54:56,493 - INFO - evaluating now!
2023-10-04 19:54:56,494 - INFO - Epoch [1027/4000] train_loss: 4.9482, lr: 0.000500, 0.04s
2023-10-04 19:54:56,532 - INFO - epoch complete!
2023-10-04 19:54:56,532 - INFO - evaluating now!
2023-10-04 19:54:56,532 - INFO - Epoch [1028/4000] train_loss: 4.8360, lr: 0.000500, 0.04s
2023-10-04 19:54:56,571 - INFO - epoch complete!
2023-10-04 19:54:56,571 - INFO - evaluating now!
2023-10-04 19:54:56,571 - INFO - Epoch [1029/4000] train_loss: 4.6432, lr: 0.000500, 0.04s
2023-10-04 19:54:56,610 - INFO - epoch complete!
2023-10-04 19:54:56,610 - INFO - evaluating now!
2023-10-04 19:54:56,610 - INFO - Epoch [1030/4000] train_loss: 4.8884, lr: 0.000500, 0.04s
2023-10-04 19:54:56,649 - INFO - epoch complete!
2023-10-04 19:54:56,649 - INFO - evaluating now!
2023-10-04 19:54:56,649 - INFO - Epoch [1031/4000] train_loss: 4.5611, lr: 0.000500, 0.04s
2023-10-04 19:54:56,688 - INFO - epoch complete!
2023-10-04 19:54:56,688 - INFO - evaluating now!
2023-10-04 19:54:56,688 - INFO - Epoch [1032/4000] train_loss: 4.6985, lr: 0.000500, 0.04s
2023-10-04 19:54:56,727 - INFO - epoch complete!
2023-10-04 19:54:56,727 - INFO - evaluating now!
2023-10-04 19:54:56,727 - INFO - Epoch [1033/4000] train_loss: 4.6189, lr: 0.000500, 0.04s
2023-10-04 19:54:56,765 - INFO - epoch complete!
2023-10-04 19:54:56,766 - INFO - evaluating now!
2023-10-04 19:54:56,766 - INFO - Epoch [1034/4000] train_loss: 4.4089, lr: 0.000500, 0.04s
2023-10-04 19:54:56,996 - INFO - Saved model at 1034
2023-10-04 19:54:56,996 - INFO - Val loss decrease from 4.4555 to 4.4089, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1034.tar
2023-10-04 19:54:57,036 - INFO - epoch complete!
2023-10-04 19:54:57,036 - INFO - evaluating now!
2023-10-04 19:54:57,037 - INFO - Epoch [1035/4000] train_loss: 4.7255, lr: 0.000500, 0.04s
2023-10-04 19:54:57,076 - INFO - epoch complete!
2023-10-04 19:54:57,077 - INFO - evaluating now!
2023-10-04 19:54:57,077 - INFO - Epoch [1036/4000] train_loss: 4.6363, lr: 0.000500, 0.04s
2023-10-04 19:54:57,116 - INFO - epoch complete!
2023-10-04 19:54:57,117 - INFO - evaluating now!
2023-10-04 19:54:57,117 - INFO - Epoch [1037/4000] train_loss: 4.5559, lr: 0.000500, 0.04s
2023-10-04 19:54:57,156 - INFO - epoch complete!
2023-10-04 19:54:57,157 - INFO - evaluating now!
2023-10-04 19:54:57,157 - INFO - Epoch [1038/4000] train_loss: 4.8377, lr: 0.000500, 0.04s
2023-10-04 19:54:57,196 - INFO - epoch complete!
2023-10-04 19:54:57,196 - INFO - evaluating now!
2023-10-04 19:54:57,196 - INFO - Epoch [1039/4000] train_loss: 4.9086, lr: 0.000500, 0.04s
2023-10-04 19:54:57,235 - INFO - epoch complete!
2023-10-04 19:54:57,236 - INFO - evaluating now!
2023-10-04 19:54:57,236 - INFO - Epoch [1040/4000] train_loss: 4.6609, lr: 0.000500, 0.04s
2023-10-04 19:54:57,275 - INFO - epoch complete!
2023-10-04 19:54:57,275 - INFO - evaluating now!
2023-10-04 19:54:57,275 - INFO - Epoch [1041/4000] train_loss: 4.5841, lr: 0.000500, 0.04s
2023-10-04 19:54:57,315 - INFO - epoch complete!
2023-10-04 19:54:57,315 - INFO - evaluating now!
2023-10-04 19:54:57,315 - INFO - Epoch [1042/4000] train_loss: 4.8959, lr: 0.000500, 0.04s
2023-10-04 19:54:57,353 - INFO - epoch complete!
2023-10-04 19:54:57,354 - INFO - evaluating now!
2023-10-04 19:54:57,354 - INFO - Epoch [1043/4000] train_loss: 4.4773, lr: 0.000500, 0.04s
2023-10-04 19:54:57,392 - INFO - epoch complete!
2023-10-04 19:54:57,393 - INFO - evaluating now!
2023-10-04 19:54:57,393 - INFO - Epoch [1044/4000] train_loss: 4.6431, lr: 0.000500, 0.04s
2023-10-04 19:54:57,431 - INFO - epoch complete!
2023-10-04 19:54:57,431 - INFO - evaluating now!
2023-10-04 19:54:57,431 - INFO - Epoch [1045/4000] train_loss: 4.7432, lr: 0.000500, 0.04s
2023-10-04 19:54:57,470 - INFO - epoch complete!
2023-10-04 19:54:57,470 - INFO - evaluating now!
2023-10-04 19:54:57,471 - INFO - Epoch [1046/4000] train_loss: 4.6408, lr: 0.000500, 0.04s
2023-10-04 19:54:57,509 - INFO - epoch complete!
2023-10-04 19:54:57,509 - INFO - evaluating now!
2023-10-04 19:54:57,509 - INFO - Epoch [1047/4000] train_loss: 4.4953, lr: 0.000500, 0.04s
2023-10-04 19:54:57,548 - INFO - epoch complete!
2023-10-04 19:54:57,548 - INFO - evaluating now!
2023-10-04 19:54:57,548 - INFO - Epoch [1048/4000] train_loss: 4.5623, lr: 0.000500, 0.04s
2023-10-04 19:54:57,587 - INFO - epoch complete!
2023-10-04 19:54:57,587 - INFO - evaluating now!
2023-10-04 19:54:57,587 - INFO - Epoch [1049/4000] train_loss: 4.6800, lr: 0.000500, 0.04s
2023-10-04 19:54:57,626 - INFO - epoch complete!
2023-10-04 19:54:57,626 - INFO - evaluating now!
2023-10-04 19:54:57,626 - INFO - Epoch [1050/4000] train_loss: 4.3616, lr: 0.000500, 0.04s
2023-10-04 19:54:57,856 - INFO - Saved model at 1050
2023-10-04 19:54:57,857 - INFO - Val loss decrease from 4.4089 to 4.3616, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1050.tar
2023-10-04 19:54:57,897 - INFO - epoch complete!
2023-10-04 19:54:57,898 - INFO - evaluating now!
2023-10-04 19:54:57,898 - INFO - Epoch [1051/4000] train_loss: 4.8145, lr: 0.000500, 0.04s
2023-10-04 19:54:57,938 - INFO - epoch complete!
2023-10-04 19:54:57,938 - INFO - evaluating now!
2023-10-04 19:54:57,938 - INFO - Epoch [1052/4000] train_loss: 4.6267, lr: 0.000500, 0.04s
2023-10-04 19:54:57,978 - INFO - epoch complete!
2023-10-04 19:54:57,978 - INFO - evaluating now!
2023-10-04 19:54:57,978 - INFO - Epoch [1053/4000] train_loss: 4.9540, lr: 0.000500, 0.04s
2023-10-04 19:54:58,018 - INFO - epoch complete!
2023-10-04 19:54:58,018 - INFO - evaluating now!
2023-10-04 19:54:58,018 - INFO - Epoch [1054/4000] train_loss: 4.6311, lr: 0.000500, 0.04s
2023-10-04 19:54:58,060 - INFO - epoch complete!
2023-10-04 19:54:58,060 - INFO - evaluating now!
2023-10-04 19:54:58,061 - INFO - Epoch [1055/4000] train_loss: 4.9292, lr: 0.000500, 0.04s
2023-10-04 19:54:58,101 - INFO - epoch complete!
2023-10-04 19:54:58,101 - INFO - evaluating now!
2023-10-04 19:54:58,102 - INFO - Epoch [1056/4000] train_loss: 4.3869, lr: 0.000500, 0.04s
2023-10-04 19:54:58,141 - INFO - epoch complete!
2023-10-04 19:54:58,141 - INFO - evaluating now!
2023-10-04 19:54:58,141 - INFO - Epoch [1057/4000] train_loss: 4.5903, lr: 0.000500, 0.04s
2023-10-04 19:54:58,181 - INFO - epoch complete!
2023-10-04 19:54:58,181 - INFO - evaluating now!
2023-10-04 19:54:58,181 - INFO - Epoch [1058/4000] train_loss: 4.5388, lr: 0.000500, 0.04s
2023-10-04 19:54:58,221 - INFO - epoch complete!
2023-10-04 19:54:58,221 - INFO - evaluating now!
2023-10-04 19:54:58,221 - INFO - Epoch [1059/4000] train_loss: 4.5397, lr: 0.000500, 0.04s
2023-10-04 19:54:58,259 - INFO - epoch complete!
2023-10-04 19:54:58,260 - INFO - evaluating now!
2023-10-04 19:54:58,260 - INFO - Epoch [1060/4000] train_loss: 4.5644, lr: 0.000500, 0.04s
2023-10-04 19:54:58,298 - INFO - epoch complete!
2023-10-04 19:54:58,299 - INFO - evaluating now!
2023-10-04 19:54:58,299 - INFO - Epoch [1061/4000] train_loss: 4.5044, lr: 0.000500, 0.04s
2023-10-04 19:54:58,337 - INFO - epoch complete!
2023-10-04 19:54:58,337 - INFO - evaluating now!
2023-10-04 19:54:58,338 - INFO - Epoch [1062/4000] train_loss: 4.6541, lr: 0.000500, 0.04s
2023-10-04 19:54:58,376 - INFO - epoch complete!
2023-10-04 19:54:58,376 - INFO - evaluating now!
2023-10-04 19:54:58,376 - INFO - Epoch [1063/4000] train_loss: 4.7176, lr: 0.000500, 0.04s
2023-10-04 19:54:58,415 - INFO - epoch complete!
2023-10-04 19:54:58,415 - INFO - evaluating now!
2023-10-04 19:54:58,415 - INFO - Epoch [1064/4000] train_loss: 4.4519, lr: 0.000500, 0.04s
2023-10-04 19:54:58,454 - INFO - epoch complete!
2023-10-04 19:54:58,454 - INFO - evaluating now!
2023-10-04 19:54:58,454 - INFO - Epoch [1065/4000] train_loss: 4.6259, lr: 0.000500, 0.04s
2023-10-04 19:54:58,493 - INFO - epoch complete!
2023-10-04 19:54:58,493 - INFO - evaluating now!
2023-10-04 19:54:58,493 - INFO - Epoch [1066/4000] train_loss: 4.7884, lr: 0.000500, 0.04s
2023-10-04 19:54:58,532 - INFO - epoch complete!
2023-10-04 19:54:58,532 - INFO - evaluating now!
2023-10-04 19:54:58,532 - INFO - Epoch [1067/4000] train_loss: 4.4972, lr: 0.000500, 0.04s
2023-10-04 19:54:58,571 - INFO - epoch complete!
2023-10-04 19:54:58,571 - INFO - evaluating now!
2023-10-04 19:54:58,571 - INFO - Epoch [1068/4000] train_loss: 4.4219, lr: 0.000500, 0.04s
2023-10-04 19:54:58,610 - INFO - epoch complete!
2023-10-04 19:54:58,610 - INFO - evaluating now!
2023-10-04 19:54:58,610 - INFO - Epoch [1069/4000] train_loss: 4.3432, lr: 0.000500, 0.04s
2023-10-04 19:54:58,841 - INFO - Saved model at 1069
2023-10-04 19:54:58,841 - INFO - Val loss decrease from 4.3616 to 4.3432, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1069.tar
2023-10-04 19:54:58,881 - INFO - epoch complete!
2023-10-04 19:54:58,881 - INFO - evaluating now!
2023-10-04 19:54:58,882 - INFO - Epoch [1070/4000] train_loss: 4.4233, lr: 0.000500, 0.04s
2023-10-04 19:54:58,921 - INFO - epoch complete!
2023-10-04 19:54:58,921 - INFO - evaluating now!
2023-10-04 19:54:58,921 - INFO - Epoch [1071/4000] train_loss: 4.9194, lr: 0.000500, 0.04s
2023-10-04 19:54:58,961 - INFO - epoch complete!
2023-10-04 19:54:58,961 - INFO - evaluating now!
2023-10-04 19:54:58,961 - INFO - Epoch [1072/4000] train_loss: 4.9632, lr: 0.000500, 0.04s
2023-10-04 19:54:59,000 - INFO - epoch complete!
2023-10-04 19:54:59,001 - INFO - evaluating now!
2023-10-04 19:54:59,001 - INFO - Epoch [1073/4000] train_loss: 4.4927, lr: 0.000500, 0.04s
2023-10-04 19:54:59,040 - INFO - epoch complete!
2023-10-04 19:54:59,040 - INFO - evaluating now!
2023-10-04 19:54:59,040 - INFO - Epoch [1074/4000] train_loss: 4.5013, lr: 0.000500, 0.04s
2023-10-04 19:54:59,079 - INFO - epoch complete!
2023-10-04 19:54:59,079 - INFO - evaluating now!
2023-10-04 19:54:59,079 - INFO - Epoch [1075/4000] train_loss: 4.6587, lr: 0.000500, 0.04s
2023-10-04 19:54:59,117 - INFO - epoch complete!
2023-10-04 19:54:59,118 - INFO - evaluating now!
2023-10-04 19:54:59,118 - INFO - Epoch [1076/4000] train_loss: 4.4527, lr: 0.000500, 0.04s
2023-10-04 19:54:59,156 - INFO - epoch complete!
2023-10-04 19:54:59,157 - INFO - evaluating now!
2023-10-04 19:54:59,157 - INFO - Epoch [1077/4000] train_loss: 4.3620, lr: 0.000500, 0.04s
2023-10-04 19:54:59,195 - INFO - epoch complete!
2023-10-04 19:54:59,196 - INFO - evaluating now!
2023-10-04 19:54:59,196 - INFO - Epoch [1078/4000] train_loss: 4.5556, lr: 0.000500, 0.04s
2023-10-04 19:54:59,234 - INFO - epoch complete!
2023-10-04 19:54:59,234 - INFO - evaluating now!
2023-10-04 19:54:59,234 - INFO - Epoch [1079/4000] train_loss: 4.2218, lr: 0.000500, 0.04s
2023-10-04 19:54:59,465 - INFO - Saved model at 1079
2023-10-04 19:54:59,465 - INFO - Val loss decrease from 4.3432 to 4.2218, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1079.tar
2023-10-04 19:54:59,504 - INFO - epoch complete!
2023-10-04 19:54:59,505 - INFO - evaluating now!
2023-10-04 19:54:59,505 - INFO - Epoch [1080/4000] train_loss: 4.7262, lr: 0.000500, 0.04s
2023-10-04 19:54:59,544 - INFO - epoch complete!
2023-10-04 19:54:59,545 - INFO - evaluating now!
2023-10-04 19:54:59,545 - INFO - Epoch [1081/4000] train_loss: 4.1742, lr: 0.000500, 0.04s
2023-10-04 19:54:59,776 - INFO - Saved model at 1081
2023-10-04 19:54:59,776 - INFO - Val loss decrease from 4.2218 to 4.1742, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1081.tar
2023-10-04 19:54:59,816 - INFO - epoch complete!
2023-10-04 19:54:59,817 - INFO - evaluating now!
2023-10-04 19:54:59,817 - INFO - Epoch [1082/4000] train_loss: 4.5853, lr: 0.000500, 0.04s
2023-10-04 19:54:59,856 - INFO - epoch complete!
2023-10-04 19:54:59,857 - INFO - evaluating now!
2023-10-04 19:54:59,857 - INFO - Epoch [1083/4000] train_loss: 4.4136, lr: 0.000500, 0.04s
2023-10-04 19:54:59,896 - INFO - epoch complete!
2023-10-04 19:54:59,896 - INFO - evaluating now!
2023-10-04 19:54:59,896 - INFO - Epoch [1084/4000] train_loss: 4.6871, lr: 0.000500, 0.04s
2023-10-04 19:54:59,935 - INFO - epoch complete!
2023-10-04 19:54:59,936 - INFO - evaluating now!
2023-10-04 19:54:59,936 - INFO - Epoch [1085/4000] train_loss: 4.8345, lr: 0.000500, 0.04s
2023-10-04 19:54:59,974 - INFO - epoch complete!
2023-10-04 19:54:59,975 - INFO - evaluating now!
2023-10-04 19:54:59,975 - INFO - Epoch [1086/4000] train_loss: 4.5322, lr: 0.000500, 0.04s
2023-10-04 19:55:00,013 - INFO - epoch complete!
2023-10-04 19:55:00,014 - INFO - evaluating now!
2023-10-04 19:55:00,014 - INFO - Epoch [1087/4000] train_loss: 4.3413, lr: 0.000500, 0.04s
2023-10-04 19:55:00,054 - INFO - epoch complete!
2023-10-04 19:55:00,054 - INFO - evaluating now!
2023-10-04 19:55:00,054 - INFO - Epoch [1088/4000] train_loss: 4.4714, lr: 0.000500, 0.04s
2023-10-04 19:55:00,095 - INFO - epoch complete!
2023-10-04 19:55:00,095 - INFO - evaluating now!
2023-10-04 19:55:00,095 - INFO - Epoch [1089/4000] train_loss: 4.3871, lr: 0.000500, 0.04s
2023-10-04 19:55:00,135 - INFO - epoch complete!
2023-10-04 19:55:00,135 - INFO - evaluating now!
2023-10-04 19:55:00,135 - INFO - Epoch [1090/4000] train_loss: 4.3027, lr: 0.000500, 0.04s
2023-10-04 19:55:00,174 - INFO - epoch complete!
2023-10-04 19:55:00,175 - INFO - evaluating now!
2023-10-04 19:55:00,175 - INFO - Epoch [1091/4000] train_loss: 4.5539, lr: 0.000500, 0.04s
2023-10-04 19:55:00,214 - INFO - epoch complete!
2023-10-04 19:55:00,214 - INFO - evaluating now!
2023-10-04 19:55:00,214 - INFO - Epoch [1092/4000] train_loss: 4.7774, lr: 0.000500, 0.04s
2023-10-04 19:55:00,253 - INFO - epoch complete!
2023-10-04 19:55:00,254 - INFO - evaluating now!
2023-10-04 19:55:00,254 - INFO - Epoch [1093/4000] train_loss: 4.3556, lr: 0.000500, 0.04s
2023-10-04 19:55:00,293 - INFO - epoch complete!
2023-10-04 19:55:00,293 - INFO - evaluating now!
2023-10-04 19:55:00,293 - INFO - Epoch [1094/4000] train_loss: 4.3268, lr: 0.000500, 0.04s
2023-10-04 19:55:00,333 - INFO - epoch complete!
2023-10-04 19:55:00,333 - INFO - evaluating now!
2023-10-04 19:55:00,333 - INFO - Epoch [1095/4000] train_loss: 4.4082, lr: 0.000500, 0.04s
2023-10-04 19:55:00,372 - INFO - epoch complete!
2023-10-04 19:55:00,373 - INFO - evaluating now!
2023-10-04 19:55:00,373 - INFO - Epoch [1096/4000] train_loss: 4.4217, lr: 0.000500, 0.04s
2023-10-04 19:55:00,412 - INFO - epoch complete!
2023-10-04 19:55:00,412 - INFO - evaluating now!
2023-10-04 19:55:00,412 - INFO - Epoch [1097/4000] train_loss: 4.4892, lr: 0.000500, 0.04s
2023-10-04 19:55:00,452 - INFO - epoch complete!
2023-10-04 19:55:00,452 - INFO - evaluating now!
2023-10-04 19:55:00,452 - INFO - Epoch [1098/4000] train_loss: 4.5466, lr: 0.000500, 0.04s
2023-10-04 19:55:00,492 - INFO - epoch complete!
2023-10-04 19:55:00,492 - INFO - evaluating now!
2023-10-04 19:55:00,492 - INFO - Epoch [1099/4000] train_loss: 4.5750, lr: 0.000500, 0.04s
2023-10-04 19:55:00,531 - INFO - epoch complete!
2023-10-04 19:55:00,531 - INFO - evaluating now!
2023-10-04 19:55:00,532 - INFO - Epoch [1100/4000] train_loss: 4.6415, lr: 0.000500, 0.04s
2023-10-04 19:55:00,571 - INFO - epoch complete!
2023-10-04 19:55:00,571 - INFO - evaluating now!
2023-10-04 19:55:00,571 - INFO - Epoch [1101/4000] train_loss: 4.4771, lr: 0.000500, 0.04s
2023-10-04 19:55:00,610 - INFO - epoch complete!
2023-10-04 19:55:00,611 - INFO - evaluating now!
2023-10-04 19:55:00,611 - INFO - Epoch [1102/4000] train_loss: 4.6161, lr: 0.000500, 0.04s
2023-10-04 19:55:00,650 - INFO - epoch complete!
2023-10-04 19:55:00,650 - INFO - evaluating now!
2023-10-04 19:55:00,650 - INFO - Epoch [1103/4000] train_loss: 4.4859, lr: 0.000500, 0.04s
2023-10-04 19:55:00,690 - INFO - epoch complete!
2023-10-04 19:55:00,690 - INFO - evaluating now!
2023-10-04 19:55:00,690 - INFO - Epoch [1104/4000] train_loss: 4.4285, lr: 0.000500, 0.04s
2023-10-04 19:55:00,729 - INFO - epoch complete!
2023-10-04 19:55:00,730 - INFO - evaluating now!
2023-10-04 19:55:00,730 - INFO - Epoch [1105/4000] train_loss: 4.5001, lr: 0.000500, 0.04s
2023-10-04 19:55:00,769 - INFO - epoch complete!
2023-10-04 19:55:00,769 - INFO - evaluating now!
2023-10-04 19:55:00,769 - INFO - Epoch [1106/4000] train_loss: 4.3423, lr: 0.000500, 0.04s
2023-10-04 19:55:00,809 - INFO - epoch complete!
2023-10-04 19:55:00,809 - INFO - evaluating now!
2023-10-04 19:55:00,809 - INFO - Epoch [1107/4000] train_loss: 4.5802, lr: 0.000500, 0.04s
2023-10-04 19:55:00,848 - INFO - epoch complete!
2023-10-04 19:55:00,849 - INFO - evaluating now!
2023-10-04 19:55:00,849 - INFO - Epoch [1108/4000] train_loss: 4.6102, lr: 0.000500, 0.04s
2023-10-04 19:55:00,888 - INFO - epoch complete!
2023-10-04 19:55:00,888 - INFO - evaluating now!
2023-10-04 19:55:00,888 - INFO - Epoch [1109/4000] train_loss: 4.2519, lr: 0.000500, 0.04s
2023-10-04 19:55:00,927 - INFO - epoch complete!
2023-10-04 19:55:00,928 - INFO - evaluating now!
2023-10-04 19:55:00,928 - INFO - Epoch [1110/4000] train_loss: 4.5540, lr: 0.000500, 0.04s
2023-10-04 19:55:00,967 - INFO - epoch complete!
2023-10-04 19:55:00,967 - INFO - evaluating now!
2023-10-04 19:55:00,968 - INFO - Epoch [1111/4000] train_loss: 4.7170, lr: 0.000500, 0.04s
2023-10-04 19:55:01,007 - INFO - epoch complete!
2023-10-04 19:55:01,007 - INFO - evaluating now!
2023-10-04 19:55:01,007 - INFO - Epoch [1112/4000] train_loss: 4.3762, lr: 0.000500, 0.04s
2023-10-04 19:55:01,047 - INFO - epoch complete!
2023-10-04 19:55:01,047 - INFO - evaluating now!
2023-10-04 19:55:01,047 - INFO - Epoch [1113/4000] train_loss: 4.5415, lr: 0.000500, 0.04s
2023-10-04 19:55:01,087 - INFO - epoch complete!
2023-10-04 19:55:01,087 - INFO - evaluating now!
2023-10-04 19:55:01,087 - INFO - Epoch [1114/4000] train_loss: 4.6081, lr: 0.000500, 0.04s
2023-10-04 19:55:01,126 - INFO - epoch complete!
2023-10-04 19:55:01,127 - INFO - evaluating now!
2023-10-04 19:55:01,127 - INFO - Epoch [1115/4000] train_loss: 4.8108, lr: 0.000500, 0.04s
2023-10-04 19:55:01,166 - INFO - epoch complete!
2023-10-04 19:55:01,166 - INFO - evaluating now!
2023-10-04 19:55:01,166 - INFO - Epoch [1116/4000] train_loss: 4.5513, lr: 0.000500, 0.04s
2023-10-04 19:55:01,206 - INFO - epoch complete!
2023-10-04 19:55:01,206 - INFO - evaluating now!
2023-10-04 19:55:01,206 - INFO - Epoch [1117/4000] train_loss: 4.3507, lr: 0.000500, 0.04s
2023-10-04 19:55:01,246 - INFO - epoch complete!
2023-10-04 19:55:01,246 - INFO - evaluating now!
2023-10-04 19:55:01,246 - INFO - Epoch [1118/4000] train_loss: 4.3284, lr: 0.000500, 0.04s
2023-10-04 19:55:01,286 - INFO - epoch complete!
2023-10-04 19:55:01,286 - INFO - evaluating now!
2023-10-04 19:55:01,286 - INFO - Epoch [1119/4000] train_loss: 4.6319, lr: 0.000500, 0.04s
2023-10-04 19:55:01,328 - INFO - epoch complete!
2023-10-04 19:55:01,328 - INFO - evaluating now!
2023-10-04 19:55:01,328 - INFO - Epoch [1120/4000] train_loss: 4.7486, lr: 0.000500, 0.04s
2023-10-04 19:55:01,368 - INFO - epoch complete!
2023-10-04 19:55:01,368 - INFO - evaluating now!
2023-10-04 19:55:01,369 - INFO - Epoch [1121/4000] train_loss: 4.6679, lr: 0.000500, 0.04s
2023-10-04 19:55:01,408 - INFO - epoch complete!
2023-10-04 19:55:01,408 - INFO - evaluating now!
2023-10-04 19:55:01,408 - INFO - Epoch [1122/4000] train_loss: 4.6211, lr: 0.000500, 0.04s
2023-10-04 19:55:01,448 - INFO - epoch complete!
2023-10-04 19:55:01,448 - INFO - evaluating now!
2023-10-04 19:55:01,448 - INFO - Epoch [1123/4000] train_loss: 4.4080, lr: 0.000500, 0.04s
2023-10-04 19:55:01,488 - INFO - epoch complete!
2023-10-04 19:55:01,488 - INFO - evaluating now!
2023-10-04 19:55:01,488 - INFO - Epoch [1124/4000] train_loss: 4.5065, lr: 0.000500, 0.04s
2023-10-04 19:55:01,528 - INFO - epoch complete!
2023-10-04 19:55:01,528 - INFO - evaluating now!
2023-10-04 19:55:01,528 - INFO - Epoch [1125/4000] train_loss: 4.4270, lr: 0.000500, 0.04s
2023-10-04 19:55:01,568 - INFO - epoch complete!
2023-10-04 19:55:01,568 - INFO - evaluating now!
2023-10-04 19:55:01,568 - INFO - Epoch [1126/4000] train_loss: 4.4870, lr: 0.000500, 0.04s
2023-10-04 19:55:01,608 - INFO - epoch complete!
2023-10-04 19:55:01,608 - INFO - evaluating now!
2023-10-04 19:55:01,608 - INFO - Epoch [1127/4000] train_loss: 4.7517, lr: 0.000500, 0.04s
2023-10-04 19:55:01,648 - INFO - epoch complete!
2023-10-04 19:55:01,648 - INFO - evaluating now!
2023-10-04 19:55:01,648 - INFO - Epoch [1128/4000] train_loss: 4.4793, lr: 0.000500, 0.04s
2023-10-04 19:55:01,687 - INFO - epoch complete!
2023-10-04 19:55:01,687 - INFO - evaluating now!
2023-10-04 19:55:01,688 - INFO - Epoch [1129/4000] train_loss: 4.8576, lr: 0.000500, 0.04s
2023-10-04 19:55:01,727 - INFO - epoch complete!
2023-10-04 19:55:01,727 - INFO - evaluating now!
2023-10-04 19:55:01,727 - INFO - Epoch [1130/4000] train_loss: 4.5004, lr: 0.000500, 0.04s
2023-10-04 19:55:01,767 - INFO - epoch complete!
2023-10-04 19:55:01,767 - INFO - evaluating now!
2023-10-04 19:55:01,767 - INFO - Epoch [1131/4000] train_loss: 4.5786, lr: 0.000500, 0.04s
2023-10-04 19:55:01,806 - INFO - epoch complete!
2023-10-04 19:55:01,807 - INFO - evaluating now!
2023-10-04 19:55:01,807 - INFO - Epoch [1132/4000] train_loss: 4.3476, lr: 0.000500, 0.04s
2023-10-04 19:55:01,846 - INFO - epoch complete!
2023-10-04 19:55:01,847 - INFO - evaluating now!
2023-10-04 19:55:01,847 - INFO - Epoch [1133/4000] train_loss: 4.4473, lr: 0.000500, 0.04s
2023-10-04 19:55:01,886 - INFO - epoch complete!
2023-10-04 19:55:01,887 - INFO - evaluating now!
2023-10-04 19:55:01,887 - INFO - Epoch [1134/4000] train_loss: 4.3894, lr: 0.000500, 0.04s
2023-10-04 19:55:01,926 - INFO - epoch complete!
2023-10-04 19:55:01,927 - INFO - evaluating now!
2023-10-04 19:55:01,927 - INFO - Epoch [1135/4000] train_loss: 4.6103, lr: 0.000500, 0.04s
2023-10-04 19:55:01,966 - INFO - epoch complete!
2023-10-04 19:55:01,966 - INFO - evaluating now!
2023-10-04 19:55:01,966 - INFO - Epoch [1136/4000] train_loss: 4.2512, lr: 0.000500, 0.04s
2023-10-04 19:55:02,006 - INFO - epoch complete!
2023-10-04 19:55:02,006 - INFO - evaluating now!
2023-10-04 19:55:02,006 - INFO - Epoch [1137/4000] train_loss: 4.4800, lr: 0.000500, 0.04s
2023-10-04 19:55:02,046 - INFO - epoch complete!
2023-10-04 19:55:02,046 - INFO - evaluating now!
2023-10-04 19:55:02,046 - INFO - Epoch [1138/4000] train_loss: 4.2979, lr: 0.000500, 0.04s
2023-10-04 19:55:02,086 - INFO - epoch complete!
2023-10-04 19:55:02,086 - INFO - evaluating now!
2023-10-04 19:55:02,086 - INFO - Epoch [1139/4000] train_loss: 4.3980, lr: 0.000500, 0.04s
2023-10-04 19:55:02,126 - INFO - epoch complete!
2023-10-04 19:55:02,126 - INFO - evaluating now!
2023-10-04 19:55:02,126 - INFO - Epoch [1140/4000] train_loss: 4.3075, lr: 0.000500, 0.04s
2023-10-04 19:55:02,165 - INFO - epoch complete!
2023-10-04 19:55:02,165 - INFO - evaluating now!
2023-10-04 19:55:02,166 - INFO - Epoch [1141/4000] train_loss: 4.1563, lr: 0.000500, 0.04s
2023-10-04 19:55:02,395 - INFO - Saved model at 1141
2023-10-04 19:55:02,395 - INFO - Val loss decrease from 4.1742 to 4.1563, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1141.tar
2023-10-04 19:55:02,435 - INFO - epoch complete!
2023-10-04 19:55:02,436 - INFO - evaluating now!
2023-10-04 19:55:02,436 - INFO - Epoch [1142/4000] train_loss: 4.6227, lr: 0.000500, 0.04s
2023-10-04 19:55:02,475 - INFO - epoch complete!
2023-10-04 19:55:02,476 - INFO - evaluating now!
2023-10-04 19:55:02,476 - INFO - Epoch [1143/4000] train_loss: 4.5775, lr: 0.000500, 0.04s
2023-10-04 19:55:02,514 - INFO - epoch complete!
2023-10-04 19:55:02,515 - INFO - evaluating now!
2023-10-04 19:55:02,515 - INFO - Epoch [1144/4000] train_loss: 4.3284, lr: 0.000500, 0.04s
2023-10-04 19:55:02,553 - INFO - epoch complete!
2023-10-04 19:55:02,554 - INFO - evaluating now!
2023-10-04 19:55:02,554 - INFO - Epoch [1145/4000] train_loss: 4.2825, lr: 0.000500, 0.04s
2023-10-04 19:55:02,592 - INFO - epoch complete!
2023-10-04 19:55:02,593 - INFO - evaluating now!
2023-10-04 19:55:02,593 - INFO - Epoch [1146/4000] train_loss: 4.3048, lr: 0.000500, 0.04s
2023-10-04 19:55:02,631 - INFO - epoch complete!
2023-10-04 19:55:02,631 - INFO - evaluating now!
2023-10-04 19:55:02,632 - INFO - Epoch [1147/4000] train_loss: 4.2670, lr: 0.000500, 0.04s
2023-10-04 19:55:02,670 - INFO - epoch complete!
2023-10-04 19:55:02,670 - INFO - evaluating now!
2023-10-04 19:55:02,671 - INFO - Epoch [1148/4000] train_loss: 4.6674, lr: 0.000500, 0.04s
2023-10-04 19:55:02,709 - INFO - epoch complete!
2023-10-04 19:55:02,709 - INFO - evaluating now!
2023-10-04 19:55:02,710 - INFO - Epoch [1149/4000] train_loss: 4.6265, lr: 0.000500, 0.04s
2023-10-04 19:55:02,748 - INFO - epoch complete!
2023-10-04 19:55:02,749 - INFO - evaluating now!
2023-10-04 19:55:02,749 - INFO - Epoch [1150/4000] train_loss: 4.3893, lr: 0.000500, 0.04s
2023-10-04 19:55:02,787 - INFO - epoch complete!
2023-10-04 19:55:02,788 - INFO - evaluating now!
2023-10-04 19:55:02,788 - INFO - Epoch [1151/4000] train_loss: 4.5316, lr: 0.000500, 0.04s
2023-10-04 19:55:02,827 - INFO - epoch complete!
2023-10-04 19:55:02,827 - INFO - evaluating now!
2023-10-04 19:55:02,827 - INFO - Epoch [1152/4000] train_loss: 4.4768, lr: 0.000500, 0.04s
2023-10-04 19:55:02,866 - INFO - epoch complete!
2023-10-04 19:55:02,866 - INFO - evaluating now!
2023-10-04 19:55:02,866 - INFO - Epoch [1153/4000] train_loss: 4.5791, lr: 0.000500, 0.04s
2023-10-04 19:55:02,905 - INFO - epoch complete!
2023-10-04 19:55:02,906 - INFO - evaluating now!
2023-10-04 19:55:02,906 - INFO - Epoch [1154/4000] train_loss: 4.6312, lr: 0.000500, 0.04s
2023-10-04 19:55:02,944 - INFO - epoch complete!
2023-10-04 19:55:02,944 - INFO - evaluating now!
2023-10-04 19:55:02,945 - INFO - Epoch [1155/4000] train_loss: 4.4779, lr: 0.000500, 0.04s
2023-10-04 19:55:02,983 - INFO - epoch complete!
2023-10-04 19:55:02,983 - INFO - evaluating now!
2023-10-04 19:55:02,984 - INFO - Epoch [1156/4000] train_loss: 4.2022, lr: 0.000500, 0.04s
2023-10-04 19:55:03,022 - INFO - epoch complete!
2023-10-04 19:55:03,023 - INFO - evaluating now!
2023-10-04 19:55:03,023 - INFO - Epoch [1157/4000] train_loss: 4.3291, lr: 0.000500, 0.04s
2023-10-04 19:55:03,062 - INFO - epoch complete!
2023-10-04 19:55:03,062 - INFO - evaluating now!
2023-10-04 19:55:03,062 - INFO - Epoch [1158/4000] train_loss: 4.3727, lr: 0.000500, 0.04s
2023-10-04 19:55:03,101 - INFO - epoch complete!
2023-10-04 19:55:03,101 - INFO - evaluating now!
2023-10-04 19:55:03,101 - INFO - Epoch [1159/4000] train_loss: 4.3320, lr: 0.000500, 0.04s
2023-10-04 19:55:03,140 - INFO - epoch complete!
2023-10-04 19:55:03,141 - INFO - evaluating now!
2023-10-04 19:55:03,141 - INFO - Epoch [1160/4000] train_loss: 4.1229, lr: 0.000500, 0.04s
2023-10-04 19:55:03,371 - INFO - Saved model at 1160
2023-10-04 19:55:03,371 - INFO - Val loss decrease from 4.1563 to 4.1229, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1160.tar
2023-10-04 19:55:03,411 - INFO - epoch complete!
2023-10-04 19:55:03,411 - INFO - evaluating now!
2023-10-04 19:55:03,411 - INFO - Epoch [1161/4000] train_loss: 4.3127, lr: 0.000500, 0.04s
2023-10-04 19:55:03,451 - INFO - epoch complete!
2023-10-04 19:55:03,451 - INFO - evaluating now!
2023-10-04 19:55:03,451 - INFO - Epoch [1162/4000] train_loss: 4.1637, lr: 0.000500, 0.04s
2023-10-04 19:55:03,490 - INFO - epoch complete!
2023-10-04 19:55:03,490 - INFO - evaluating now!
2023-10-04 19:55:03,491 - INFO - Epoch [1163/4000] train_loss: 4.4280, lr: 0.000500, 0.04s
2023-10-04 19:55:03,529 - INFO - epoch complete!
2023-10-04 19:55:03,529 - INFO - evaluating now!
2023-10-04 19:55:03,529 - INFO - Epoch [1164/4000] train_loss: 4.0541, lr: 0.000500, 0.04s
2023-10-04 19:55:03,761 - INFO - Saved model at 1164
2023-10-04 19:55:03,761 - INFO - Val loss decrease from 4.1229 to 4.0541, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1164.tar
2023-10-04 19:55:03,801 - INFO - epoch complete!
2023-10-04 19:55:03,801 - INFO - evaluating now!
2023-10-04 19:55:03,802 - INFO - Epoch [1165/4000] train_loss: 4.5537, lr: 0.000500, 0.04s
2023-10-04 19:55:03,841 - INFO - epoch complete!
2023-10-04 19:55:03,841 - INFO - evaluating now!
2023-10-04 19:55:03,841 - INFO - Epoch [1166/4000] train_loss: 4.3085, lr: 0.000500, 0.04s
2023-10-04 19:55:03,880 - INFO - epoch complete!
2023-10-04 19:55:03,881 - INFO - evaluating now!
2023-10-04 19:55:03,881 - INFO - Epoch [1167/4000] train_loss: 4.2316, lr: 0.000500, 0.04s
2023-10-04 19:55:03,920 - INFO - epoch complete!
2023-10-04 19:55:03,920 - INFO - evaluating now!
2023-10-04 19:55:03,920 - INFO - Epoch [1168/4000] train_loss: 4.2874, lr: 0.000500, 0.04s
2023-10-04 19:55:03,959 - INFO - epoch complete!
2023-10-04 19:55:03,959 - INFO - evaluating now!
2023-10-04 19:55:03,959 - INFO - Epoch [1169/4000] train_loss: 4.2597, lr: 0.000500, 0.04s
2023-10-04 19:55:03,998 - INFO - epoch complete!
2023-10-04 19:55:03,998 - INFO - evaluating now!
2023-10-04 19:55:03,999 - INFO - Epoch [1170/4000] train_loss: 4.4420, lr: 0.000500, 0.04s
2023-10-04 19:55:04,038 - INFO - epoch complete!
2023-10-04 19:55:04,039 - INFO - evaluating now!
2023-10-04 19:55:04,039 - INFO - Epoch [1171/4000] train_loss: 4.4125, lr: 0.000500, 0.04s
2023-10-04 19:55:04,078 - INFO - epoch complete!
2023-10-04 19:55:04,078 - INFO - evaluating now!
2023-10-04 19:55:04,078 - INFO - Epoch [1172/4000] train_loss: 4.6978, lr: 0.000500, 0.04s
2023-10-04 19:55:04,117 - INFO - epoch complete!
2023-10-04 19:55:04,117 - INFO - evaluating now!
2023-10-04 19:55:04,117 - INFO - Epoch [1173/4000] train_loss: 4.3817, lr: 0.000500, 0.04s
2023-10-04 19:55:04,156 - INFO - epoch complete!
2023-10-04 19:55:04,156 - INFO - evaluating now!
2023-10-04 19:55:04,156 - INFO - Epoch [1174/4000] train_loss: 4.2379, lr: 0.000500, 0.04s
2023-10-04 19:55:04,195 - INFO - epoch complete!
2023-10-04 19:55:04,195 - INFO - evaluating now!
2023-10-04 19:55:04,195 - INFO - Epoch [1175/4000] train_loss: 4.5541, lr: 0.000500, 0.04s
2023-10-04 19:55:04,234 - INFO - epoch complete!
2023-10-04 19:55:04,234 - INFO - evaluating now!
2023-10-04 19:55:04,234 - INFO - Epoch [1176/4000] train_loss: 4.1758, lr: 0.000500, 0.04s
2023-10-04 19:55:04,272 - INFO - epoch complete!
2023-10-04 19:55:04,272 - INFO - evaluating now!
2023-10-04 19:55:04,273 - INFO - Epoch [1177/4000] train_loss: 4.1821, lr: 0.000500, 0.04s
2023-10-04 19:55:04,311 - INFO - epoch complete!
2023-10-04 19:55:04,312 - INFO - evaluating now!
2023-10-04 19:55:04,312 - INFO - Epoch [1178/4000] train_loss: 4.4952, lr: 0.000500, 0.04s
2023-10-04 19:55:04,350 - INFO - epoch complete!
2023-10-04 19:55:04,350 - INFO - evaluating now!
2023-10-04 19:55:04,350 - INFO - Epoch [1179/4000] train_loss: 4.5207, lr: 0.000500, 0.04s
2023-10-04 19:55:04,389 - INFO - epoch complete!
2023-10-04 19:55:04,389 - INFO - evaluating now!
2023-10-04 19:55:04,389 - INFO - Epoch [1180/4000] train_loss: 4.2378, lr: 0.000500, 0.04s
2023-10-04 19:55:04,427 - INFO - epoch complete!
2023-10-04 19:55:04,428 - INFO - evaluating now!
2023-10-04 19:55:04,428 - INFO - Epoch [1181/4000] train_loss: 4.3112, lr: 0.000500, 0.04s
2023-10-04 19:55:04,466 - INFO - epoch complete!
2023-10-04 19:55:04,466 - INFO - evaluating now!
2023-10-04 19:55:04,467 - INFO - Epoch [1182/4000] train_loss: 4.1568, lr: 0.000500, 0.04s
2023-10-04 19:55:04,505 - INFO - epoch complete!
2023-10-04 19:55:04,506 - INFO - evaluating now!
2023-10-04 19:55:04,506 - INFO - Epoch [1183/4000] train_loss: 4.2415, lr: 0.000500, 0.04s
2023-10-04 19:55:04,544 - INFO - epoch complete!
2023-10-04 19:55:04,544 - INFO - evaluating now!
2023-10-04 19:55:04,544 - INFO - Epoch [1184/4000] train_loss: 4.3930, lr: 0.000500, 0.04s
2023-10-04 19:55:04,583 - INFO - epoch complete!
2023-10-04 19:55:04,583 - INFO - evaluating now!
2023-10-04 19:55:04,583 - INFO - Epoch [1185/4000] train_loss: 4.5580, lr: 0.000500, 0.04s
2023-10-04 19:55:04,622 - INFO - epoch complete!
2023-10-04 19:55:04,622 - INFO - evaluating now!
2023-10-04 19:55:04,622 - INFO - Epoch [1186/4000] train_loss: 4.3785, lr: 0.000500, 0.04s
2023-10-04 19:55:04,660 - INFO - epoch complete!
2023-10-04 19:55:04,661 - INFO - evaluating now!
2023-10-04 19:55:04,661 - INFO - Epoch [1187/4000] train_loss: 4.6031, lr: 0.000500, 0.04s
2023-10-04 19:55:04,699 - INFO - epoch complete!
2023-10-04 19:55:04,699 - INFO - evaluating now!
2023-10-04 19:55:04,700 - INFO - Epoch [1188/4000] train_loss: 4.3155, lr: 0.000500, 0.04s
2023-10-04 19:55:04,738 - INFO - epoch complete!
2023-10-04 19:55:04,738 - INFO - evaluating now!
2023-10-04 19:55:04,738 - INFO - Epoch [1189/4000] train_loss: 4.1967, lr: 0.000500, 0.04s
2023-10-04 19:55:04,777 - INFO - epoch complete!
2023-10-04 19:55:04,777 - INFO - evaluating now!
2023-10-04 19:55:04,777 - INFO - Epoch [1190/4000] train_loss: 4.2414, lr: 0.000500, 0.04s
2023-10-04 19:55:04,815 - INFO - epoch complete!
2023-10-04 19:55:04,816 - INFO - evaluating now!
2023-10-04 19:55:04,816 - INFO - Epoch [1191/4000] train_loss: 4.2923, lr: 0.000500, 0.04s
2023-10-04 19:55:04,854 - INFO - epoch complete!
2023-10-04 19:55:04,855 - INFO - evaluating now!
2023-10-04 19:55:04,855 - INFO - Epoch [1192/4000] train_loss: 4.4118, lr: 0.000500, 0.04s
2023-10-04 19:55:04,893 - INFO - epoch complete!
2023-10-04 19:55:04,893 - INFO - evaluating now!
2023-10-04 19:55:04,893 - INFO - Epoch [1193/4000] train_loss: 4.6315, lr: 0.000500, 0.04s
2023-10-04 19:55:04,932 - INFO - epoch complete!
2023-10-04 19:55:04,932 - INFO - evaluating now!
2023-10-04 19:55:04,932 - INFO - Epoch [1194/4000] train_loss: 4.3962, lr: 0.000500, 0.04s
2023-10-04 19:55:04,971 - INFO - epoch complete!
2023-10-04 19:55:04,971 - INFO - evaluating now!
2023-10-04 19:55:04,971 - INFO - Epoch [1195/4000] train_loss: 4.3343, lr: 0.000500, 0.04s
2023-10-04 19:55:05,010 - INFO - epoch complete!
2023-10-04 19:55:05,010 - INFO - evaluating now!
2023-10-04 19:55:05,010 - INFO - Epoch [1196/4000] train_loss: 4.6960, lr: 0.000500, 0.04s
2023-10-04 19:55:05,050 - INFO - epoch complete!
2023-10-04 19:55:05,051 - INFO - evaluating now!
2023-10-04 19:55:05,051 - INFO - Epoch [1197/4000] train_loss: 4.2442, lr: 0.000500, 0.04s
2023-10-04 19:55:05,089 - INFO - epoch complete!
2023-10-04 19:55:05,090 - INFO - evaluating now!
2023-10-04 19:55:05,090 - INFO - Epoch [1198/4000] train_loss: 4.4455, lr: 0.000500, 0.04s
2023-10-04 19:55:05,128 - INFO - epoch complete!
2023-10-04 19:55:05,128 - INFO - evaluating now!
2023-10-04 19:55:05,128 - INFO - Epoch [1199/4000] train_loss: 4.3210, lr: 0.000500, 0.04s
2023-10-04 19:55:05,167 - INFO - epoch complete!
2023-10-04 19:55:05,167 - INFO - evaluating now!
2023-10-04 19:55:05,167 - INFO - Epoch [1200/4000] train_loss: 4.3599, lr: 0.000500, 0.04s
2023-10-04 19:55:05,206 - INFO - epoch complete!
2023-10-04 19:55:05,206 - INFO - evaluating now!
2023-10-04 19:55:05,206 - INFO - Epoch [1201/4000] train_loss: 4.2379, lr: 0.000500, 0.04s
2023-10-04 19:55:05,245 - INFO - epoch complete!
2023-10-04 19:55:05,245 - INFO - evaluating now!
2023-10-04 19:55:05,245 - INFO - Epoch [1202/4000] train_loss: 4.7699, lr: 0.000500, 0.04s
2023-10-04 19:55:05,283 - INFO - epoch complete!
2023-10-04 19:55:05,284 - INFO - evaluating now!
2023-10-04 19:55:05,284 - INFO - Epoch [1203/4000] train_loss: 4.3090, lr: 0.000500, 0.04s
2023-10-04 19:55:05,322 - INFO - epoch complete!
2023-10-04 19:55:05,322 - INFO - evaluating now!
2023-10-04 19:55:05,322 - INFO - Epoch [1204/4000] train_loss: 4.4083, lr: 0.000500, 0.04s
2023-10-04 19:55:05,361 - INFO - epoch complete!
2023-10-04 19:55:05,361 - INFO - evaluating now!
2023-10-04 19:55:05,361 - INFO - Epoch [1205/4000] train_loss: 4.2413, lr: 0.000500, 0.04s
2023-10-04 19:55:05,399 - INFO - epoch complete!
2023-10-04 19:55:05,399 - INFO - evaluating now!
2023-10-04 19:55:05,400 - INFO - Epoch [1206/4000] train_loss: 4.0432, lr: 0.000500, 0.04s
2023-10-04 19:55:05,630 - INFO - Saved model at 1206
2023-10-04 19:55:05,630 - INFO - Val loss decrease from 4.0541 to 4.0432, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1206.tar
2023-10-04 19:55:05,670 - INFO - epoch complete!
2023-10-04 19:55:05,671 - INFO - evaluating now!
2023-10-04 19:55:05,671 - INFO - Epoch [1207/4000] train_loss: 4.7326, lr: 0.000500, 0.04s
2023-10-04 19:55:05,710 - INFO - epoch complete!
2023-10-04 19:55:05,711 - INFO - evaluating now!
2023-10-04 19:55:05,711 - INFO - Epoch [1208/4000] train_loss: 4.2504, lr: 0.000500, 0.04s
2023-10-04 19:55:05,750 - INFO - epoch complete!
2023-10-04 19:55:05,751 - INFO - evaluating now!
2023-10-04 19:55:05,751 - INFO - Epoch [1209/4000] train_loss: 4.7843, lr: 0.000500, 0.04s
2023-10-04 19:55:05,791 - INFO - epoch complete!
2023-10-04 19:55:05,791 - INFO - evaluating now!
2023-10-04 19:55:05,791 - INFO - Epoch [1210/4000] train_loss: 4.3803, lr: 0.000500, 0.04s
2023-10-04 19:55:05,830 - INFO - epoch complete!
2023-10-04 19:55:05,830 - INFO - evaluating now!
2023-10-04 19:55:05,831 - INFO - Epoch [1211/4000] train_loss: 4.2104, lr: 0.000500, 0.04s
2023-10-04 19:55:05,870 - INFO - epoch complete!
2023-10-04 19:55:05,870 - INFO - evaluating now!
2023-10-04 19:55:05,870 - INFO - Epoch [1212/4000] train_loss: 4.2708, lr: 0.000500, 0.04s
2023-10-04 19:55:05,909 - INFO - epoch complete!
2023-10-04 19:55:05,910 - INFO - evaluating now!
2023-10-04 19:55:05,910 - INFO - Epoch [1213/4000] train_loss: 4.4394, lr: 0.000500, 0.04s
2023-10-04 19:55:05,949 - INFO - epoch complete!
2023-10-04 19:55:05,949 - INFO - evaluating now!
2023-10-04 19:55:05,949 - INFO - Epoch [1214/4000] train_loss: 4.2510, lr: 0.000500, 0.04s
2023-10-04 19:55:05,988 - INFO - epoch complete!
2023-10-04 19:55:05,988 - INFO - evaluating now!
2023-10-04 19:55:05,988 - INFO - Epoch [1215/4000] train_loss: 4.3342, lr: 0.000500, 0.04s
2023-10-04 19:55:06,027 - INFO - epoch complete!
2023-10-04 19:55:06,027 - INFO - evaluating now!
2023-10-04 19:55:06,027 - INFO - Epoch [1216/4000] train_loss: 4.0560, lr: 0.000500, 0.04s
2023-10-04 19:55:06,065 - INFO - epoch complete!
2023-10-04 19:55:06,066 - INFO - evaluating now!
2023-10-04 19:55:06,066 - INFO - Epoch [1217/4000] train_loss: 4.1188, lr: 0.000500, 0.04s
2023-10-04 19:55:06,104 - INFO - epoch complete!
2023-10-04 19:55:06,105 - INFO - evaluating now!
2023-10-04 19:55:06,105 - INFO - Epoch [1218/4000] train_loss: 4.2144, lr: 0.000500, 0.04s
2023-10-04 19:55:06,143 - INFO - epoch complete!
2023-10-04 19:55:06,143 - INFO - evaluating now!
2023-10-04 19:55:06,144 - INFO - Epoch [1219/4000] train_loss: 4.0680, lr: 0.000500, 0.04s
2023-10-04 19:55:06,182 - INFO - epoch complete!
2023-10-04 19:55:06,182 - INFO - evaluating now!
2023-10-04 19:55:06,182 - INFO - Epoch [1220/4000] train_loss: 4.3698, lr: 0.000500, 0.04s
2023-10-04 19:55:06,221 - INFO - epoch complete!
2023-10-04 19:55:06,221 - INFO - evaluating now!
2023-10-04 19:55:06,221 - INFO - Epoch [1221/4000] train_loss: 4.2310, lr: 0.000500, 0.04s
2023-10-04 19:55:06,260 - INFO - epoch complete!
2023-10-04 19:55:06,260 - INFO - evaluating now!
2023-10-04 19:55:06,260 - INFO - Epoch [1222/4000] train_loss: 4.1118, lr: 0.000500, 0.04s
2023-10-04 19:55:06,298 - INFO - epoch complete!
2023-10-04 19:55:06,299 - INFO - evaluating now!
2023-10-04 19:55:06,299 - INFO - Epoch [1223/4000] train_loss: 4.2062, lr: 0.000500, 0.04s
2023-10-04 19:55:06,337 - INFO - epoch complete!
2023-10-04 19:55:06,337 - INFO - evaluating now!
2023-10-04 19:55:06,338 - INFO - Epoch [1224/4000] train_loss: 4.4025, lr: 0.000500, 0.04s
2023-10-04 19:55:06,376 - INFO - epoch complete!
2023-10-04 19:55:06,376 - INFO - evaluating now!
2023-10-04 19:55:06,376 - INFO - Epoch [1225/4000] train_loss: 4.0936, lr: 0.000500, 0.04s
2023-10-04 19:55:06,415 - INFO - epoch complete!
2023-10-04 19:55:06,415 - INFO - evaluating now!
2023-10-04 19:55:06,415 - INFO - Epoch [1226/4000] train_loss: 4.3243, lr: 0.000500, 0.04s
2023-10-04 19:55:06,454 - INFO - epoch complete!
2023-10-04 19:55:06,454 - INFO - evaluating now!
2023-10-04 19:55:06,454 - INFO - Epoch [1227/4000] train_loss: 4.1752, lr: 0.000500, 0.04s
2023-10-04 19:55:06,493 - INFO - epoch complete!
2023-10-04 19:55:06,493 - INFO - evaluating now!
2023-10-04 19:55:06,493 - INFO - Epoch [1228/4000] train_loss: 4.6656, lr: 0.000500, 0.04s
2023-10-04 19:55:06,531 - INFO - epoch complete!
2023-10-04 19:55:06,532 - INFO - evaluating now!
2023-10-04 19:55:06,532 - INFO - Epoch [1229/4000] train_loss: 4.3202, lr: 0.000500, 0.04s
2023-10-04 19:55:06,570 - INFO - epoch complete!
2023-10-04 19:55:06,570 - INFO - evaluating now!
2023-10-04 19:55:06,571 - INFO - Epoch [1230/4000] train_loss: 4.1825, lr: 0.000500, 0.04s
2023-10-04 19:55:06,609 - INFO - epoch complete!
2023-10-04 19:55:06,609 - INFO - evaluating now!
2023-10-04 19:55:06,609 - INFO - Epoch [1231/4000] train_loss: 4.4246, lr: 0.000500, 0.04s
2023-10-04 19:55:06,648 - INFO - epoch complete!
2023-10-04 19:55:06,648 - INFO - evaluating now!
2023-10-04 19:55:06,648 - INFO - Epoch [1232/4000] train_loss: 4.1639, lr: 0.000500, 0.04s
2023-10-04 19:55:06,687 - INFO - epoch complete!
2023-10-04 19:55:06,687 - INFO - evaluating now!
2023-10-04 19:55:06,687 - INFO - Epoch [1233/4000] train_loss: 4.1319, lr: 0.000500, 0.04s
2023-10-04 19:55:06,725 - INFO - epoch complete!
2023-10-04 19:55:06,726 - INFO - evaluating now!
2023-10-04 19:55:06,726 - INFO - Epoch [1234/4000] train_loss: 4.1497, lr: 0.000500, 0.04s
2023-10-04 19:55:06,764 - INFO - epoch complete!
2023-10-04 19:55:06,764 - INFO - evaluating now!
2023-10-04 19:55:06,765 - INFO - Epoch [1235/4000] train_loss: 4.0646, lr: 0.000500, 0.04s
2023-10-04 19:55:06,803 - INFO - epoch complete!
2023-10-04 19:55:06,803 - INFO - evaluating now!
2023-10-04 19:55:06,803 - INFO - Epoch [1236/4000] train_loss: 4.4189, lr: 0.000500, 0.04s
2023-10-04 19:55:06,841 - INFO - epoch complete!
2023-10-04 19:55:06,842 - INFO - evaluating now!
2023-10-04 19:55:06,842 - INFO - Epoch [1237/4000] train_loss: 4.1303, lr: 0.000500, 0.04s
2023-10-04 19:55:06,880 - INFO - epoch complete!
2023-10-04 19:55:06,880 - INFO - evaluating now!
2023-10-04 19:55:06,881 - INFO - Epoch [1238/4000] train_loss: 4.4909, lr: 0.000500, 0.04s
2023-10-04 19:55:06,919 - INFO - epoch complete!
2023-10-04 19:55:06,919 - INFO - evaluating now!
2023-10-04 19:55:06,919 - INFO - Epoch [1239/4000] train_loss: 4.4814, lr: 0.000500, 0.04s
2023-10-04 19:55:06,958 - INFO - epoch complete!
2023-10-04 19:55:06,958 - INFO - evaluating now!
2023-10-04 19:55:06,958 - INFO - Epoch [1240/4000] train_loss: 4.0773, lr: 0.000500, 0.04s
2023-10-04 19:55:06,997 - INFO - epoch complete!
2023-10-04 19:55:06,997 - INFO - evaluating now!
2023-10-04 19:55:06,997 - INFO - Epoch [1241/4000] train_loss: 4.3619, lr: 0.000500, 0.04s
2023-10-04 19:55:07,036 - INFO - epoch complete!
2023-10-04 19:55:07,036 - INFO - evaluating now!
2023-10-04 19:55:07,036 - INFO - Epoch [1242/4000] train_loss: 4.0795, lr: 0.000500, 0.04s
2023-10-04 19:55:07,075 - INFO - epoch complete!
2023-10-04 19:55:07,075 - INFO - evaluating now!
2023-10-04 19:55:07,075 - INFO - Epoch [1243/4000] train_loss: 4.2798, lr: 0.000500, 0.04s
2023-10-04 19:55:07,114 - INFO - epoch complete!
2023-10-04 19:55:07,114 - INFO - evaluating now!
2023-10-04 19:55:07,114 - INFO - Epoch [1244/4000] train_loss: 4.2782, lr: 0.000500, 0.04s
2023-10-04 19:55:07,152 - INFO - epoch complete!
2023-10-04 19:55:07,153 - INFO - evaluating now!
2023-10-04 19:55:07,153 - INFO - Epoch [1245/4000] train_loss: 4.1080, lr: 0.000500, 0.04s
2023-10-04 19:55:07,191 - INFO - epoch complete!
2023-10-04 19:55:07,192 - INFO - evaluating now!
2023-10-04 19:55:07,192 - INFO - Epoch [1246/4000] train_loss: 4.3226, lr: 0.000500, 0.04s
2023-10-04 19:55:07,230 - INFO - epoch complete!
2023-10-04 19:55:07,231 - INFO - evaluating now!
2023-10-04 19:55:07,231 - INFO - Epoch [1247/4000] train_loss: 4.4576, lr: 0.000500, 0.04s
2023-10-04 19:55:07,269 - INFO - epoch complete!
2023-10-04 19:55:07,269 - INFO - evaluating now!
2023-10-04 19:55:07,269 - INFO - Epoch [1248/4000] train_loss: 4.0668, lr: 0.000500, 0.04s
2023-10-04 19:55:07,308 - INFO - epoch complete!
2023-10-04 19:55:07,308 - INFO - evaluating now!
2023-10-04 19:55:07,308 - INFO - Epoch [1249/4000] train_loss: 4.0633, lr: 0.000500, 0.04s
2023-10-04 19:55:07,347 - INFO - epoch complete!
2023-10-04 19:55:07,347 - INFO - evaluating now!
2023-10-04 19:55:07,347 - INFO - Epoch [1250/4000] train_loss: 4.2564, lr: 0.000500, 0.04s
2023-10-04 19:55:07,386 - INFO - epoch complete!
2023-10-04 19:55:07,386 - INFO - evaluating now!
2023-10-04 19:55:07,386 - INFO - Epoch [1251/4000] train_loss: 4.2568, lr: 0.000500, 0.04s
2023-10-04 19:55:07,424 - INFO - epoch complete!
2023-10-04 19:55:07,425 - INFO - evaluating now!
2023-10-04 19:55:07,425 - INFO - Epoch [1252/4000] train_loss: 4.3383, lr: 0.000500, 0.04s
2023-10-04 19:55:07,463 - INFO - epoch complete!
2023-10-04 19:55:07,464 - INFO - evaluating now!
2023-10-04 19:55:07,464 - INFO - Epoch [1253/4000] train_loss: 4.3669, lr: 0.000500, 0.04s
2023-10-04 19:55:07,502 - INFO - epoch complete!
2023-10-04 19:55:07,503 - INFO - evaluating now!
2023-10-04 19:55:07,503 - INFO - Epoch [1254/4000] train_loss: 4.3840, lr: 0.000500, 0.04s
2023-10-04 19:55:07,541 - INFO - epoch complete!
2023-10-04 19:55:07,541 - INFO - evaluating now!
2023-10-04 19:55:07,542 - INFO - Epoch [1255/4000] train_loss: 4.3409, lr: 0.000500, 0.04s
2023-10-04 19:55:07,580 - INFO - epoch complete!
2023-10-04 19:55:07,580 - INFO - evaluating now!
2023-10-04 19:55:07,580 - INFO - Epoch [1256/4000] train_loss: 4.1892, lr: 0.000500, 0.04s
2023-10-04 19:55:07,619 - INFO - epoch complete!
2023-10-04 19:55:07,619 - INFO - evaluating now!
2023-10-04 19:55:07,619 - INFO - Epoch [1257/4000] train_loss: 4.2206, lr: 0.000500, 0.04s
2023-10-04 19:55:07,657 - INFO - epoch complete!
2023-10-04 19:55:07,658 - INFO - evaluating now!
2023-10-04 19:55:07,658 - INFO - Epoch [1258/4000] train_loss: 4.3518, lr: 0.000500, 0.04s
2023-10-04 19:55:07,697 - INFO - epoch complete!
2023-10-04 19:55:07,697 - INFO - evaluating now!
2023-10-04 19:55:07,697 - INFO - Epoch [1259/4000] train_loss: 4.2259, lr: 0.000500, 0.04s
2023-10-04 19:55:07,737 - INFO - epoch complete!
2023-10-04 19:55:07,737 - INFO - evaluating now!
2023-10-04 19:55:07,737 - INFO - Epoch [1260/4000] train_loss: 4.3371, lr: 0.000500, 0.04s
2023-10-04 19:55:07,777 - INFO - epoch complete!
2023-10-04 19:55:07,777 - INFO - evaluating now!
2023-10-04 19:55:07,777 - INFO - Epoch [1261/4000] train_loss: 4.2085, lr: 0.000500, 0.04s
2023-10-04 19:55:07,816 - INFO - epoch complete!
2023-10-04 19:55:07,816 - INFO - evaluating now!
2023-10-04 19:55:07,816 - INFO - Epoch [1262/4000] train_loss: 4.2082, lr: 0.000500, 0.04s
2023-10-04 19:55:07,855 - INFO - epoch complete!
2023-10-04 19:55:07,855 - INFO - evaluating now!
2023-10-04 19:55:07,855 - INFO - Epoch [1263/4000] train_loss: 3.9719, lr: 0.000500, 0.04s
2023-10-04 19:55:08,085 - INFO - Saved model at 1263
2023-10-04 19:55:08,085 - INFO - Val loss decrease from 4.0432 to 3.9719, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1263.tar
2023-10-04 19:55:08,126 - INFO - epoch complete!
2023-10-04 19:55:08,126 - INFO - evaluating now!
2023-10-04 19:55:08,127 - INFO - Epoch [1264/4000] train_loss: 3.9775, lr: 0.000500, 0.04s
2023-10-04 19:55:08,166 - INFO - epoch complete!
2023-10-04 19:55:08,166 - INFO - evaluating now!
2023-10-04 19:55:08,166 - INFO - Epoch [1265/4000] train_loss: 4.3545, lr: 0.000500, 0.04s
2023-10-04 19:55:08,206 - INFO - epoch complete!
2023-10-04 19:55:08,206 - INFO - evaluating now!
2023-10-04 19:55:08,206 - INFO - Epoch [1266/4000] train_loss: 4.2015, lr: 0.000500, 0.04s
2023-10-04 19:55:08,245 - INFO - epoch complete!
2023-10-04 19:55:08,245 - INFO - evaluating now!
2023-10-04 19:55:08,245 - INFO - Epoch [1267/4000] train_loss: 4.2320, lr: 0.000500, 0.04s
2023-10-04 19:55:08,285 - INFO - epoch complete!
2023-10-04 19:55:08,285 - INFO - evaluating now!
2023-10-04 19:55:08,285 - INFO - Epoch [1268/4000] train_loss: 4.3687, lr: 0.000500, 0.04s
2023-10-04 19:55:08,323 - INFO - epoch complete!
2023-10-04 19:55:08,324 - INFO - evaluating now!
2023-10-04 19:55:08,324 - INFO - Epoch [1269/4000] train_loss: 4.2097, lr: 0.000500, 0.04s
2023-10-04 19:55:08,362 - INFO - epoch complete!
2023-10-04 19:55:08,363 - INFO - evaluating now!
2023-10-04 19:55:08,363 - INFO - Epoch [1270/4000] train_loss: 4.3720, lr: 0.000500, 0.04s
2023-10-04 19:55:08,401 - INFO - epoch complete!
2023-10-04 19:55:08,401 - INFO - evaluating now!
2023-10-04 19:55:08,402 - INFO - Epoch [1271/4000] train_loss: 4.2869, lr: 0.000500, 0.04s
2023-10-04 19:55:08,440 - INFO - epoch complete!
2023-10-04 19:55:08,440 - INFO - evaluating now!
2023-10-04 19:55:08,440 - INFO - Epoch [1272/4000] train_loss: 4.3284, lr: 0.000500, 0.04s
2023-10-04 19:55:08,479 - INFO - epoch complete!
2023-10-04 19:55:08,479 - INFO - evaluating now!
2023-10-04 19:55:08,479 - INFO - Epoch [1273/4000] train_loss: 4.5475, lr: 0.000500, 0.04s
2023-10-04 19:55:08,518 - INFO - epoch complete!
2023-10-04 19:55:08,518 - INFO - evaluating now!
2023-10-04 19:55:08,518 - INFO - Epoch [1274/4000] train_loss: 4.3788, lr: 0.000500, 0.04s
2023-10-04 19:55:08,557 - INFO - epoch complete!
2023-10-04 19:55:08,557 - INFO - evaluating now!
2023-10-04 19:55:08,557 - INFO - Epoch [1275/4000] train_loss: 4.3088, lr: 0.000500, 0.04s
2023-10-04 19:55:08,596 - INFO - epoch complete!
2023-10-04 19:55:08,596 - INFO - evaluating now!
2023-10-04 19:55:08,596 - INFO - Epoch [1276/4000] train_loss: 4.3956, lr: 0.000500, 0.04s
2023-10-04 19:55:08,634 - INFO - epoch complete!
2023-10-04 19:55:08,635 - INFO - evaluating now!
2023-10-04 19:55:08,635 - INFO - Epoch [1277/4000] train_loss: 4.2404, lr: 0.000500, 0.04s
2023-10-04 19:55:08,673 - INFO - epoch complete!
2023-10-04 19:55:08,673 - INFO - evaluating now!
2023-10-04 19:55:08,674 - INFO - Epoch [1278/4000] train_loss: 4.2443, lr: 0.000500, 0.04s
2023-10-04 19:55:08,712 - INFO - epoch complete!
2023-10-04 19:55:08,712 - INFO - evaluating now!
2023-10-04 19:55:08,713 - INFO - Epoch [1279/4000] train_loss: 4.6346, lr: 0.000500, 0.04s
2023-10-04 19:55:08,751 - INFO - epoch complete!
2023-10-04 19:55:08,751 - INFO - evaluating now!
2023-10-04 19:55:08,752 - INFO - Epoch [1280/4000] train_loss: 4.1442, lr: 0.000500, 0.04s
2023-10-04 19:55:08,790 - INFO - epoch complete!
2023-10-04 19:55:08,790 - INFO - evaluating now!
2023-10-04 19:55:08,790 - INFO - Epoch [1281/4000] train_loss: 4.1729, lr: 0.000500, 0.04s
2023-10-04 19:55:08,829 - INFO - epoch complete!
2023-10-04 19:55:08,829 - INFO - evaluating now!
2023-10-04 19:55:08,829 - INFO - Epoch [1282/4000] train_loss: 4.2506, lr: 0.000500, 0.04s
2023-10-04 19:55:08,867 - INFO - epoch complete!
2023-10-04 19:55:08,868 - INFO - evaluating now!
2023-10-04 19:55:08,868 - INFO - Epoch [1283/4000] train_loss: 4.2280, lr: 0.000500, 0.04s
2023-10-04 19:55:08,906 - INFO - epoch complete!
2023-10-04 19:55:08,907 - INFO - evaluating now!
2023-10-04 19:55:08,907 - INFO - Epoch [1284/4000] train_loss: 3.9850, lr: 0.000500, 0.04s
2023-10-04 19:55:08,945 - INFO - epoch complete!
2023-10-04 19:55:08,945 - INFO - evaluating now!
2023-10-04 19:55:08,946 - INFO - Epoch [1285/4000] train_loss: 3.8511, lr: 0.000500, 0.04s
2023-10-04 19:55:09,176 - INFO - Saved model at 1285
2023-10-04 19:55:09,176 - INFO - Val loss decrease from 3.9719 to 3.8511, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1285.tar
2023-10-04 19:55:09,216 - INFO - epoch complete!
2023-10-04 19:55:09,216 - INFO - evaluating now!
2023-10-04 19:55:09,216 - INFO - Epoch [1286/4000] train_loss: 3.9152, lr: 0.000500, 0.04s
2023-10-04 19:55:09,255 - INFO - epoch complete!
2023-10-04 19:55:09,256 - INFO - evaluating now!
2023-10-04 19:55:09,256 - INFO - Epoch [1287/4000] train_loss: 4.3649, lr: 0.000500, 0.04s
2023-10-04 19:55:09,295 - INFO - epoch complete!
2023-10-04 19:55:09,295 - INFO - evaluating now!
2023-10-04 19:55:09,296 - INFO - Epoch [1288/4000] train_loss: 4.2596, lr: 0.000500, 0.04s
2023-10-04 19:55:09,335 - INFO - epoch complete!
2023-10-04 19:55:09,335 - INFO - evaluating now!
2023-10-04 19:55:09,335 - INFO - Epoch [1289/4000] train_loss: 4.1219, lr: 0.000500, 0.04s
2023-10-04 19:55:09,374 - INFO - epoch complete!
2023-10-04 19:55:09,375 - INFO - evaluating now!
2023-10-04 19:55:09,375 - INFO - Epoch [1290/4000] train_loss: 4.2184, lr: 0.000500, 0.04s
2023-10-04 19:55:09,414 - INFO - epoch complete!
2023-10-04 19:55:09,414 - INFO - evaluating now!
2023-10-04 19:55:09,415 - INFO - Epoch [1291/4000] train_loss: 4.1079, lr: 0.000500, 0.04s
2023-10-04 19:55:09,454 - INFO - epoch complete!
2023-10-04 19:55:09,454 - INFO - evaluating now!
2023-10-04 19:55:09,454 - INFO - Epoch [1292/4000] train_loss: 4.5158, lr: 0.000500, 0.04s
2023-10-04 19:55:09,494 - INFO - epoch complete!
2023-10-04 19:55:09,494 - INFO - evaluating now!
2023-10-04 19:55:09,494 - INFO - Epoch [1293/4000] train_loss: 3.9672, lr: 0.000500, 0.04s
2023-10-04 19:55:09,533 - INFO - epoch complete!
2023-10-04 19:55:09,534 - INFO - evaluating now!
2023-10-04 19:55:09,534 - INFO - Epoch [1294/4000] train_loss: 4.2425, lr: 0.000500, 0.04s
2023-10-04 19:55:09,573 - INFO - epoch complete!
2023-10-04 19:55:09,573 - INFO - evaluating now!
2023-10-04 19:55:09,574 - INFO - Epoch [1295/4000] train_loss: 4.1331, lr: 0.000500, 0.04s
2023-10-04 19:55:09,613 - INFO - epoch complete!
2023-10-04 19:55:09,613 - INFO - evaluating now!
2023-10-04 19:55:09,613 - INFO - Epoch [1296/4000] train_loss: 4.1538, lr: 0.000500, 0.04s
2023-10-04 19:55:09,653 - INFO - epoch complete!
2023-10-04 19:55:09,653 - INFO - evaluating now!
2023-10-04 19:55:09,653 - INFO - Epoch [1297/4000] train_loss: 4.2712, lr: 0.000500, 0.04s
2023-10-04 19:55:09,692 - INFO - epoch complete!
2023-10-04 19:55:09,692 - INFO - evaluating now!
2023-10-04 19:55:09,693 - INFO - Epoch [1298/4000] train_loss: 4.1869, lr: 0.000500, 0.04s
2023-10-04 19:55:09,731 - INFO - epoch complete!
2023-10-04 19:55:09,731 - INFO - evaluating now!
2023-10-04 19:55:09,732 - INFO - Epoch [1299/4000] train_loss: 4.0505, lr: 0.000500, 0.04s
2023-10-04 19:55:09,770 - INFO - epoch complete!
2023-10-04 19:55:09,770 - INFO - evaluating now!
2023-10-04 19:55:09,770 - INFO - Epoch [1300/4000] train_loss: 4.0033, lr: 0.000500, 0.04s
2023-10-04 19:55:09,809 - INFO - epoch complete!
2023-10-04 19:55:09,809 - INFO - evaluating now!
2023-10-04 19:55:09,810 - INFO - Epoch [1301/4000] train_loss: 4.2145, lr: 0.000500, 0.04s
2023-10-04 19:55:09,848 - INFO - epoch complete!
2023-10-04 19:55:09,848 - INFO - evaluating now!
2023-10-04 19:55:09,848 - INFO - Epoch [1302/4000] train_loss: 4.0110, lr: 0.000500, 0.04s
2023-10-04 19:55:09,887 - INFO - epoch complete!
2023-10-04 19:55:09,887 - INFO - evaluating now!
2023-10-04 19:55:09,887 - INFO - Epoch [1303/4000] train_loss: 3.9888, lr: 0.000500, 0.04s
2023-10-04 19:55:09,926 - INFO - epoch complete!
2023-10-04 19:55:09,926 - INFO - evaluating now!
2023-10-04 19:55:09,926 - INFO - Epoch [1304/4000] train_loss: 4.1135, lr: 0.000500, 0.04s
2023-10-04 19:55:09,965 - INFO - epoch complete!
2023-10-04 19:55:09,965 - INFO - evaluating now!
2023-10-04 19:55:09,965 - INFO - Epoch [1305/4000] train_loss: 3.8879, lr: 0.000500, 0.04s
2023-10-04 19:55:10,004 - INFO - epoch complete!
2023-10-04 19:55:10,004 - INFO - evaluating now!
2023-10-04 19:55:10,004 - INFO - Epoch [1306/4000] train_loss: 4.1447, lr: 0.000500, 0.04s
2023-10-04 19:55:10,042 - INFO - epoch complete!
2023-10-04 19:55:10,043 - INFO - evaluating now!
2023-10-04 19:55:10,043 - INFO - Epoch [1307/4000] train_loss: 3.9581, lr: 0.000500, 0.04s
2023-10-04 19:55:10,081 - INFO - epoch complete!
2023-10-04 19:55:10,082 - INFO - evaluating now!
2023-10-04 19:55:10,082 - INFO - Epoch [1308/4000] train_loss: 4.2817, lr: 0.000500, 0.04s
2023-10-04 19:55:10,120 - INFO - epoch complete!
2023-10-04 19:55:10,120 - INFO - evaluating now!
2023-10-04 19:55:10,121 - INFO - Epoch [1309/4000] train_loss: 4.0259, lr: 0.000500, 0.04s
2023-10-04 19:55:10,159 - INFO - epoch complete!
2023-10-04 19:55:10,159 - INFO - evaluating now!
2023-10-04 19:55:10,160 - INFO - Epoch [1310/4000] train_loss: 4.4326, lr: 0.000500, 0.04s
2023-10-04 19:55:10,198 - INFO - epoch complete!
2023-10-04 19:55:10,198 - INFO - evaluating now!
2023-10-04 19:55:10,199 - INFO - Epoch [1311/4000] train_loss: 4.2053, lr: 0.000500, 0.04s
2023-10-04 19:55:10,237 - INFO - epoch complete!
2023-10-04 19:55:10,237 - INFO - evaluating now!
2023-10-04 19:55:10,237 - INFO - Epoch [1312/4000] train_loss: 4.1028, lr: 0.000500, 0.04s
2023-10-04 19:55:10,276 - INFO - epoch complete!
2023-10-04 19:55:10,276 - INFO - evaluating now!
2023-10-04 19:55:10,276 - INFO - Epoch [1313/4000] train_loss: 3.9912, lr: 0.000500, 0.04s
2023-10-04 19:55:10,315 - INFO - epoch complete!
2023-10-04 19:55:10,315 - INFO - evaluating now!
2023-10-04 19:55:10,315 - INFO - Epoch [1314/4000] train_loss: 4.0684, lr: 0.000500, 0.04s
2023-10-04 19:55:10,354 - INFO - epoch complete!
2023-10-04 19:55:10,354 - INFO - evaluating now!
2023-10-04 19:55:10,354 - INFO - Epoch [1315/4000] train_loss: 3.9238, lr: 0.000500, 0.04s
2023-10-04 19:55:10,393 - INFO - epoch complete!
2023-10-04 19:55:10,393 - INFO - evaluating now!
2023-10-04 19:55:10,393 - INFO - Epoch [1316/4000] train_loss: 3.9533, lr: 0.000500, 0.04s
2023-10-04 19:55:10,431 - INFO - epoch complete!
2023-10-04 19:55:10,432 - INFO - evaluating now!
2023-10-04 19:55:10,432 - INFO - Epoch [1317/4000] train_loss: 4.2834, lr: 0.000500, 0.04s
2023-10-04 19:55:10,471 - INFO - epoch complete!
2023-10-04 19:55:10,471 - INFO - evaluating now!
2023-10-04 19:55:10,471 - INFO - Epoch [1318/4000] train_loss: 4.2253, lr: 0.000500, 0.04s
2023-10-04 19:55:10,510 - INFO - epoch complete!
2023-10-04 19:55:10,510 - INFO - evaluating now!
2023-10-04 19:55:10,510 - INFO - Epoch [1319/4000] train_loss: 3.9921, lr: 0.000500, 0.04s
2023-10-04 19:55:10,549 - INFO - epoch complete!
2023-10-04 19:55:10,549 - INFO - evaluating now!
2023-10-04 19:55:10,549 - INFO - Epoch [1320/4000] train_loss: 4.0653, lr: 0.000500, 0.04s
2023-10-04 19:55:10,587 - INFO - epoch complete!
2023-10-04 19:55:10,588 - INFO - evaluating now!
2023-10-04 19:55:10,588 - INFO - Epoch [1321/4000] train_loss: 3.9742, lr: 0.000500, 0.04s
2023-10-04 19:55:10,627 - INFO - epoch complete!
2023-10-04 19:55:10,627 - INFO - evaluating now!
2023-10-04 19:55:10,627 - INFO - Epoch [1322/4000] train_loss: 4.0947, lr: 0.000500, 0.04s
2023-10-04 19:55:10,666 - INFO - epoch complete!
2023-10-04 19:55:10,666 - INFO - evaluating now!
2023-10-04 19:55:10,666 - INFO - Epoch [1323/4000] train_loss: 3.9236, lr: 0.000500, 0.04s
2023-10-04 19:55:10,705 - INFO - epoch complete!
2023-10-04 19:55:10,705 - INFO - evaluating now!
2023-10-04 19:55:10,705 - INFO - Epoch [1324/4000] train_loss: 3.9745, lr: 0.000500, 0.04s
2023-10-04 19:55:10,743 - INFO - epoch complete!
2023-10-04 19:55:10,744 - INFO - evaluating now!
2023-10-04 19:55:10,744 - INFO - Epoch [1325/4000] train_loss: 4.0550, lr: 0.000500, 0.04s
2023-10-04 19:55:10,782 - INFO - epoch complete!
2023-10-04 19:55:10,783 - INFO - evaluating now!
2023-10-04 19:55:10,783 - INFO - Epoch [1326/4000] train_loss: 3.9993, lr: 0.000500, 0.04s
2023-10-04 19:55:10,821 - INFO - epoch complete!
2023-10-04 19:55:10,822 - INFO - evaluating now!
2023-10-04 19:55:10,822 - INFO - Epoch [1327/4000] train_loss: 4.1071, lr: 0.000500, 0.04s
2023-10-04 19:55:10,860 - INFO - epoch complete!
2023-10-04 19:55:10,860 - INFO - evaluating now!
2023-10-04 19:55:10,861 - INFO - Epoch [1328/4000] train_loss: 4.1537, lr: 0.000500, 0.04s
2023-10-04 19:55:10,899 - INFO - epoch complete!
2023-10-04 19:55:10,899 - INFO - evaluating now!
2023-10-04 19:55:10,899 - INFO - Epoch [1329/4000] train_loss: 4.1867, lr: 0.000500, 0.04s
2023-10-04 19:55:10,938 - INFO - epoch complete!
2023-10-04 19:55:10,938 - INFO - evaluating now!
2023-10-04 19:55:10,938 - INFO - Epoch [1330/4000] train_loss: 4.0595, lr: 0.000500, 0.04s
2023-10-04 19:55:10,977 - INFO - epoch complete!
2023-10-04 19:55:10,977 - INFO - evaluating now!
2023-10-04 19:55:10,977 - INFO - Epoch [1331/4000] train_loss: 3.9764, lr: 0.000500, 0.04s
2023-10-04 19:55:11,016 - INFO - epoch complete!
2023-10-04 19:55:11,016 - INFO - evaluating now!
2023-10-04 19:55:11,016 - INFO - Epoch [1332/4000] train_loss: 4.1570, lr: 0.000500, 0.04s
2023-10-04 19:55:11,055 - INFO - epoch complete!
2023-10-04 19:55:11,055 - INFO - evaluating now!
2023-10-04 19:55:11,055 - INFO - Epoch [1333/4000] train_loss: 3.9705, lr: 0.000500, 0.04s
2023-10-04 19:55:11,093 - INFO - epoch complete!
2023-10-04 19:55:11,094 - INFO - evaluating now!
2023-10-04 19:55:11,094 - INFO - Epoch [1334/4000] train_loss: 3.9820, lr: 0.000500, 0.04s
2023-10-04 19:55:11,132 - INFO - epoch complete!
2023-10-04 19:55:11,133 - INFO - evaluating now!
2023-10-04 19:55:11,133 - INFO - Epoch [1335/4000] train_loss: 4.1558, lr: 0.000500, 0.04s
2023-10-04 19:55:11,171 - INFO - epoch complete!
2023-10-04 19:55:11,171 - INFO - evaluating now!
2023-10-04 19:55:11,171 - INFO - Epoch [1336/4000] train_loss: 4.3086, lr: 0.000500, 0.04s
2023-10-04 19:55:11,210 - INFO - epoch complete!
2023-10-04 19:55:11,210 - INFO - evaluating now!
2023-10-04 19:55:11,210 - INFO - Epoch [1337/4000] train_loss: 4.1906, lr: 0.000500, 0.04s
2023-10-04 19:55:11,249 - INFO - epoch complete!
2023-10-04 19:55:11,249 - INFO - evaluating now!
2023-10-04 19:55:11,249 - INFO - Epoch [1338/4000] train_loss: 4.1812, lr: 0.000500, 0.04s
2023-10-04 19:55:11,288 - INFO - epoch complete!
2023-10-04 19:55:11,288 - INFO - evaluating now!
2023-10-04 19:55:11,288 - INFO - Epoch [1339/4000] train_loss: 4.3410, lr: 0.000500, 0.04s
2023-10-04 19:55:11,327 - INFO - epoch complete!
2023-10-04 19:55:11,327 - INFO - evaluating now!
2023-10-04 19:55:11,327 - INFO - Epoch [1340/4000] train_loss: 4.1454, lr: 0.000500, 0.04s
2023-10-04 19:55:11,365 - INFO - epoch complete!
2023-10-04 19:55:11,366 - INFO - evaluating now!
2023-10-04 19:55:11,366 - INFO - Epoch [1341/4000] train_loss: 4.0299, lr: 0.000500, 0.04s
2023-10-04 19:55:11,404 - INFO - epoch complete!
2023-10-04 19:55:11,405 - INFO - evaluating now!
2023-10-04 19:55:11,405 - INFO - Epoch [1342/4000] train_loss: 4.0695, lr: 0.000500, 0.04s
2023-10-04 19:55:11,443 - INFO - epoch complete!
2023-10-04 19:55:11,444 - INFO - evaluating now!
2023-10-04 19:55:11,444 - INFO - Epoch [1343/4000] train_loss: 3.9816, lr: 0.000500, 0.04s
2023-10-04 19:55:11,484 - INFO - epoch complete!
2023-10-04 19:55:11,484 - INFO - evaluating now!
2023-10-04 19:55:11,484 - INFO - Epoch [1344/4000] train_loss: 4.1371, lr: 0.000500, 0.04s
2023-10-04 19:55:11,524 - INFO - epoch complete!
2023-10-04 19:55:11,524 - INFO - evaluating now!
2023-10-04 19:55:11,524 - INFO - Epoch [1345/4000] train_loss: 4.4533, lr: 0.000500, 0.04s
2023-10-04 19:55:11,563 - INFO - epoch complete!
2023-10-04 19:55:11,563 - INFO - evaluating now!
2023-10-04 19:55:11,563 - INFO - Epoch [1346/4000] train_loss: 3.8241, lr: 0.000500, 0.04s
2023-10-04 19:55:11,794 - INFO - Saved model at 1346
2023-10-04 19:55:11,794 - INFO - Val loss decrease from 3.8511 to 3.8241, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1346.tar
2023-10-04 19:55:11,833 - INFO - epoch complete!
2023-10-04 19:55:11,834 - INFO - evaluating now!
2023-10-04 19:55:11,834 - INFO - Epoch [1347/4000] train_loss: 4.0133, lr: 0.000500, 0.04s
2023-10-04 19:55:11,873 - INFO - epoch complete!
2023-10-04 19:55:11,873 - INFO - evaluating now!
2023-10-04 19:55:11,874 - INFO - Epoch [1348/4000] train_loss: 4.0046, lr: 0.000500, 0.04s
2023-10-04 19:55:11,913 - INFO - epoch complete!
2023-10-04 19:55:11,913 - INFO - evaluating now!
2023-10-04 19:55:11,913 - INFO - Epoch [1349/4000] train_loss: 4.0305, lr: 0.000500, 0.04s
2023-10-04 19:55:11,952 - INFO - epoch complete!
2023-10-04 19:55:11,952 - INFO - evaluating now!
2023-10-04 19:55:11,952 - INFO - Epoch [1350/4000] train_loss: 3.8688, lr: 0.000500, 0.04s
2023-10-04 19:55:11,990 - INFO - epoch complete!
2023-10-04 19:55:11,991 - INFO - evaluating now!
2023-10-04 19:55:11,991 - INFO - Epoch [1351/4000] train_loss: 4.1677, lr: 0.000500, 0.04s
2023-10-04 19:55:12,029 - INFO - epoch complete!
2023-10-04 19:55:12,030 - INFO - evaluating now!
2023-10-04 19:55:12,030 - INFO - Epoch [1352/4000] train_loss: 3.9387, lr: 0.000500, 0.04s
2023-10-04 19:55:12,068 - INFO - epoch complete!
2023-10-04 19:55:12,068 - INFO - evaluating now!
2023-10-04 19:55:12,068 - INFO - Epoch [1353/4000] train_loss: 4.0330, lr: 0.000500, 0.04s
2023-10-04 19:55:12,107 - INFO - epoch complete!
2023-10-04 19:55:12,107 - INFO - evaluating now!
2023-10-04 19:55:12,107 - INFO - Epoch [1354/4000] train_loss: 4.0623, lr: 0.000500, 0.04s
2023-10-04 19:55:12,146 - INFO - epoch complete!
2023-10-04 19:55:12,146 - INFO - evaluating now!
2023-10-04 19:55:12,146 - INFO - Epoch [1355/4000] train_loss: 4.2601, lr: 0.000500, 0.04s
2023-10-04 19:55:12,185 - INFO - epoch complete!
2023-10-04 19:55:12,185 - INFO - evaluating now!
2023-10-04 19:55:12,185 - INFO - Epoch [1356/4000] train_loss: 3.9514, lr: 0.000500, 0.04s
2023-10-04 19:55:12,223 - INFO - epoch complete!
2023-10-04 19:55:12,224 - INFO - evaluating now!
2023-10-04 19:55:12,224 - INFO - Epoch [1357/4000] train_loss: 4.0182, lr: 0.000500, 0.04s
2023-10-04 19:55:12,262 - INFO - epoch complete!
2023-10-04 19:55:12,262 - INFO - evaluating now!
2023-10-04 19:55:12,263 - INFO - Epoch [1358/4000] train_loss: 3.9941, lr: 0.000500, 0.04s
2023-10-04 19:55:12,301 - INFO - epoch complete!
2023-10-04 19:55:12,301 - INFO - evaluating now!
2023-10-04 19:55:12,301 - INFO - Epoch [1359/4000] train_loss: 4.0260, lr: 0.000500, 0.04s
2023-10-04 19:55:12,340 - INFO - epoch complete!
2023-10-04 19:55:12,340 - INFO - evaluating now!
2023-10-04 19:55:12,340 - INFO - Epoch [1360/4000] train_loss: 3.9380, lr: 0.000500, 0.04s
2023-10-04 19:55:12,378 - INFO - epoch complete!
2023-10-04 19:55:12,379 - INFO - evaluating now!
2023-10-04 19:55:12,379 - INFO - Epoch [1361/4000] train_loss: 3.9026, lr: 0.000500, 0.04s
2023-10-04 19:55:12,417 - INFO - epoch complete!
2023-10-04 19:55:12,417 - INFO - evaluating now!
2023-10-04 19:55:12,418 - INFO - Epoch [1362/4000] train_loss: 3.8275, lr: 0.000500, 0.04s
2023-10-04 19:55:12,456 - INFO - epoch complete!
2023-10-04 19:55:12,456 - INFO - evaluating now!
2023-10-04 19:55:12,456 - INFO - Epoch [1363/4000] train_loss: 3.8890, lr: 0.000500, 0.04s
2023-10-04 19:55:12,495 - INFO - epoch complete!
2023-10-04 19:55:12,495 - INFO - evaluating now!
2023-10-04 19:55:12,495 - INFO - Epoch [1364/4000] train_loss: 4.1453, lr: 0.000500, 0.04s
2023-10-04 19:55:12,534 - INFO - epoch complete!
2023-10-04 19:55:12,534 - INFO - evaluating now!
2023-10-04 19:55:12,534 - INFO - Epoch [1365/4000] train_loss: 3.9716, lr: 0.000500, 0.04s
2023-10-04 19:55:12,573 - INFO - epoch complete!
2023-10-04 19:55:12,573 - INFO - evaluating now!
2023-10-04 19:55:12,573 - INFO - Epoch [1366/4000] train_loss: 4.0536, lr: 0.000500, 0.04s
2023-10-04 19:55:12,611 - INFO - epoch complete!
2023-10-04 19:55:12,612 - INFO - evaluating now!
2023-10-04 19:55:12,612 - INFO - Epoch [1367/4000] train_loss: 4.2249, lr: 0.000500, 0.04s
2023-10-04 19:55:12,650 - INFO - epoch complete!
2023-10-04 19:55:12,650 - INFO - evaluating now!
2023-10-04 19:55:12,651 - INFO - Epoch [1368/4000] train_loss: 4.0096, lr: 0.000500, 0.04s
2023-10-04 19:55:12,689 - INFO - epoch complete!
2023-10-04 19:55:12,689 - INFO - evaluating now!
2023-10-04 19:55:12,689 - INFO - Epoch [1369/4000] train_loss: 3.8793, lr: 0.000500, 0.04s
2023-10-04 19:55:12,728 - INFO - epoch complete!
2023-10-04 19:55:12,728 - INFO - evaluating now!
2023-10-04 19:55:12,728 - INFO - Epoch [1370/4000] train_loss: 3.9786, lr: 0.000500, 0.04s
2023-10-04 19:55:12,767 - INFO - epoch complete!
2023-10-04 19:55:12,767 - INFO - evaluating now!
2023-10-04 19:55:12,767 - INFO - Epoch [1371/4000] train_loss: 3.9160, lr: 0.000500, 0.04s
2023-10-04 19:55:12,806 - INFO - epoch complete!
2023-10-04 19:55:12,806 - INFO - evaluating now!
2023-10-04 19:55:12,806 - INFO - Epoch [1372/4000] train_loss: 4.0756, lr: 0.000500, 0.04s
2023-10-04 19:55:12,844 - INFO - epoch complete!
2023-10-04 19:55:12,845 - INFO - evaluating now!
2023-10-04 19:55:12,845 - INFO - Epoch [1373/4000] train_loss: 4.2503, lr: 0.000500, 0.04s
2023-10-04 19:55:12,883 - INFO - epoch complete!
2023-10-04 19:55:12,883 - INFO - evaluating now!
2023-10-04 19:55:12,883 - INFO - Epoch [1374/4000] train_loss: 4.2511, lr: 0.000500, 0.04s
2023-10-04 19:55:12,922 - INFO - epoch complete!
2023-10-04 19:55:12,922 - INFO - evaluating now!
2023-10-04 19:55:12,922 - INFO - Epoch [1375/4000] train_loss: 4.2205, lr: 0.000500, 0.04s
2023-10-04 19:55:12,961 - INFO - epoch complete!
2023-10-04 19:55:12,961 - INFO - evaluating now!
2023-10-04 19:55:12,961 - INFO - Epoch [1376/4000] train_loss: 4.1071, lr: 0.000500, 0.04s
2023-10-04 19:55:12,999 - INFO - epoch complete!
2023-10-04 19:55:13,000 - INFO - evaluating now!
2023-10-04 19:55:13,000 - INFO - Epoch [1377/4000] train_loss: 3.9963, lr: 0.000500, 0.04s
2023-10-04 19:55:13,038 - INFO - epoch complete!
2023-10-04 19:55:13,039 - INFO - evaluating now!
2023-10-04 19:55:13,039 - INFO - Epoch [1378/4000] train_loss: 4.1307, lr: 0.000500, 0.04s
2023-10-04 19:55:13,077 - INFO - epoch complete!
2023-10-04 19:55:13,077 - INFO - evaluating now!
2023-10-04 19:55:13,077 - INFO - Epoch [1379/4000] train_loss: 4.1784, lr: 0.000500, 0.04s
2023-10-04 19:55:13,116 - INFO - epoch complete!
2023-10-04 19:55:13,116 - INFO - evaluating now!
2023-10-04 19:55:13,116 - INFO - Epoch [1380/4000] train_loss: 3.8672, lr: 0.000500, 0.04s
2023-10-04 19:55:13,155 - INFO - epoch complete!
2023-10-04 19:55:13,155 - INFO - evaluating now!
2023-10-04 19:55:13,155 - INFO - Epoch [1381/4000] train_loss: 3.9609, lr: 0.000500, 0.04s
2023-10-04 19:55:13,193 - INFO - epoch complete!
2023-10-04 19:55:13,194 - INFO - evaluating now!
2023-10-04 19:55:13,194 - INFO - Epoch [1382/4000] train_loss: 4.0748, lr: 0.000500, 0.04s
2023-10-04 19:55:13,232 - INFO - epoch complete!
2023-10-04 19:55:13,232 - INFO - evaluating now!
2023-10-04 19:55:13,232 - INFO - Epoch [1383/4000] train_loss: 4.0526, lr: 0.000500, 0.04s
2023-10-04 19:55:13,271 - INFO - epoch complete!
2023-10-04 19:55:13,271 - INFO - evaluating now!
2023-10-04 19:55:13,271 - INFO - Epoch [1384/4000] train_loss: 3.9497, lr: 0.000500, 0.04s
2023-10-04 19:55:13,310 - INFO - epoch complete!
2023-10-04 19:55:13,310 - INFO - evaluating now!
2023-10-04 19:55:13,310 - INFO - Epoch [1385/4000] train_loss: 4.0724, lr: 0.000500, 0.04s
2023-10-04 19:55:13,348 - INFO - epoch complete!
2023-10-04 19:55:13,349 - INFO - evaluating now!
2023-10-04 19:55:13,349 - INFO - Epoch [1386/4000] train_loss: 3.7992, lr: 0.000500, 0.04s
2023-10-04 19:55:13,584 - INFO - Saved model at 1386
2023-10-04 19:55:13,585 - INFO - Val loss decrease from 3.8241 to 3.7992, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1386.tar
2023-10-04 19:55:13,625 - INFO - epoch complete!
2023-10-04 19:55:13,625 - INFO - evaluating now!
2023-10-04 19:55:13,625 - INFO - Epoch [1387/4000] train_loss: 3.9997, lr: 0.000500, 0.04s
2023-10-04 19:55:13,665 - INFO - epoch complete!
2023-10-04 19:55:13,665 - INFO - evaluating now!
2023-10-04 19:55:13,665 - INFO - Epoch [1388/4000] train_loss: 3.9521, lr: 0.000500, 0.04s
2023-10-04 19:55:13,705 - INFO - epoch complete!
2023-10-04 19:55:13,705 - INFO - evaluating now!
2023-10-04 19:55:13,705 - INFO - Epoch [1389/4000] train_loss: 4.0901, lr: 0.000500, 0.04s
2023-10-04 19:55:13,745 - INFO - epoch complete!
2023-10-04 19:55:13,745 - INFO - evaluating now!
2023-10-04 19:55:13,745 - INFO - Epoch [1390/4000] train_loss: 3.9468, lr: 0.000500, 0.04s
2023-10-04 19:55:13,784 - INFO - epoch complete!
2023-10-04 19:55:13,785 - INFO - evaluating now!
2023-10-04 19:55:13,785 - INFO - Epoch [1391/4000] train_loss: 3.8131, lr: 0.000500, 0.04s
2023-10-04 19:55:13,824 - INFO - epoch complete!
2023-10-04 19:55:13,824 - INFO - evaluating now!
2023-10-04 19:55:13,824 - INFO - Epoch [1392/4000] train_loss: 3.9676, lr: 0.000500, 0.04s
2023-10-04 19:55:13,862 - INFO - epoch complete!
2023-10-04 19:55:13,863 - INFO - evaluating now!
2023-10-04 19:55:13,863 - INFO - Epoch [1393/4000] train_loss: 4.0810, lr: 0.000500, 0.04s
2023-10-04 19:55:13,901 - INFO - epoch complete!
2023-10-04 19:55:13,902 - INFO - evaluating now!
2023-10-04 19:55:13,902 - INFO - Epoch [1394/4000] train_loss: 3.8107, lr: 0.000500, 0.04s
2023-10-04 19:55:13,940 - INFO - epoch complete!
2023-10-04 19:55:13,941 - INFO - evaluating now!
2023-10-04 19:55:13,941 - INFO - Epoch [1395/4000] train_loss: 4.0584, lr: 0.000500, 0.04s
2023-10-04 19:55:13,979 - INFO - epoch complete!
2023-10-04 19:55:13,980 - INFO - evaluating now!
2023-10-04 19:55:13,980 - INFO - Epoch [1396/4000] train_loss: 4.0666, lr: 0.000500, 0.04s
2023-10-04 19:55:14,019 - INFO - epoch complete!
2023-10-04 19:55:14,019 - INFO - evaluating now!
2023-10-04 19:55:14,019 - INFO - Epoch [1397/4000] train_loss: 3.9305, lr: 0.000500, 0.04s
2023-10-04 19:55:14,058 - INFO - epoch complete!
2023-10-04 19:55:14,058 - INFO - evaluating now!
2023-10-04 19:55:14,058 - INFO - Epoch [1398/4000] train_loss: 3.9719, lr: 0.000500, 0.04s
2023-10-04 19:55:14,097 - INFO - epoch complete!
2023-10-04 19:55:14,097 - INFO - evaluating now!
2023-10-04 19:55:14,097 - INFO - Epoch [1399/4000] train_loss: 3.9748, lr: 0.000500, 0.04s
2023-10-04 19:55:14,136 - INFO - epoch complete!
2023-10-04 19:55:14,136 - INFO - evaluating now!
2023-10-04 19:55:14,136 - INFO - Epoch [1400/4000] train_loss: 4.0459, lr: 0.000500, 0.04s
2023-10-04 19:55:14,175 - INFO - epoch complete!
2023-10-04 19:55:14,175 - INFO - evaluating now!
2023-10-04 19:55:14,175 - INFO - Epoch [1401/4000] train_loss: 3.9221, lr: 0.000500, 0.04s
2023-10-04 19:55:14,213 - INFO - epoch complete!
2023-10-04 19:55:14,214 - INFO - evaluating now!
2023-10-04 19:55:14,214 - INFO - Epoch [1402/4000] train_loss: 3.9875, lr: 0.000500, 0.04s
2023-10-04 19:55:14,252 - INFO - epoch complete!
2023-10-04 19:55:14,253 - INFO - evaluating now!
2023-10-04 19:55:14,253 - INFO - Epoch [1403/4000] train_loss: 3.9254, lr: 0.000500, 0.04s
2023-10-04 19:55:14,291 - INFO - epoch complete!
2023-10-04 19:55:14,292 - INFO - evaluating now!
2023-10-04 19:55:14,292 - INFO - Epoch [1404/4000] train_loss: 4.0162, lr: 0.000500, 0.04s
2023-10-04 19:55:14,330 - INFO - epoch complete!
2023-10-04 19:55:14,330 - INFO - evaluating now!
2023-10-04 19:55:14,331 - INFO - Epoch [1405/4000] train_loss: 3.9375, lr: 0.000500, 0.04s
2023-10-04 19:55:14,369 - INFO - epoch complete!
2023-10-04 19:55:14,369 - INFO - evaluating now!
2023-10-04 19:55:14,369 - INFO - Epoch [1406/4000] train_loss: 3.9829, lr: 0.000500, 0.04s
2023-10-04 19:55:14,408 - INFO - epoch complete!
2023-10-04 19:55:14,408 - INFO - evaluating now!
2023-10-04 19:55:14,408 - INFO - Epoch [1407/4000] train_loss: 4.0612, lr: 0.000500, 0.04s
2023-10-04 19:55:14,447 - INFO - epoch complete!
2023-10-04 19:55:14,447 - INFO - evaluating now!
2023-10-04 19:55:14,447 - INFO - Epoch [1408/4000] train_loss: 4.0751, lr: 0.000500, 0.04s
2023-10-04 19:55:14,486 - INFO - epoch complete!
2023-10-04 19:55:14,486 - INFO - evaluating now!
2023-10-04 19:55:14,486 - INFO - Epoch [1409/4000] train_loss: 4.0046, lr: 0.000500, 0.04s
2023-10-04 19:55:14,525 - INFO - epoch complete!
2023-10-04 19:55:14,525 - INFO - evaluating now!
2023-10-04 19:55:14,525 - INFO - Epoch [1410/4000] train_loss: 4.1087, lr: 0.000500, 0.04s
2023-10-04 19:55:14,564 - INFO - epoch complete!
2023-10-04 19:55:14,564 - INFO - evaluating now!
2023-10-04 19:55:14,564 - INFO - Epoch [1411/4000] train_loss: 4.0181, lr: 0.000500, 0.04s
2023-10-04 19:55:14,603 - INFO - epoch complete!
2023-10-04 19:55:14,603 - INFO - evaluating now!
2023-10-04 19:55:14,603 - INFO - Epoch [1412/4000] train_loss: 4.0828, lr: 0.000500, 0.04s
2023-10-04 19:55:14,642 - INFO - epoch complete!
2023-10-04 19:55:14,642 - INFO - evaluating now!
2023-10-04 19:55:14,642 - INFO - Epoch [1413/4000] train_loss: 3.9682, lr: 0.000500, 0.04s
2023-10-04 19:55:14,680 - INFO - epoch complete!
2023-10-04 19:55:14,681 - INFO - evaluating now!
2023-10-04 19:55:14,681 - INFO - Epoch [1414/4000] train_loss: 4.2522, lr: 0.000500, 0.04s
2023-10-04 19:55:14,719 - INFO - epoch complete!
2023-10-04 19:55:14,720 - INFO - evaluating now!
2023-10-04 19:55:14,720 - INFO - Epoch [1415/4000] train_loss: 3.8387, lr: 0.000500, 0.04s
2023-10-04 19:55:14,758 - INFO - epoch complete!
2023-10-04 19:55:14,759 - INFO - evaluating now!
2023-10-04 19:55:14,759 - INFO - Epoch [1416/4000] train_loss: 3.8465, lr: 0.000500, 0.04s
2023-10-04 19:55:14,797 - INFO - epoch complete!
2023-10-04 19:55:14,797 - INFO - evaluating now!
2023-10-04 19:55:14,798 - INFO - Epoch [1417/4000] train_loss: 3.8360, lr: 0.000500, 0.04s
2023-10-04 19:55:14,836 - INFO - epoch complete!
2023-10-04 19:55:14,836 - INFO - evaluating now!
2023-10-04 19:55:14,836 - INFO - Epoch [1418/4000] train_loss: 3.8151, lr: 0.000500, 0.04s
2023-10-04 19:55:14,875 - INFO - epoch complete!
2023-10-04 19:55:14,875 - INFO - evaluating now!
2023-10-04 19:55:14,875 - INFO - Epoch [1419/4000] train_loss: 3.9111, lr: 0.000500, 0.04s
2023-10-04 19:55:14,914 - INFO - epoch complete!
2023-10-04 19:55:14,914 - INFO - evaluating now!
2023-10-04 19:55:14,914 - INFO - Epoch [1420/4000] train_loss: 4.0629, lr: 0.000500, 0.04s
2023-10-04 19:55:14,953 - INFO - epoch complete!
2023-10-04 19:55:14,954 - INFO - evaluating now!
2023-10-04 19:55:14,954 - INFO - Epoch [1421/4000] train_loss: 3.9258, lr: 0.000500, 0.04s
2023-10-04 19:55:14,992 - INFO - epoch complete!
2023-10-04 19:55:14,992 - INFO - evaluating now!
2023-10-04 19:55:14,993 - INFO - Epoch [1422/4000] train_loss: 3.9523, lr: 0.000500, 0.04s
2023-10-04 19:55:15,031 - INFO - epoch complete!
2023-10-04 19:55:15,031 - INFO - evaluating now!
2023-10-04 19:55:15,032 - INFO - Epoch [1423/4000] train_loss: 4.0631, lr: 0.000500, 0.04s
2023-10-04 19:55:15,070 - INFO - epoch complete!
2023-10-04 19:55:15,070 - INFO - evaluating now!
2023-10-04 19:55:15,070 - INFO - Epoch [1424/4000] train_loss: 4.1351, lr: 0.000500, 0.04s
2023-10-04 19:55:15,109 - INFO - epoch complete!
2023-10-04 19:55:15,109 - INFO - evaluating now!
2023-10-04 19:55:15,109 - INFO - Epoch [1425/4000] train_loss: 3.8808, lr: 0.000500, 0.04s
2023-10-04 19:55:15,148 - INFO - epoch complete!
2023-10-04 19:55:15,148 - INFO - evaluating now!
2023-10-04 19:55:15,148 - INFO - Epoch [1426/4000] train_loss: 3.9009, lr: 0.000500, 0.04s
2023-10-04 19:55:15,187 - INFO - epoch complete!
2023-10-04 19:55:15,187 - INFO - evaluating now!
2023-10-04 19:55:15,187 - INFO - Epoch [1427/4000] train_loss: 3.7373, lr: 0.000500, 0.04s
2023-10-04 19:55:15,420 - INFO - Saved model at 1427
2023-10-04 19:55:15,420 - INFO - Val loss decrease from 3.7992 to 3.7373, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1427.tar
2023-10-04 19:55:15,460 - INFO - epoch complete!
2023-10-04 19:55:15,461 - INFO - evaluating now!
2023-10-04 19:55:15,461 - INFO - Epoch [1428/4000] train_loss: 3.9308, lr: 0.000500, 0.04s
2023-10-04 19:55:15,500 - INFO - epoch complete!
2023-10-04 19:55:15,501 - INFO - evaluating now!
2023-10-04 19:55:15,501 - INFO - Epoch [1429/4000] train_loss: 4.1360, lr: 0.000500, 0.04s
2023-10-04 19:55:15,539 - INFO - epoch complete!
2023-10-04 19:55:15,540 - INFO - evaluating now!
2023-10-04 19:55:15,540 - INFO - Epoch [1430/4000] train_loss: 3.9049, lr: 0.000500, 0.04s
2023-10-04 19:55:15,578 - INFO - epoch complete!
2023-10-04 19:55:15,578 - INFO - evaluating now!
2023-10-04 19:55:15,579 - INFO - Epoch [1431/4000] train_loss: 3.8329, lr: 0.000500, 0.04s
2023-10-04 19:55:15,617 - INFO - epoch complete!
2023-10-04 19:55:15,618 - INFO - evaluating now!
2023-10-04 19:55:15,618 - INFO - Epoch [1432/4000] train_loss: 4.1264, lr: 0.000500, 0.04s
2023-10-04 19:55:15,656 - INFO - epoch complete!
2023-10-04 19:55:15,657 - INFO - evaluating now!
2023-10-04 19:55:15,657 - INFO - Epoch [1433/4000] train_loss: 3.8989, lr: 0.000500, 0.04s
2023-10-04 19:55:15,695 - INFO - epoch complete!
2023-10-04 19:55:15,696 - INFO - evaluating now!
2023-10-04 19:55:15,696 - INFO - Epoch [1434/4000] train_loss: 3.8808, lr: 0.000500, 0.04s
2023-10-04 19:55:15,734 - INFO - epoch complete!
2023-10-04 19:55:15,735 - INFO - evaluating now!
2023-10-04 19:55:15,735 - INFO - Epoch [1435/4000] train_loss: 3.8995, lr: 0.000500, 0.04s
2023-10-04 19:55:15,773 - INFO - epoch complete!
2023-10-04 19:55:15,774 - INFO - evaluating now!
2023-10-04 19:55:15,774 - INFO - Epoch [1436/4000] train_loss: 4.0602, lr: 0.000500, 0.04s
2023-10-04 19:55:15,812 - INFO - epoch complete!
2023-10-04 19:55:15,813 - INFO - evaluating now!
2023-10-04 19:55:15,813 - INFO - Epoch [1437/4000] train_loss: 3.8789, lr: 0.000500, 0.04s
2023-10-04 19:55:15,852 - INFO - epoch complete!
2023-10-04 19:55:15,852 - INFO - evaluating now!
2023-10-04 19:55:15,852 - INFO - Epoch [1438/4000] train_loss: 4.0380, lr: 0.000500, 0.04s
2023-10-04 19:55:15,891 - INFO - epoch complete!
2023-10-04 19:55:15,891 - INFO - evaluating now!
2023-10-04 19:55:15,891 - INFO - Epoch [1439/4000] train_loss: 3.8428, lr: 0.000500, 0.04s
2023-10-04 19:55:15,930 - INFO - epoch complete!
2023-10-04 19:55:15,930 - INFO - evaluating now!
2023-10-04 19:55:15,931 - INFO - Epoch [1440/4000] train_loss: 3.8464, lr: 0.000500, 0.04s
2023-10-04 19:55:15,969 - INFO - epoch complete!
2023-10-04 19:55:15,970 - INFO - evaluating now!
2023-10-04 19:55:15,970 - INFO - Epoch [1441/4000] train_loss: 3.8356, lr: 0.000500, 0.04s
2023-10-04 19:55:16,009 - INFO - epoch complete!
2023-10-04 19:55:16,009 - INFO - evaluating now!
2023-10-04 19:55:16,009 - INFO - Epoch [1442/4000] train_loss: 3.8976, lr: 0.000500, 0.04s
2023-10-04 19:55:16,048 - INFO - epoch complete!
2023-10-04 19:55:16,048 - INFO - evaluating now!
2023-10-04 19:55:16,049 - INFO - Epoch [1443/4000] train_loss: 3.9939, lr: 0.000500, 0.04s
2023-10-04 19:55:16,087 - INFO - epoch complete!
2023-10-04 19:55:16,088 - INFO - evaluating now!
2023-10-04 19:55:16,088 - INFO - Epoch [1444/4000] train_loss: 3.6622, lr: 0.000500, 0.04s
2023-10-04 19:55:16,318 - INFO - Saved model at 1444
2023-10-04 19:55:16,318 - INFO - Val loss decrease from 3.7373 to 3.6622, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1444.tar
2023-10-04 19:55:16,358 - INFO - epoch complete!
2023-10-04 19:55:16,359 - INFO - evaluating now!
2023-10-04 19:55:16,359 - INFO - Epoch [1445/4000] train_loss: 4.0035, lr: 0.000500, 0.04s
2023-10-04 19:55:16,398 - INFO - epoch complete!
2023-10-04 19:55:16,399 - INFO - evaluating now!
2023-10-04 19:55:16,399 - INFO - Epoch [1446/4000] train_loss: 4.1405, lr: 0.000500, 0.04s
2023-10-04 19:55:16,438 - INFO - epoch complete!
2023-10-04 19:55:16,438 - INFO - evaluating now!
2023-10-04 19:55:16,438 - INFO - Epoch [1447/4000] train_loss: 4.2057, lr: 0.000500, 0.04s
2023-10-04 19:55:16,477 - INFO - epoch complete!
2023-10-04 19:55:16,477 - INFO - evaluating now!
2023-10-04 19:55:16,477 - INFO - Epoch [1448/4000] train_loss: 3.8061, lr: 0.000500, 0.04s
2023-10-04 19:55:16,519 - INFO - epoch complete!
2023-10-04 19:55:16,519 - INFO - evaluating now!
2023-10-04 19:55:16,519 - INFO - Epoch [1449/4000] train_loss: 3.9933, lr: 0.000500, 0.04s
2023-10-04 19:55:16,558 - INFO - epoch complete!
2023-10-04 19:55:16,558 - INFO - evaluating now!
2023-10-04 19:55:16,558 - INFO - Epoch [1450/4000] train_loss: 3.8356, lr: 0.000500, 0.04s
2023-10-04 19:55:16,597 - INFO - epoch complete!
2023-10-04 19:55:16,598 - INFO - evaluating now!
2023-10-04 19:55:16,598 - INFO - Epoch [1451/4000] train_loss: 4.1173, lr: 0.000500, 0.04s
2023-10-04 19:55:16,637 - INFO - epoch complete!
2023-10-04 19:55:16,637 - INFO - evaluating now!
2023-10-04 19:55:16,638 - INFO - Epoch [1452/4000] train_loss: 3.9303, lr: 0.000500, 0.04s
2023-10-04 19:55:16,677 - INFO - epoch complete!
2023-10-04 19:55:16,677 - INFO - evaluating now!
2023-10-04 19:55:16,677 - INFO - Epoch [1453/4000] train_loss: 3.9480, lr: 0.000500, 0.04s
2023-10-04 19:55:16,716 - INFO - epoch complete!
2023-10-04 19:55:16,716 - INFO - evaluating now!
2023-10-04 19:55:16,716 - INFO - Epoch [1454/4000] train_loss: 3.8552, lr: 0.000500, 0.04s
2023-10-04 19:55:16,755 - INFO - epoch complete!
2023-10-04 19:55:16,755 - INFO - evaluating now!
2023-10-04 19:55:16,755 - INFO - Epoch [1455/4000] train_loss: 4.1164, lr: 0.000500, 0.04s
2023-10-04 19:55:16,794 - INFO - epoch complete!
2023-10-04 19:55:16,794 - INFO - evaluating now!
2023-10-04 19:55:16,795 - INFO - Epoch [1456/4000] train_loss: 3.7913, lr: 0.000500, 0.04s
2023-10-04 19:55:16,833 - INFO - epoch complete!
2023-10-04 19:55:16,834 - INFO - evaluating now!
2023-10-04 19:55:16,834 - INFO - Epoch [1457/4000] train_loss: 3.8353, lr: 0.000500, 0.04s
2023-10-04 19:55:16,872 - INFO - epoch complete!
2023-10-04 19:55:16,873 - INFO - evaluating now!
2023-10-04 19:55:16,873 - INFO - Epoch [1458/4000] train_loss: 3.8943, lr: 0.000500, 0.04s
2023-10-04 19:55:16,911 - INFO - epoch complete!
2023-10-04 19:55:16,912 - INFO - evaluating now!
2023-10-04 19:55:16,912 - INFO - Epoch [1459/4000] train_loss: 3.8427, lr: 0.000500, 0.04s
2023-10-04 19:55:16,951 - INFO - epoch complete!
2023-10-04 19:55:16,951 - INFO - evaluating now!
2023-10-04 19:55:16,951 - INFO - Epoch [1460/4000] train_loss: 4.0164, lr: 0.000500, 0.04s
2023-10-04 19:55:16,990 - INFO - epoch complete!
2023-10-04 19:55:16,990 - INFO - evaluating now!
2023-10-04 19:55:16,990 - INFO - Epoch [1461/4000] train_loss: 3.8710, lr: 0.000500, 0.04s
2023-10-04 19:55:17,029 - INFO - epoch complete!
2023-10-04 19:55:17,029 - INFO - evaluating now!
2023-10-04 19:55:17,030 - INFO - Epoch [1462/4000] train_loss: 3.8695, lr: 0.000500, 0.04s
2023-10-04 19:55:17,068 - INFO - epoch complete!
2023-10-04 19:55:17,068 - INFO - evaluating now!
2023-10-04 19:55:17,068 - INFO - Epoch [1463/4000] train_loss: 3.9825, lr: 0.000500, 0.04s
2023-10-04 19:55:17,107 - INFO - epoch complete!
2023-10-04 19:55:17,108 - INFO - evaluating now!
2023-10-04 19:55:17,108 - INFO - Epoch [1464/4000] train_loss: 3.7288, lr: 0.000500, 0.04s
2023-10-04 19:55:17,147 - INFO - epoch complete!
2023-10-04 19:55:17,147 - INFO - evaluating now!
2023-10-04 19:55:17,147 - INFO - Epoch [1465/4000] train_loss: 3.8933, lr: 0.000500, 0.04s
2023-10-04 19:55:17,186 - INFO - epoch complete!
2023-10-04 19:55:17,186 - INFO - evaluating now!
2023-10-04 19:55:17,187 - INFO - Epoch [1466/4000] train_loss: 3.7709, lr: 0.000500, 0.04s
2023-10-04 19:55:17,225 - INFO - epoch complete!
2023-10-04 19:55:17,225 - INFO - evaluating now!
2023-10-04 19:55:17,226 - INFO - Epoch [1467/4000] train_loss: 3.7240, lr: 0.000500, 0.04s
2023-10-04 19:55:17,264 - INFO - epoch complete!
2023-10-04 19:55:17,265 - INFO - evaluating now!
2023-10-04 19:55:17,265 - INFO - Epoch [1468/4000] train_loss: 4.0378, lr: 0.000500, 0.04s
2023-10-04 19:55:17,304 - INFO - epoch complete!
2023-10-04 19:55:17,304 - INFO - evaluating now!
2023-10-04 19:55:17,304 - INFO - Epoch [1469/4000] train_loss: 3.6310, lr: 0.000500, 0.04s
2023-10-04 19:55:17,535 - INFO - Saved model at 1469
2023-10-04 19:55:17,535 - INFO - Val loss decrease from 3.6622 to 3.6310, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1469.tar
2023-10-04 19:55:17,574 - INFO - epoch complete!
2023-10-04 19:55:17,575 - INFO - evaluating now!
2023-10-04 19:55:17,575 - INFO - Epoch [1470/4000] train_loss: 3.9354, lr: 0.000500, 0.04s
2023-10-04 19:55:17,614 - INFO - epoch complete!
2023-10-04 19:55:17,614 - INFO - evaluating now!
2023-10-04 19:55:17,615 - INFO - Epoch [1471/4000] train_loss: 3.8969, lr: 0.000500, 0.04s
2023-10-04 19:55:17,653 - INFO - epoch complete!
2023-10-04 19:55:17,654 - INFO - evaluating now!
2023-10-04 19:55:17,654 - INFO - Epoch [1472/4000] train_loss: 3.6659, lr: 0.000500, 0.04s
2023-10-04 19:55:17,693 - INFO - epoch complete!
2023-10-04 19:55:17,693 - INFO - evaluating now!
2023-10-04 19:55:17,693 - INFO - Epoch [1473/4000] train_loss: 3.8226, lr: 0.000500, 0.04s
2023-10-04 19:55:17,732 - INFO - epoch complete!
2023-10-04 19:55:17,732 - INFO - evaluating now!
2023-10-04 19:55:17,732 - INFO - Epoch [1474/4000] train_loss: 3.7901, lr: 0.000500, 0.04s
2023-10-04 19:55:17,771 - INFO - epoch complete!
2023-10-04 19:55:17,771 - INFO - evaluating now!
2023-10-04 19:55:17,771 - INFO - Epoch [1475/4000] train_loss: 4.6307, lr: 0.000500, 0.04s
2023-10-04 19:55:17,810 - INFO - epoch complete!
2023-10-04 19:55:17,811 - INFO - evaluating now!
2023-10-04 19:55:17,811 - INFO - Epoch [1476/4000] train_loss: 3.8638, lr: 0.000500, 0.04s
2023-10-04 19:55:17,849 - INFO - epoch complete!
2023-10-04 19:55:17,850 - INFO - evaluating now!
2023-10-04 19:55:17,850 - INFO - Epoch [1477/4000] train_loss: 3.8804, lr: 0.000500, 0.04s
2023-10-04 19:55:17,889 - INFO - epoch complete!
2023-10-04 19:55:17,889 - INFO - evaluating now!
2023-10-04 19:55:17,889 - INFO - Epoch [1478/4000] train_loss: 3.6905, lr: 0.000500, 0.04s
2023-10-04 19:55:17,928 - INFO - epoch complete!
2023-10-04 19:55:17,928 - INFO - evaluating now!
2023-10-04 19:55:17,928 - INFO - Epoch [1479/4000] train_loss: 3.8699, lr: 0.000500, 0.04s
2023-10-04 19:55:17,967 - INFO - epoch complete!
2023-10-04 19:55:17,967 - INFO - evaluating now!
2023-10-04 19:55:17,968 - INFO - Epoch [1480/4000] train_loss: 3.8492, lr: 0.000500, 0.04s
2023-10-04 19:55:18,006 - INFO - epoch complete!
2023-10-04 19:55:18,007 - INFO - evaluating now!
2023-10-04 19:55:18,007 - INFO - Epoch [1481/4000] train_loss: 3.9833, lr: 0.000500, 0.04s
2023-10-04 19:55:18,046 - INFO - epoch complete!
2023-10-04 19:55:18,046 - INFO - evaluating now!
2023-10-04 19:55:18,046 - INFO - Epoch [1482/4000] train_loss: 3.9280, lr: 0.000500, 0.04s
2023-10-04 19:55:18,085 - INFO - epoch complete!
2023-10-04 19:55:18,085 - INFO - evaluating now!
2023-10-04 19:55:18,086 - INFO - Epoch [1483/4000] train_loss: 3.8681, lr: 0.000500, 0.04s
2023-10-04 19:55:18,124 - INFO - epoch complete!
2023-10-04 19:55:18,125 - INFO - evaluating now!
2023-10-04 19:55:18,125 - INFO - Epoch [1484/4000] train_loss: 3.7139, lr: 0.000500, 0.04s
2023-10-04 19:55:18,164 - INFO - epoch complete!
2023-10-04 19:55:18,164 - INFO - evaluating now!
2023-10-04 19:55:18,164 - INFO - Epoch [1485/4000] train_loss: 4.0547, lr: 0.000500, 0.04s
2023-10-04 19:55:18,203 - INFO - epoch complete!
2023-10-04 19:55:18,203 - INFO - evaluating now!
2023-10-04 19:55:18,203 - INFO - Epoch [1486/4000] train_loss: 3.9125, lr: 0.000500, 0.04s
2023-10-04 19:55:18,242 - INFO - epoch complete!
2023-10-04 19:55:18,243 - INFO - evaluating now!
2023-10-04 19:55:18,243 - INFO - Epoch [1487/4000] train_loss: 4.0388, lr: 0.000500, 0.04s
2023-10-04 19:55:18,281 - INFO - epoch complete!
2023-10-04 19:55:18,282 - INFO - evaluating now!
2023-10-04 19:55:18,282 - INFO - Epoch [1488/4000] train_loss: 3.8522, lr: 0.000500, 0.04s
2023-10-04 19:55:18,321 - INFO - epoch complete!
2023-10-04 19:55:18,321 - INFO - evaluating now!
2023-10-04 19:55:18,321 - INFO - Epoch [1489/4000] train_loss: 3.8467, lr: 0.000500, 0.04s
2023-10-04 19:55:18,360 - INFO - epoch complete!
2023-10-04 19:55:18,360 - INFO - evaluating now!
2023-10-04 19:55:18,360 - INFO - Epoch [1490/4000] train_loss: 3.9950, lr: 0.000500, 0.04s
2023-10-04 19:55:18,399 - INFO - epoch complete!
2023-10-04 19:55:18,399 - INFO - evaluating now!
2023-10-04 19:55:18,399 - INFO - Epoch [1491/4000] train_loss: 3.8713, lr: 0.000500, 0.04s
2023-10-04 19:55:18,438 - INFO - epoch complete!
2023-10-04 19:55:18,438 - INFO - evaluating now!
2023-10-04 19:55:18,439 - INFO - Epoch [1492/4000] train_loss: 3.9643, lr: 0.000500, 0.04s
2023-10-04 19:55:18,478 - INFO - epoch complete!
2023-10-04 19:55:18,478 - INFO - evaluating now!
2023-10-04 19:55:18,478 - INFO - Epoch [1493/4000] train_loss: 3.8738, lr: 0.000500, 0.04s
2023-10-04 19:55:18,516 - INFO - epoch complete!
2023-10-04 19:55:18,517 - INFO - evaluating now!
2023-10-04 19:55:18,517 - INFO - Epoch [1494/4000] train_loss: 3.8893, lr: 0.000500, 0.04s
2023-10-04 19:55:18,555 - INFO - epoch complete!
2023-10-04 19:55:18,556 - INFO - evaluating now!
2023-10-04 19:55:18,556 - INFO - Epoch [1495/4000] train_loss: 3.9579, lr: 0.000500, 0.04s
2023-10-04 19:55:18,594 - INFO - epoch complete!
2023-10-04 19:55:18,595 - INFO - evaluating now!
2023-10-04 19:55:18,595 - INFO - Epoch [1496/4000] train_loss: 3.8616, lr: 0.000500, 0.04s
2023-10-04 19:55:18,633 - INFO - epoch complete!
2023-10-04 19:55:18,633 - INFO - evaluating now!
2023-10-04 19:55:18,634 - INFO - Epoch [1497/4000] train_loss: 3.7672, lr: 0.000500, 0.04s
2023-10-04 19:55:18,672 - INFO - epoch complete!
2023-10-04 19:55:18,672 - INFO - evaluating now!
2023-10-04 19:55:18,672 - INFO - Epoch [1498/4000] train_loss: 4.1813, lr: 0.000500, 0.04s
2023-10-04 19:55:18,711 - INFO - epoch complete!
2023-10-04 19:55:18,711 - INFO - evaluating now!
2023-10-04 19:55:18,711 - INFO - Epoch [1499/4000] train_loss: 3.8910, lr: 0.000500, 0.04s
2023-10-04 19:55:18,749 - INFO - epoch complete!
2023-10-04 19:55:18,750 - INFO - evaluating now!
2023-10-04 19:55:18,750 - INFO - Epoch [1500/4000] train_loss: 3.9370, lr: 0.000500, 0.04s
2023-10-04 19:55:18,788 - INFO - epoch complete!
2023-10-04 19:55:18,789 - INFO - evaluating now!
2023-10-04 19:55:18,789 - INFO - Epoch [1501/4000] train_loss: 3.7436, lr: 0.000500, 0.04s
2023-10-04 19:55:18,827 - INFO - epoch complete!
2023-10-04 19:55:18,827 - INFO - evaluating now!
2023-10-04 19:55:18,828 - INFO - Epoch [1502/4000] train_loss: 3.7701, lr: 0.000500, 0.04s
2023-10-04 19:55:18,866 - INFO - epoch complete!
2023-10-04 19:55:18,866 - INFO - evaluating now!
2023-10-04 19:55:18,866 - INFO - Epoch [1503/4000] train_loss: 3.8146, lr: 0.000500, 0.04s
2023-10-04 19:55:18,905 - INFO - epoch complete!
2023-10-04 19:55:18,905 - INFO - evaluating now!
2023-10-04 19:55:18,905 - INFO - Epoch [1504/4000] train_loss: 4.1678, lr: 0.000500, 0.04s
2023-10-04 19:55:18,944 - INFO - epoch complete!
2023-10-04 19:55:18,944 - INFO - evaluating now!
2023-10-04 19:55:18,944 - INFO - Epoch [1505/4000] train_loss: 3.9046, lr: 0.000500, 0.04s
2023-10-04 19:55:18,983 - INFO - epoch complete!
2023-10-04 19:55:18,983 - INFO - evaluating now!
2023-10-04 19:55:18,983 - INFO - Epoch [1506/4000] train_loss: 3.8896, lr: 0.000500, 0.04s
2023-10-04 19:55:19,021 - INFO - epoch complete!
2023-10-04 19:55:19,022 - INFO - evaluating now!
2023-10-04 19:55:19,022 - INFO - Epoch [1507/4000] train_loss: 4.0313, lr: 0.000500, 0.04s
2023-10-04 19:55:19,060 - INFO - epoch complete!
2023-10-04 19:55:19,060 - INFO - evaluating now!
2023-10-04 19:55:19,061 - INFO - Epoch [1508/4000] train_loss: 3.9535, lr: 0.000500, 0.04s
2023-10-04 19:55:19,099 - INFO - epoch complete!
2023-10-04 19:55:19,099 - INFO - evaluating now!
2023-10-04 19:55:19,100 - INFO - Epoch [1509/4000] train_loss: 3.8203, lr: 0.000500, 0.04s
2023-10-04 19:55:19,139 - INFO - epoch complete!
2023-10-04 19:55:19,139 - INFO - evaluating now!
2023-10-04 19:55:19,139 - INFO - Epoch [1510/4000] train_loss: 3.7339, lr: 0.000500, 0.04s
2023-10-04 19:55:19,179 - INFO - epoch complete!
2023-10-04 19:55:19,179 - INFO - evaluating now!
2023-10-04 19:55:19,179 - INFO - Epoch [1511/4000] train_loss: 3.9177, lr: 0.000500, 0.04s
2023-10-04 19:55:19,220 - INFO - epoch complete!
2023-10-04 19:55:19,220 - INFO - evaluating now!
2023-10-04 19:55:19,221 - INFO - Epoch [1512/4000] train_loss: 3.8195, lr: 0.000500, 0.04s
2023-10-04 19:55:19,259 - INFO - epoch complete!
2023-10-04 19:55:19,259 - INFO - evaluating now!
2023-10-04 19:55:19,260 - INFO - Epoch [1513/4000] train_loss: 3.9822, lr: 0.000500, 0.04s
2023-10-04 19:55:19,298 - INFO - epoch complete!
2023-10-04 19:55:19,298 - INFO - evaluating now!
2023-10-04 19:55:19,298 - INFO - Epoch [1514/4000] train_loss: 3.6075, lr: 0.000500, 0.04s
2023-10-04 19:55:19,529 - INFO - Saved model at 1514
2023-10-04 19:55:19,530 - INFO - Val loss decrease from 3.6310 to 3.6075, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1514.tar
2023-10-04 19:55:19,570 - INFO - epoch complete!
2023-10-04 19:55:19,570 - INFO - evaluating now!
2023-10-04 19:55:19,570 - INFO - Epoch [1515/4000] train_loss: 3.6413, lr: 0.000500, 0.04s
2023-10-04 19:55:19,610 - INFO - epoch complete!
2023-10-04 19:55:19,610 - INFO - evaluating now!
2023-10-04 19:55:19,610 - INFO - Epoch [1516/4000] train_loss: 3.9303, lr: 0.000500, 0.04s
2023-10-04 19:55:19,650 - INFO - epoch complete!
2023-10-04 19:55:19,650 - INFO - evaluating now!
2023-10-04 19:55:19,650 - INFO - Epoch [1517/4000] train_loss: 3.6539, lr: 0.000500, 0.04s
2023-10-04 19:55:19,690 - INFO - epoch complete!
2023-10-04 19:55:19,690 - INFO - evaluating now!
2023-10-04 19:55:19,690 - INFO - Epoch [1518/4000] train_loss: 3.8359, lr: 0.000500, 0.04s
2023-10-04 19:55:19,730 - INFO - epoch complete!
2023-10-04 19:55:19,730 - INFO - evaluating now!
2023-10-04 19:55:19,731 - INFO - Epoch [1519/4000] train_loss: 3.7365, lr: 0.000500, 0.04s
2023-10-04 19:55:19,770 - INFO - epoch complete!
2023-10-04 19:55:19,770 - INFO - evaluating now!
2023-10-04 19:55:19,770 - INFO - Epoch [1520/4000] train_loss: 3.8254, lr: 0.000500, 0.04s
2023-10-04 19:55:19,810 - INFO - epoch complete!
2023-10-04 19:55:19,810 - INFO - evaluating now!
2023-10-04 19:55:19,810 - INFO - Epoch [1521/4000] train_loss: 4.0402, lr: 0.000500, 0.04s
2023-10-04 19:55:19,849 - INFO - epoch complete!
2023-10-04 19:55:19,850 - INFO - evaluating now!
2023-10-04 19:55:19,850 - INFO - Epoch [1522/4000] train_loss: 3.5070, lr: 0.000500, 0.04s
2023-10-04 19:55:20,080 - INFO - Saved model at 1522
2023-10-04 19:55:20,080 - INFO - Val loss decrease from 3.6075 to 3.5070, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1522.tar
2023-10-04 19:55:20,121 - INFO - epoch complete!
2023-10-04 19:55:20,121 - INFO - evaluating now!
2023-10-04 19:55:20,121 - INFO - Epoch [1523/4000] train_loss: 3.7577, lr: 0.000500, 0.04s
2023-10-04 19:55:20,161 - INFO - epoch complete!
2023-10-04 19:55:20,161 - INFO - evaluating now!
2023-10-04 19:55:20,161 - INFO - Epoch [1524/4000] train_loss: 3.6793, lr: 0.000500, 0.04s
2023-10-04 19:55:20,200 - INFO - epoch complete!
2023-10-04 19:55:20,200 - INFO - evaluating now!
2023-10-04 19:55:20,201 - INFO - Epoch [1525/4000] train_loss: 3.8487, lr: 0.000500, 0.04s
2023-10-04 19:55:20,239 - INFO - epoch complete!
2023-10-04 19:55:20,239 - INFO - evaluating now!
2023-10-04 19:55:20,240 - INFO - Epoch [1526/4000] train_loss: 3.8234, lr: 0.000500, 0.04s
2023-10-04 19:55:20,278 - INFO - epoch complete!
2023-10-04 19:55:20,279 - INFO - evaluating now!
2023-10-04 19:55:20,279 - INFO - Epoch [1527/4000] train_loss: 3.9128, lr: 0.000500, 0.04s
2023-10-04 19:55:20,318 - INFO - epoch complete!
2023-10-04 19:55:20,318 - INFO - evaluating now!
2023-10-04 19:55:20,318 - INFO - Epoch [1528/4000] train_loss: 3.7345, lr: 0.000500, 0.04s
2023-10-04 19:55:20,357 - INFO - epoch complete!
2023-10-04 19:55:20,357 - INFO - evaluating now!
2023-10-04 19:55:20,357 - INFO - Epoch [1529/4000] train_loss: 3.8103, lr: 0.000500, 0.04s
2023-10-04 19:55:20,396 - INFO - epoch complete!
2023-10-04 19:55:20,396 - INFO - evaluating now!
2023-10-04 19:55:20,397 - INFO - Epoch [1530/4000] train_loss: 3.9058, lr: 0.000500, 0.04s
2023-10-04 19:55:20,435 - INFO - epoch complete!
2023-10-04 19:55:20,436 - INFO - evaluating now!
2023-10-04 19:55:20,436 - INFO - Epoch [1531/4000] train_loss: 3.7837, lr: 0.000500, 0.04s
2023-10-04 19:55:20,475 - INFO - epoch complete!
2023-10-04 19:55:20,475 - INFO - evaluating now!
2023-10-04 19:55:20,476 - INFO - Epoch [1532/4000] train_loss: 3.6161, lr: 0.000500, 0.04s
2023-10-04 19:55:20,515 - INFO - epoch complete!
2023-10-04 19:55:20,515 - INFO - evaluating now!
2023-10-04 19:55:20,515 - INFO - Epoch [1533/4000] train_loss: 3.8204, lr: 0.000500, 0.04s
2023-10-04 19:55:20,554 - INFO - epoch complete!
2023-10-04 19:55:20,554 - INFO - evaluating now!
2023-10-04 19:55:20,554 - INFO - Epoch [1534/4000] train_loss: 4.1060, lr: 0.000500, 0.04s
2023-10-04 19:55:20,593 - INFO - epoch complete!
2023-10-04 19:55:20,593 - INFO - evaluating now!
2023-10-04 19:55:20,594 - INFO - Epoch [1535/4000] train_loss: 3.9410, lr: 0.000500, 0.04s
2023-10-04 19:55:20,633 - INFO - epoch complete!
2023-10-04 19:55:20,633 - INFO - evaluating now!
2023-10-04 19:55:20,633 - INFO - Epoch [1536/4000] train_loss: 3.8778, lr: 0.000500, 0.04s
2023-10-04 19:55:20,672 - INFO - epoch complete!
2023-10-04 19:55:20,673 - INFO - evaluating now!
2023-10-04 19:55:20,673 - INFO - Epoch [1537/4000] train_loss: 3.7626, lr: 0.000500, 0.04s
2023-10-04 19:55:20,712 - INFO - epoch complete!
2023-10-04 19:55:20,712 - INFO - evaluating now!
2023-10-04 19:55:20,713 - INFO - Epoch [1538/4000] train_loss: 3.4737, lr: 0.000500, 0.04s
2023-10-04 19:55:20,943 - INFO - Saved model at 1538
2023-10-04 19:55:20,943 - INFO - Val loss decrease from 3.5070 to 3.4737, saving to ./libgptb/cache/89190/model_cache/GBT_Cora_epoch1538.tar
2023-10-04 19:55:20,983 - INFO - epoch complete!
2023-10-04 19:55:20,984 - INFO - evaluating now!
2023-10-04 19:55:20,984 - INFO - Epoch [1539/4000] train_loss: 3.8694, lr: 0.000500, 0.04s
2023-10-04 19:55:21,024 - INFO - epoch complete!
2023-10-04 19:55:21,024 - INFO - evaluating now!
2023-10-04 19:55:21,024 - INFO - Epoch [1540/4000] train_loss: 3.8163, lr: 0.000500, 0.04s
2023-10-04 19:55:21,064 - INFO - epoch complete!
2023-10-04 19:55:21,064 - INFO - evaluating now!
2023-10-04 19:55:21,064 - INFO - Epoch [1541/4000] train_loss: 3.8133, lr: 0.000500, 0.04s
2023-10-04 19:55:21,104 - INFO - epoch complete!
2023-10-04 19:55:21,104 - INFO - evaluating now!
2023-10-04 19:55:21,104 - INFO - Epoch [1542/4000] train_loss: 3.7289, lr: 0.000500, 0.04s
2023-10-04 19:55:21,144 - INFO - epoch complete!
2023-10-04 19:55:21,144 - INFO - evaluating now!
2023-10-04 19:55:21,145 - INFO - Epoch [1543/4000] train_loss: 3.7407, lr: 0.000500, 0.04s
2023-10-04 19:55:21,184 - INFO - epoch complete!
2023-10-04 19:55:21,184 - INFO - evaluating now!
2023-10-04 19:55:21,184 - INFO - Epoch [1544/4000] train_loss: 3.7547, lr: 0.000500, 0.04s
2023-10-04 19:55:21,224 - INFO - epoch complete!
2023-10-04 19:55:21,224 - INFO - evaluating now!
2023-10-04 19:55:21,224 - INFO - Epoch [1545/4000] train_loss: 3.9352, lr: 0.000500, 0.04s
2023-10-04 19:55:21,264 - INFO - epoch complete!
2023-10-04 19:55:21,264 - INFO - evaluating now!
2023-10-04 19:55:21,264 - INFO - Epoch [1546/4000] train_loss: 3.6963, lr: 0.000500, 0.04s
2023-10-04 19:55:21,304 - INFO - epoch complete!
2023-10-04 19:55:21,304 - INFO - evaluating now!
2023-10-04 19:55:21,304 - INFO - Epoch [1547/4000] train_loss: 3.6633, lr: 0.000500, 0.04s
2023-10-04 19:55:21,343 - INFO - epoch complete!
2023-10-04 19:55:21,343 - INFO - evaluating now!
2023-10-04 19:55:21,344 - INFO - Epoch [1548/4000] train_loss: 3.9706, lr: 0.000500, 0.04s
2023-10-04 19:55:21,382 - INFO - epoch complete!
2023-10-04 19:55:21,382 - INFO - evaluating now!
2023-10-04 19:55:21,383 - INFO - Epoch [1549/4000] train_loss: 3.7992, lr: 0.000500, 0.04s
2023-10-04 19:55:21,421 - INFO - epoch complete!
2023-10-04 19:55:21,421 - INFO - evaluating now!
2023-10-04 19:55:21,421 - INFO - Epoch [1550/4000] train_loss: 3.7782, lr: 0.000500, 0.04s
2023-10-04 19:55:21,460 - INFO - epoch complete!
2023-10-04 19:55:21,460 - INFO - evaluating now!
2023-10-04 19:55:21,460 - INFO - Epoch [1551/4000] train_loss: 3.6371, lr: 0.000500, 0.04s
2023-10-04 19:55:21,499 - INFO - epoch complete!
2023-10-04 19:55:21,500 - INFO - evaluating now!
2023-10-04 19:55:21,500 - INFO - Epoch [1552/4000] train_loss: 3.6193, lr: 0.000500, 0.04s
2023-10-04 19:55:21,538 - INFO - epoch complete!
2023-10-04 19:55:21,539 - INFO - evaluating now!
2023-10-04 19:55:21,539 - INFO - Epoch [1553/4000] train_loss: 3.8708, lr: 0.000500, 0.04s
2023-10-04 19:55:21,578 - INFO - epoch complete!
2023-10-04 19:55:21,578 - INFO - evaluating now!
2023-10-04 19:55:21,578 - INFO - Epoch [1554/4000] train_loss: 3.8074, lr: 0.000500, 0.04s
2023-10-04 19:55:21,618 - INFO - epoch complete!
2023-10-04 19:55:21,618 - INFO - evaluating now!
2023-10-04 19:55:21,618 - INFO - Epoch [1555/4000] train_loss: 3.8005, lr: 0.000500, 0.04s
2023-10-04 19:55:21,657 - INFO - epoch complete!
2023-10-04 19:55:21,657 - INFO - evaluating now!
2023-10-04 19:55:21,658 - INFO - Epoch [1556/4000] train_loss: 3.9867, lr: 0.000500, 0.04s
2023-10-04 19:55:21,697 - INFO - epoch complete!
2023-10-04 19:55:21,697 - INFO - evaluating now!
2023-10-04 19:55:21,697 - INFO - Epoch [1557/4000] train_loss: 3.8323, lr: 0.000500, 0.04s
2023-10-04 19:55:21,736 - INFO - epoch complete!
2023-10-04 19:55:21,736 - INFO - evaluating now!
2023-10-04 19:55:21,736 - INFO - Epoch [1558/4000] train_loss: 3.6351, lr: 0.000500, 0.04s
2023-10-04 19:55:21,774 - INFO - epoch complete!
2023-10-04 19:55:21,775 - INFO - evaluating now!
2023-10-04 19:55:21,775 - INFO - Epoch [1559/4000] train_loss: 3.6857, lr: 0.000500, 0.04s
2023-10-04 19:55:21,813 - INFO - epoch complete!
2023-10-04 19:55:21,814 - INFO - evaluating now!
2023-10-04 19:55:21,814 - INFO - Epoch [1560/4000] train_loss: 3.6891, lr: 0.000500, 0.04s
2023-10-04 19:55:21,852 - INFO - epoch complete!
2023-10-04 19:55:21,853 - INFO - evaluating now!
2023-10-04 19:55:21,853 - INFO - Epoch [1561/4000] train_loss: 3.7089, lr: 0.000500, 0.04s
2023-10-04 19:55:21,891 - INFO - epoch complete!
2023-10-04 19:55:21,891 - INFO - evaluating now!
2023-10-04 19:55:21,892 - INFO - Epoch [1562/4000] train_loss: 3.6731, lr: 0.000500, 0.04s
2023-10-04 19:55:21,930 - INFO - epoch complete!
2023-10-04 19:55:21,931 - INFO - evaluating now!
2023-10-04 19:55:21,931 - INFO - Epoch [1563/4000] train_loss: 3.7990, lr: 0.000500, 0.04s
2023-10-04 19:55:21,969 - INFO - epoch complete!
2023-10-04 19:55:21,969 - INFO - evaluating now!
2023-10-04 19:55:21,970 - INFO - Epoch [1564/4000] train_loss: 3.9840, lr: 0.000500, 0.04s
2023-10-04 19:55:22,008 - INFO - epoch complete!
2023-10-04 19:55:22,008 - INFO - evaluating now!
2023-10-04 19:55:22,008 - INFO - Epoch [1565/4000] train_loss: 3.8153, lr: 0.000500, 0.04s
2023-10-04 19:55:22,047 - INFO - epoch complete!
2023-10-04 19:55:22,047 - INFO - evaluating now!
2023-10-04 19:55:22,047 - INFO - Epoch [1566/4000] train_loss: 4.0030, lr: 0.000500, 0.04s
2023-10-04 19:55:22,086 - INFO - epoch complete!
2023-10-04 19:55:22,086 - INFO - evaluating now!
2023-10-04 19:55:22,086 - INFO - Epoch [1567/4000] train_loss: 3.6481, lr: 0.000500, 0.04s
2023-10-04 19:55:22,125 - INFO - epoch complete!
2023-10-04 19:55:22,125 - INFO - evaluating now!
2023-10-04 19:55:22,125 - INFO - Epoch [1568/4000] train_loss: 3.7275, lr: 0.000500, 0.04s
2023-10-04 19:55:22,164 - INFO - epoch complete!
2023-10-04 19:55:22,164 - INFO - evaluating now!
2023-10-04 19:55:22,164 - INFO - Epoch [1569/4000] train_loss: 3.9537, lr: 0.000500, 0.04s
2023-10-04 19:55:22,203 - INFO - epoch complete!
2023-10-04 19:55:22,203 - INFO - evaluating now!
2023-10-04 19:55:22,203 - INFO - Epoch [1570/4000] train_loss: 3.8310, lr: 0.000500, 0.04s
2023-10-04 19:55:22,242 - INFO - epoch complete!
2023-10-04 19:55:22,242 - INFO - evaluating now!
2023-10-04 19:55:22,242 - INFO - Epoch [1571/4000] train_loss: 3.6824, lr: 0.000500, 0.04s
2023-10-04 19:55:22,281 - INFO - epoch complete!
2023-10-04 19:55:22,281 - INFO - evaluating now!
2023-10-04 19:55:22,281 - INFO - Epoch [1572/4000] train_loss: 3.8135, lr: 0.000500, 0.04s
2023-10-04 19:55:22,320 - INFO - epoch complete!
2023-10-04 19:55:22,320 - INFO - evaluating now!
2023-10-04 19:55:22,320 - INFO - Epoch [1573/4000] train_loss: 3.6447, lr: 0.000500, 0.04s
2023-10-04 19:55:22,359 - INFO - epoch complete!
2023-10-04 19:55:22,359 - INFO - evaluating now!
2023-10-04 19:55:22,359 - INFO - Epoch [1574/4000] train_loss: 3.8670, lr: 0.000500, 0.04s
2023-10-04 19:55:22,398 - INFO - epoch complete!
2023-10-04 19:55:22,398 - INFO - evaluating now!
2023-10-04 19:55:22,398 - INFO - Epoch [1575/4000] train_loss: 3.6783, lr: 0.000500, 0.04s
2023-10-04 19:55:22,437 - INFO - epoch complete!
2023-10-04 19:55:22,437 - INFO - evaluating now!
2023-10-04 19:55:22,437 - INFO - Epoch [1576/4000] train_loss: 3.9154, lr: 0.000500, 0.04s
2023-10-04 19:55:22,476 - INFO - epoch complete!
2023-10-04 19:55:22,476 - INFO - evaluating now!
2023-10-04 19:55:22,476 - INFO - Epoch [1577/4000] train_loss: 3.7320, lr: 0.000500, 0.04s
2023-10-04 19:55:22,515 - INFO - epoch complete!
2023-10-04 19:55:22,516 - INFO - evaluating now!
2023-10-04 19:55:22,516 - INFO - Epoch [1578/4000] train_loss: 3.7352, lr: 0.000500, 0.04s
2023-10-04 19:55:22,554 - INFO - epoch complete!
2023-10-04 19:55:22,555 - INFO - evaluating now!
2023-10-04 19:55:22,555 - INFO - Epoch [1579/4000] train_loss: 3.7317, lr: 0.000500, 0.04s
2023-10-04 19:55:22,593 - INFO - epoch complete!
2023-10-04 19:55:22,594 - INFO - evaluating now!
2023-10-04 19:55:22,594 - INFO - Epoch [1580/4000] train_loss: 3.9147, lr: 0.000500, 0.04s
2023-10-04 19:55:22,632 - INFO - epoch complete!
2023-10-04 19:55:22,633 - INFO - evaluating now!
2023-10-04 19:55:22,633 - INFO - Epoch [1581/4000] train_loss: 3.6833, lr: 0.000500, 0.04s
2023-10-04 19:55:22,672 - INFO - epoch complete!
2023-10-04 19:55:22,672 - INFO - evaluating now!
2023-10-04 19:55:22,672 - INFO - Epoch [1582/4000] train_loss: 3.8078, lr: 0.000500, 0.04s
2023-10-04 19:55:22,710 - INFO - epoch complete!
2023-10-04 19:55:22,711 - INFO - evaluating now!
2023-10-04 19:55:22,711 - INFO - Epoch [1583/4000] train_loss: 3.8828, lr: 0.000500, 0.04s
2023-10-04 19:55:22,750 - INFO - epoch complete!
2023-10-04 19:55:22,750 - INFO - evaluating now!
2023-10-04 19:55:22,750 - INFO - Epoch [1584/4000] train_loss: 3.6505, lr: 0.000500, 0.04s
2023-10-04 19:55:22,789 - INFO - epoch complete!
2023-10-04 19:55:22,789 - INFO - evaluating now!
2023-10-04 19:55:22,789 - INFO - Epoch [1585/4000] train_loss: 4.0866, lr: 0.000500, 0.04s
2023-10-04 19:55:22,828 - INFO - epoch complete!
2023-10-04 19:55:22,828 - INFO - evaluating now!
2023-10-04 19:55:22,828 - INFO - Epoch [1586/4000] train_loss: 3.8126, lr: 0.000500, 0.04s
2023-10-04 19:55:22,867 - INFO - epoch complete!
2023-10-04 19:55:22,867 - INFO - evaluating now!
2023-10-04 19:55:22,868 - INFO - Epoch [1587/4000] train_loss: 3.9937, lr: 0.000500, 0.04s
2023-10-04 19:55:22,906 - INFO - epoch complete!
2023-10-04 19:55:22,906 - INFO - evaluating now!
2023-10-04 19:55:22,906 - INFO - Epoch [1588/4000] train_loss: 3.7103, lr: 0.000500, 0.04s
2023-10-04 19:55:22,945 - INFO - epoch complete!
2023-10-04 19:55:22,945 - INFO - evaluating now!
2023-10-04 19:55:22,945 - INFO - Epoch [1589/4000] train_loss: 3.7486, lr: 0.000500, 0.04s
2023-10-04 19:55:22,984 - INFO - epoch complete!
2023-10-04 19:55:22,984 - INFO - evaluating now!
2023-10-04 19:55:22,985 - INFO - Epoch [1590/4000] train_loss: 3.9083, lr: 0.000500, 0.04s
2023-10-04 19:55:23,023 - INFO - epoch complete!
2023-10-04 19:55:23,023 - INFO - evaluating now!
2023-10-04 19:55:23,023 - INFO - Epoch [1591/4000] train_loss: 3.6318, lr: 0.000500, 0.04s
2023-10-04 19:55:23,062 - INFO - epoch complete!
2023-10-04 19:55:23,063 - INFO - evaluating now!
2023-10-04 19:55:23,063 - INFO - Epoch [1592/4000] train_loss: 3.6781, lr: 0.000500, 0.04s
2023-10-04 19:55:23,103 - INFO - epoch complete!
2023-10-04 19:55:23,103 - INFO - evaluating now!
2023-10-04 19:55:23,104 - INFO - Epoch [1593/4000] train_loss: 3.5340, lr: 0.000500, 0.04s
2023-10-04 19:55:23,143 - INFO - epoch complete!
2023-10-04 19:55:23,144 - INFO - evaluating now!
2023-10-04 19:55:23,144 - INFO - Epoch [1594/4000] train_loss: 3.5417, lr: 0.000500, 0.04s
2023-10-04 19:55:23,182 - INFO - epoch complete!
2023-10-04 19:55:23,183 - INFO - evaluating now!
2023-10-04 19:55:23,183 - INFO - Epoch [1595/4000] train_loss: 3.5439, lr: 0.000500, 0.04s
2023-10-04 19:55:23,222 - INFO - epoch complete!
2023-10-04 19:55:23,222 - INFO - evaluating now!
2023-10-04 19:55:23,222 - INFO - Epoch [1596/4000] train_loss: 3.7480, lr: 0.000500, 0.04s
2023-10-04 19:55:23,261 - INFO - epoch complete!
2023-10-04 19:55:23,261 - INFO - evaluating now!
2023-10-04 19:55:23,261 - INFO - Epoch [1597/4000] train_loss: 3.7215, lr: 0.000500, 0.04s
2023-10-04 19:55:23,300 - INFO - epoch complete!
2023-10-04 19:55:23,300 - INFO - evaluating now!
2023-10-04 19:55:23,300 - INFO - Epoch [1598/4000] train_loss: 3.6886, lr: 0.000500, 0.04s
2023-10-04 19:55:23,339 - INFO - epoch complete!
2023-10-04 19:55:23,339 - INFO - evaluating now!
2023-10-04 19:55:23,339 - INFO - Epoch [1599/4000] train_loss: 3.8703, lr: 0.000500, 0.04s
2023-10-04 19:55:23,378 - INFO - epoch complete!
2023-10-04 19:55:23,378 - INFO - evaluating now!
2023-10-04 19:55:23,378 - INFO - Epoch [1600/4000] train_loss: 3.6620, lr: 0.000500, 0.04s
2023-10-04 19:55:23,417 - INFO - epoch complete!
2023-10-04 19:55:23,417 - INFO - evaluating now!
2023-10-04 19:55:23,417 - INFO - Epoch [1601/4000] train_loss: 3.5324, lr: 0.000500, 0.04s
2023-10-04 19:55:23,456 - INFO - epoch complete!
2023-10-04 19:55:23,456 - INFO - evaluating now!
2023-10-04 19:55:23,456 - INFO - Epoch [1602/4000] train_loss: 3.7009, lr: 0.000500, 0.04s
2023-10-04 19:55:23,495 - INFO - epoch complete!
2023-10-04 19:55:23,495 - INFO - evaluating now!
2023-10-04 19:55:23,496 - INFO - Epoch [1603/4000] train_loss: 4.0288, lr: 0.000500, 0.04s
2023-10-04 19:55:23,534 - INFO - epoch complete!
2023-10-04 19:55:23,535 - INFO - evaluating now!
2023-10-04 19:55:23,535 - INFO - Epoch [1604/4000] train_loss: 4.1092, lr: 0.000500, 0.04s
2023-10-04 19:55:23,573 - INFO - epoch complete!
2023-10-04 19:55:23,574 - INFO - evaluating now!
2023-10-04 19:55:23,574 - INFO - Epoch [1605/4000] train_loss: 3.6278, lr: 0.000500, 0.04s
2023-10-04 19:55:23,612 - INFO - epoch complete!
2023-10-04 19:55:23,613 - INFO - evaluating now!
2023-10-04 19:55:23,613 - INFO - Epoch [1606/4000] train_loss: 3.8908, lr: 0.000500, 0.04s
2023-10-04 19:55:23,652 - INFO - epoch complete!
2023-10-04 19:55:23,652 - INFO - evaluating now!
2023-10-04 19:55:23,652 - INFO - Epoch [1607/4000] train_loss: 3.6051, lr: 0.000500, 0.04s
2023-10-04 19:55:23,691 - INFO - epoch complete!
2023-10-04 19:55:23,691 - INFO - evaluating now!
2023-10-04 19:55:23,691 - INFO - Epoch [1608/4000] train_loss: 3.5751, lr: 0.000500, 0.04s
2023-10-04 19:55:23,730 - INFO - epoch complete!
2023-10-04 19:55:23,730 - INFO - evaluating now!
2023-10-04 19:55:23,730 - INFO - Epoch [1609/4000] train_loss: 3.8568, lr: 0.000500, 0.04s
2023-10-04 19:55:23,768 - INFO - epoch complete!
2023-10-04 19:55:23,769 - INFO - evaluating now!
2023-10-04 19:55:23,769 - INFO - Epoch [1610/4000] train_loss: 3.6847, lr: 0.000500, 0.04s
2023-10-04 19:55:23,807 - INFO - epoch complete!
2023-10-04 19:55:23,807 - INFO - evaluating now!
2023-10-04 19:55:23,808 - INFO - Epoch [1611/4000] train_loss: 3.6600, lr: 0.000500, 0.04s
2023-10-04 19:55:23,846 - INFO - epoch complete!
2023-10-04 19:55:23,846 - INFO - evaluating now!
2023-10-04 19:55:23,847 - INFO - Epoch [1612/4000] train_loss: 3.7209, lr: 0.000500, 0.04s
2023-10-04 19:55:23,885 - INFO - epoch complete!
2023-10-04 19:55:23,886 - INFO - evaluating now!
2023-10-04 19:55:23,886 - INFO - Epoch [1613/4000] train_loss: 3.6997, lr: 0.000500, 0.04s
2023-10-04 19:55:23,925 - INFO - epoch complete!
2023-10-04 19:55:23,925 - INFO - evaluating now!
2023-10-04 19:55:23,925 - INFO - Epoch [1614/4000] train_loss: 3.6722, lr: 0.000500, 0.04s
2023-10-04 19:55:23,964 - INFO - epoch complete!
2023-10-04 19:55:23,964 - INFO - evaluating now!
2023-10-04 19:55:23,964 - INFO - Epoch [1615/4000] train_loss: 3.6767, lr: 0.000500, 0.04s
2023-10-04 19:55:24,003 - INFO - epoch complete!
2023-10-04 19:55:24,003 - INFO - evaluating now!
2023-10-04 19:55:24,003 - INFO - Epoch [1616/4000] train_loss: 3.7578, lr: 0.000500, 0.04s
2023-10-04 19:55:24,042 - INFO - epoch complete!
2023-10-04 19:55:24,042 - INFO - evaluating now!
2023-10-04 19:55:24,042 - INFO - Epoch [1617/4000] train_loss: 3.9090, lr: 0.000500, 0.04s
2023-10-04 19:55:24,081 - INFO - epoch complete!
2023-10-04 19:55:24,081 - INFO - evaluating now!
2023-10-04 19:55:24,081 - INFO - Epoch [1618/4000] train_loss: 3.6613, lr: 0.000500, 0.04s
2023-10-04 19:55:24,120 - INFO - epoch complete!
2023-10-04 19:55:24,120 - INFO - evaluating now!
2023-10-04 19:55:24,121 - INFO - Epoch [1619/4000] train_loss: 3.9199, lr: 0.000500, 0.04s
2023-10-04 19:55:24,159 - INFO - epoch complete!
2023-10-04 19:55:24,159 - INFO - evaluating now!
2023-10-04 19:55:24,160 - INFO - Epoch [1620/4000] train_loss: 3.5940, lr: 0.000500, 0.04s
2023-10-04 19:55:24,198 - INFO - epoch complete!
2023-10-04 19:55:24,198 - INFO - evaluating now!
2023-10-04 19:55:24,198 - INFO - Epoch [1621/4000] train_loss: 3.5527, lr: 0.000500, 0.04s
2023-10-04 19:55:24,237 - INFO - epoch complete!
2023-10-04 19:55:24,237 - INFO - evaluating now!
2023-10-04 19:55:24,238 - INFO - Epoch [1622/4000] train_loss: 3.9159, lr: 0.000500, 0.04s
2023-10-04 19:55:24,276 - INFO - epoch complete!
2023-10-04 19:55:24,276 - INFO - evaluating now!
2023-10-04 19:55:24,277 - INFO - Epoch [1623/4000] train_loss: 3.6671, lr: 0.000500, 0.04s
2023-10-04 19:55:24,315 - INFO - epoch complete!
2023-10-04 19:55:24,315 - INFO - evaluating now!
2023-10-04 19:55:24,315 - INFO - Epoch [1624/4000] train_loss: 3.5871, lr: 0.000500, 0.04s
2023-10-04 19:55:24,354 - INFO - epoch complete!
2023-10-04 19:55:24,354 - INFO - evaluating now!
2023-10-04 19:55:24,354 - INFO - Epoch [1625/4000] train_loss: 3.7538, lr: 0.000500, 0.04s
2023-10-04 19:55:24,393 - INFO - epoch complete!
2023-10-04 19:55:24,393 - INFO - evaluating now!
2023-10-04 19:55:24,393 - INFO - Epoch [1626/4000] train_loss: 3.9986, lr: 0.000500, 0.04s
2023-10-04 19:55:24,432 - INFO - epoch complete!
2023-10-04 19:55:24,432 - INFO - evaluating now!
2023-10-04 19:55:24,432 - INFO - Epoch [1627/4000] train_loss: 3.6109, lr: 0.000500, 0.04s
2023-10-04 19:55:24,471 - INFO - epoch complete!
2023-10-04 19:55:24,471 - INFO - evaluating now!
2023-10-04 19:55:24,472 - INFO - Epoch [1628/4000] train_loss: 3.9784, lr: 0.000500, 0.04s
2023-10-04 19:55:24,510 - INFO - epoch complete!
2023-10-04 19:55:24,511 - INFO - evaluating now!
2023-10-04 19:55:24,511 - INFO - Epoch [1629/4000] train_loss: 3.6301, lr: 0.000500, 0.04s
2023-10-04 19:55:24,549 - INFO - epoch complete!
2023-10-04 19:55:24,550 - INFO - evaluating now!
2023-10-04 19:55:24,550 - INFO - Epoch [1630/4000] train_loss: 3.6933, lr: 0.000500, 0.04s
2023-10-04 19:55:24,588 - INFO - epoch complete!
2023-10-04 19:55:24,589 - INFO - evaluating now!
2023-10-04 19:55:24,589 - INFO - Epoch [1631/4000] train_loss: 3.5904, lr: 0.000500, 0.04s
2023-10-04 19:55:24,627 - INFO - epoch complete!
2023-10-04 19:55:24,628 - INFO - evaluating now!
2023-10-04 19:55:24,628 - INFO - Epoch [1632/4000] train_loss: 3.8580, lr: 0.000500, 0.04s
2023-10-04 19:55:24,666 - INFO - epoch complete!
2023-10-04 19:55:24,667 - INFO - evaluating now!
2023-10-04 19:55:24,667 - INFO - Epoch [1633/4000] train_loss: 3.7394, lr: 0.000500, 0.04s
2023-10-04 19:55:24,706 - INFO - epoch complete!
2023-10-04 19:55:24,706 - INFO - evaluating now!
2023-10-04 19:55:24,706 - INFO - Epoch [1634/4000] train_loss: 3.8221, lr: 0.000500, 0.04s
2023-10-04 19:55:24,745 - INFO - epoch complete!
2023-10-04 19:55:24,746 - INFO - evaluating now!
2023-10-04 19:55:24,746 - INFO - Epoch [1635/4000] train_loss: 3.8142, lr: 0.000500, 0.04s
2023-10-04 19:55:24,785 - INFO - epoch complete!
2023-10-04 19:55:24,785 - INFO - evaluating now!
2023-10-04 19:55:24,785 - INFO - Epoch [1636/4000] train_loss: 3.6414, lr: 0.000500, 0.04s
2023-10-04 19:55:24,824 - INFO - epoch complete!
2023-10-04 19:55:24,825 - INFO - evaluating now!
2023-10-04 19:55:24,825 - INFO - Epoch [1637/4000] train_loss: 3.8261, lr: 0.000500, 0.04s
2023-10-04 19:55:24,863 - INFO - epoch complete!
2023-10-04 19:55:24,864 - INFO - evaluating now!
2023-10-04 19:55:24,864 - INFO - Epoch [1638/4000] train_loss: 3.7773, lr: 0.000500, 0.04s
2023-10-04 19:55:24,903 - INFO - epoch complete!
2023-10-04 19:55:24,903 - INFO - evaluating now!
2023-10-04 19:55:24,903 - INFO - Epoch [1639/4000] train_loss: 3.8594, lr: 0.000500, 0.04s
2023-10-04 19:55:24,942 - INFO - epoch complete!
2023-10-04 19:55:24,943 - INFO - evaluating now!
2023-10-04 19:55:24,943 - INFO - Epoch [1640/4000] train_loss: 3.5668, lr: 0.000500, 0.04s
2023-10-04 19:55:24,982 - INFO - epoch complete!
2023-10-04 19:55:24,983 - INFO - evaluating now!
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
2023-10-04 19:55:24,983 - INFO - Epoch [1641/4000] train_loss: 3.7241, lr: 0.000500, 0.04s
2023-10-04 19:55:25,023 - INFO - epoch complete!
2023-10-04 19:55:25,024 - INFO - evaluating now!
2023-10-04 19:55:25,024 - INFO - Epoch [1642/4000] train_loss: 3.8132, lr: 0.000500, 0.04s
2023-10-04 19:55:25,064 - INFO - epoch complete!
2023-10-04 19:55:25,064 - INFO - evaluating now!
slurmstepd: error: *** STEP 794752.0 ON node436 CANCELLED AT 2023-10-04T19:55:25 ***
slurmstepd: error: *** JOB 794752 ON node436 CANCELLED AT 2023-10-04T19:55:25 ***
